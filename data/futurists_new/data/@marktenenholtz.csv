,0,1,2,3
0,@marktenenholtz,2023-02-16 14:04:12+00:00,https://twitter.com/marktenenholtz/status/1626220906027823104,@rblourenco @karpathy That’s the one
1,@marktenenholtz,2023-02-16 13:46:19+00:00,https://twitter.com/marktenenholtz/status/1626216404717543424,"Try to learn insanely complex Transformer architecture, and you can watch @karpathy 's brilliant, simply explained video.

Try to learn the simple ARIMA model this is what's considered ""simply explained"": https://t.co/jvdL3CMP7x"
2,@marktenenholtz,2023-02-16 13:22:45+00:00,https://twitter.com/marktenenholtz/status/1626210474923794432,@jrosell Thanks! (Had to delete/repost the thread though)
3,@marktenenholtz,2023-02-15 18:49:17+00:00,https://twitter.com/marktenenholtz/status/1625930263342481411,@asbdahfg @AlphaMinus2 and to the left
4,@marktenenholtz,2023-02-15 18:00:28+00:00,https://twitter.com/marktenenholtz/status/1625917975630073856,You can smile at the progress we've made or be bitter that we aren't there yet https://t.co/Huc48eLZUq
5,@marktenenholtz,2023-02-15 15:54:19+00:00,https://twitter.com/marktenenholtz/status/1625886230964568068,Practicing writing like GPT-3 so I get flagged and can hide from GPT-4’s training data
6,@marktenenholtz,2023-02-15 14:48:44+00:00,https://twitter.com/marktenenholtz/status/1625869726726356995,"@svpino @tunguz You're both wrong. It's 90% modeling, and 90% figuring out how to make your methods sound more fancy than they really are"
7,@marktenenholtz,2023-02-15 13:16:23+00:00,https://twitter.com/marktenenholtz/status/1625846484888330248,"@atchimolajuwon It’s not a perfect comparison, but still mind blowing even when you consider that"
8,@marktenenholtz,2023-02-15 13:03:23+00:00,https://twitter.com/marktenenholtz/status/1625843213830602755,"Insane.

The more powerful pre-trained models we get, the more they will permeate everything you use

And the more they do that, the more the moat surrounding any type of intellectual work shrinks.

AI won’t replace you any time soon, but someone using AI will! https://t.co/kHxrf8xK3Y"
9,@marktenenholtz,2023-02-14 20:58:59+00:00,https://twitter.com/marktenenholtz/status/1625600514246189064,"@vad13irt Thanks for all of your replies -- love seeing how another power user does it!

Plus a bonus shoutout for intermediate use of PyTorch Lightning callbacks 😂"
10,@marktenenholtz,2023-02-14 13:01:14+00:00,https://twitter.com/marktenenholtz/status/1625480284950110209,"If you liked this thread, go try Weights &amp; Biases out! https://t.co/hykrGzBCtw"
11,@marktenenholtz,2023-02-14 13:01:13+00:00,https://twitter.com/marktenenholtz/status/1625480282374799360,"It was as simple as that! Thanks to @weights_biases for sponsoring this.

In my next thread, I'll show you how this all looks in the dashboard, plus some handy tips for navigating and comparing runs!

Follow me @marktenenholtz so you don't miss it!"
12,@marktenenholtz,2023-02-14 13:01:13+00:00,https://twitter.com/marktenenholtz/status/1625480279757553668,"An even better way to store run-level settings like batch size is to pass a ""config"" argument with a dictionary of parameters to the function that creates the logger.

You can also add other arguments, like more detailed notes."
13,@marktenenholtz,2023-02-14 13:01:12+00:00,https://twitter.com/marktenenholtz/status/1625480275584249856,"You can see that after my training code runs, I store a few values in a ""summary"" object.

Any values that you set here will get stored in a tabular format in your W&amp;B dashboard. https://t.co/7eYn8W4wh8"
14,@marktenenholtz,2023-02-14 13:01:10+00:00,https://twitter.com/marktenenholtz/status/1625480266444840962,"One last thing to notice: I'm storing the value of the best validation loss as a model attribute.

W&amp;B actually has a built-in feature for this, but I didn't know it at the time.

define_metric() with the ""summary"" parameter accomplishes the same thing. https://t.co/lfnC3CieBi"
15,@marktenenholtz,2023-02-14 13:01:08+00:00,https://twitter.com/marktenenholtz/status/1625480258672816129,"You'll also notice on line 350 that I loop over some target columns and log a few extra values.

That's because this was a multi-target regression competition.

W&amp;B made it really easy to track errors on all of the underlying targets!"
16,@marktenenholtz,2023-02-14 13:01:07+00:00,https://twitter.com/marktenenholtz/status/1625480254587551744,"If you then take the average of all of the averages, you're overweighting the error from the final samples by 2x!

So, I instead combine all of my validation predictions and labels and calculate the results all at once:

(you can always sum them all up and divide by N, though) https://t.co/N4ZITneJ3F"
17,@marktenenholtz,2023-02-14 13:01:05+00:00,https://twitter.com/marktenenholtz/status/1625480246203129856,"A common mistake ML practitioners make is incorrectly accumulating their metrics.

If you have 450 samples and you use a batch size of 100, your first 4 batches will have a metric averaged over 400 samples.

But the last one will be averaged over 50!"
18,@marktenenholtz,2023-02-14 13:01:04+00:00,https://twitter.com/marktenenholtz/status/1625480242684100612,"Now, we can move on to the training code.

There's nothing too crazy here, and that's the beauty.

W&amp;B simply handles all of the complexity of logging my training loss.

But you'll notice that there's no logging in the validation step.

Why is that? https://t.co/BLJFcE8bR4"
19,@marktenenholtz,2023-02-14 13:01:02+00:00,https://twitter.com/marktenenholtz/status/1625480234844954625,"Let's create our model.

I had a *lot* of hyperparameters to track here.

Fortunately, W&amp;B's integrates with PyTorch Lightning's ""save_hyperparameters()"" method, and tracks all of these for me! https://t.co/P2yyl218Gv"
20,@marktenenholtz,2023-02-14 13:01:00+00:00,https://twitter.com/marktenenholtz/status/1625480227328790528,"The real key, though is the ""group"" parameter.

That allows me to keep cross-validation experiments linked together.

I can easily use the dashboard to filter by experiment name and pre-trained model name and compare the average CV error metrics!"
21,@marktenenholtz,2023-02-14 13:01:00+00:00,https://twitter.com/marktenenholtz/status/1625480224715730946,"In my case, I just take the name of the pre-trained model I'm using and concatenate that with a short, descriptive name.

I pass that descriptive name as a command line argument for every experiment (and I require that argument to be passed every time)."
22,@marktenenholtz,2023-02-14 13:00:59+00:00,https://twitter.com/marktenenholtz/status/1625480222069133313,"The naming convention itself isn't too creative.

But I see so many folks relying on random names because they can't be bothered to name each experiment.

It's so easy to lose track of experiments if you do that.

But that's just my opinion! W&amp;B has great support for auto-names."
23,@marktenenholtz,2023-02-14 13:00:58+00:00,https://twitter.com/marktenenholtz/status/1625480219422515200,"The first step was (uncreatively) naming my project the same as the Kaggle competition, and turning off artifact storage since I can't use that in competitions.

The second step was creating a naming convention that allows me to effortlessly search my experiments."
24,@marktenenholtz,2023-02-14 13:00:58+00:00,https://twitter.com/marktenenholtz/status/1625480215941251074,"Let's start with creating the logger object.

Since I was using PyTorch Lightning to train my models, all I had to do to get going was import the built-in logger integration for W&amp;B.

But, there are some *crucial* organizational steps I took here. https://t.co/WL48esDbXi"
25,@marktenenholtz,2023-02-14 13:00:56+00:00,https://twitter.com/marktenenholtz/status/1625480208857051138,"Creating great ML models is an iterative process.

It's not always iterative to the level of ""1300 runs,"" but for me, this exercise was one in seeing just how far I could push experiment tracking.

And, in this case, I was pushing @weights_biases to the limit."
26,@marktenenholtz,2023-02-14 13:00:55+00:00,https://twitter.com/marktenenholtz/status/1625480205224779776,"How I used experiment tracking to manage 1300 experiments and bag a 🥈 medal in a Kaggle competition:

(Part 1: the code) https://t.co/QcVWrMufUk"
27,@marktenenholtz,2023-02-14 03:29:19+00:00,https://twitter.com/marktenenholtz/status/1625336356774854656,@machsci @laurenbalik @alexkyllo @sh_reya @emollick @itsandrewgao Always interesting to see how people classify me. Thanks for the shoutout!
28,@marktenenholtz,2023-02-13 14:55:53+00:00,https://twitter.com/marktenenholtz/status/1625146748908740611,@Sergei_Imaging As long as it can maintain its performance in the real world it’s still amazing to me
29,@marktenenholtz,2023-02-13 14:55:28+00:00,https://twitter.com/marktenenholtz/status/1625146644516728834,@marcelotournier Yeah nearly every model I’ve deployed had a rules engine built on top
30,@marktenenholtz,2023-02-13 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1625117656717340672,"Making ML models work in the real world, at scale is an amazing combination of creativity, persistence, and black magic."
31,@marktenenholtz,2023-02-12 18:29:03+00:00,https://twitter.com/marktenenholtz/status/1624838006195585025,Which is not necessarily a bad thing. But wild that the tooling got so good for this stuff that we just leapfrogged the most productive use-case of AI in the whole world.
32,@marktenenholtz,2023-02-12 18:27:36+00:00,https://twitter.com/marktenenholtz/status/1624837640502624256,@CanumaGdt Bringing a new architecture =/= improving performance
33,@marktenenholtz,2023-02-12 18:00:14+00:00,https://twitter.com/marktenenholtz/status/1624830756374863879,So many folks using 175B parameter LLMs would have no idea how to forecast their own server usage.
34,@marktenenholtz,2023-02-12 18:00:14+00:00,https://twitter.com/marktenenholtz/status/1624830753761820672,"We've made so much progress in the last 7-8 years on complex domains like text and images and basically none on simple sequences of numbers (i.e. time-series forecasting).

In fact, I think we've regressed since good information is so hard to find."
35,@marktenenholtz,2023-02-12 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1624755214812323840,"Research papers are treasure troves of awful UX.

Defining your variables after you write the equation is a recipe for confusion.

Not to mention that the variables are usually defined using terse jargon. And if you don't understand that, then, well... 🤷‍♂️"
36,@marktenenholtz,2023-02-11 21:01:48+00:00,https://twitter.com/marktenenholtz/status/1624514058111131650,@LouayRouabeh5 Better buy a comfy chair 😂
37,@marktenenholtz,2023-02-11 21:01:36+00:00,https://twitter.com/marktenenholtz/status/1624514011193647105,@willbaebae Don't hold your breath!
38,@marktenenholtz,2023-02-11 20:51:12+00:00,https://twitter.com/marktenenholtz/status/1624511393738137603,"If you think LLMs can cause a lot of damage to society, wait until you see what AGI can do."
39,@marktenenholtz,2023-02-11 18:01:33+00:00,https://twitter.com/marktenenholtz/status/1624468699758444545,@svpino I like the bit on writing tests. Copilot Labs now lets you do that
40,@marktenenholtz,2023-02-11 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1624392854809477122,"An experiment run without:

• An experiment tracker
• An evaluation setup
• An output visualizer

is probably a waste of time.

Set those up first and you’ll have a much better idea of what you’re hoping to get out of your time investment."
41,@marktenenholtz,2023-02-10 17:26:39+00:00,https://twitter.com/marktenenholtz/status/1624097526902140928,@tunguz I don’t see anything
42,@marktenenholtz,2023-02-10 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1624030492348809216,"""Generative models have taken over all aspects of ML!""

XGBoost: https://t.co/IWP3PXKvkw"
43,@marktenenholtz,2023-02-09 18:00:48+00:00,https://twitter.com/marktenenholtz/status/1623743735149301762,@AlphaMinus2 Bingo!
44,@marktenenholtz,2023-02-09 18:00:31+00:00,https://twitter.com/marktenenholtz/status/1623743663468576770,@voskresenskiia Time-series forecasting
45,@marktenenholtz,2023-02-09 18:00:01+00:00,https://twitter.com/marktenenholtz/status/1623743536775528451,@goose_god_chaos Hmm that’s not *exactly* what the denominator represents
46,@marktenenholtz,2023-02-09 17:59:10+00:00,https://twitter.com/marktenenholtz/status/1623743323264409602,@BritneyMuller You got that right!
47,@marktenenholtz,2023-02-09 17:58:50+00:00,https://twitter.com/marktenenholtz/status/1623743238208200705,@AlphaMinus2 Close! It’s not an absolute error
48,@marktenenholtz,2023-02-09 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1623668099529740289,"Hint: h is forecast horizon, n is the number of data points in the training set."
49,@marktenenholtz,2023-02-09 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1623668096438534144,"Let's play a game:

Name that error metric! https://t.co/BDHfTvon0N"
50,@marktenenholtz,2023-02-09 02:58:44+00:00,https://twitter.com/marktenenholtz/status/1623516720265560066,@TheZachMueller @gusthema Strong looks pretty great. I’ll give it a shot.
51,@marktenenholtz,2023-02-09 00:25:56+00:00,https://twitter.com/marktenenholtz/status/1623478268077780998,@karpathy Good luck Andrej! We always need more builders in AI working in high leverage positions.
52,@marktenenholtz,2023-02-08 23:51:06+00:00,https://twitter.com/marktenenholtz/status/1623469502645108737,@gusthema Notes 😛
53,@marktenenholtz,2023-02-08 18:41:03+00:00,https://twitter.com/marktenenholtz/status/1623391475357696002,@tunguz I would also add: there is a major disconnect between what is popular and what actually drives business value.
54,@marktenenholtz,2023-02-08 18:00:09+00:00,https://twitter.com/marktenenholtz/status/1623381181512421377,On that ChatGPT Plus waitlist while it's down like https://t.co/1LcPKdlm2X
55,@marktenenholtz,2023-02-08 16:16:12+00:00,https://twitter.com/marktenenholtz/status/1623355023458533378,@Shpigford @DetangleAI @justinmfarrugia So excited for this. Can’t wait to actually bring something original (and so useful) to the table!
56,@marktenenholtz,2023-02-08 14:56:42+00:00,https://twitter.com/marktenenholtz/status/1623335017681195010,@dkhundley 🥳
57,@marktenenholtz,2023-02-08 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1623305724951662594,"Feature engineering for time-series forecasting is insanely prone to bugs, even though Pandas is built for it.

So I wrote a pipeline to calculate grouped rolling and lagged features once and just copy/paste it.

Take my course and you'll get that code!
https://t.co/y0DHX1NowF"
58,@marktenenholtz,2023-02-07 19:43:37+00:00,https://twitter.com/marktenenholtz/status/1623044831667982336,Read more here: https://t.co/Y21gtkDvtJ
59,@marktenenholtz,2023-02-07 19:43:36+00:00,https://twitter.com/marktenenholtz/status/1623044827930841088,"Microsoft just announced Bing + Edge integration with AI.

BUT! It's not with ChatGPT.

It's with a ""new, next-generation OpenAI large language model that is more powerful than ChatGPT and customized specifically for search.""

Maybe I'll finally give Bing + Edge a try? 🥴 https://t.co/QECgGeQjVq"
60,@marktenenholtz,2023-02-07 17:29:39+00:00,https://twitter.com/marktenenholtz/status/1623011120842698752,@tunguz 😔
61,@marktenenholtz,2023-02-07 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1622943279808667648,"The folks telling you we’re going to have AGI in the next year or two probably were introduced to be space by GPT-3 🤡

Expect to be disappointed.

Go build something useful in the meantime."
62,@marktenenholtz,2023-02-06 20:44:36+00:00,https://twitter.com/marktenenholtz/status/1622697791209316352,@tunguz Trying our luck are we?
63,@marktenenholtz,2023-02-06 19:40:51+00:00,https://twitter.com/marktenenholtz/status/1622681750311944192,Read more here: https://t.co/6LQBYb6DI4
64,@marktenenholtz,2023-02-06 19:40:50+00:00,https://twitter.com/marktenenholtz/status/1622681745895329799,"Google just announced Bard, a conversational AI service powered by their LaMDA model 🎉

AKA, their ChatGPT competitor. https://t.co/RinyrT67NR"
65,@marktenenholtz,2023-02-06 14:03:14+00:00,https://twitter.com/marktenenholtz/status/1622596783594434560,@ChristophMolnar 100%. So much easier to approach evaluation in the messy real world after that
66,@marktenenholtz,2023-02-06 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1622580912780345346,"Kaggle won’t teach you everything.

But you’ll learn some things on Kaggle that you’d probably never pick up elsewhere."
67,@marktenenholtz,2023-02-05 21:42:38+00:00,https://twitter.com/marktenenholtz/status/1622350009248190464,@ExcelHumor https://t.co/sIZM607BW2
68,@marktenenholtz,2023-02-05 19:52:48+00:00,https://twitter.com/marktenenholtz/status/1622322366847111169,@tunguz Mid &lt;pause&gt; journey
69,@marktenenholtz,2023-02-05 17:17:30+00:00,https://twitter.com/marktenenholtz/status/1622283287044538375,@JTFouquier Just deposited your extra 4 words into a high-interest savings account
70,@marktenenholtz,2023-02-05 17:16:32+00:00,https://twitter.com/marktenenholtz/status/1622283041325326338,@donnellymjd Counter argument: GPT-3
71,@marktenenholtz,2023-02-05 17:15:06+00:00,https://twitter.com/marktenenholtz/status/1622282679725989890,@ntkris Good enough correlation == causation 😂
72,@marktenenholtz,2023-02-05 17:14:16+00:00,https://twitter.com/marktenenholtz/status/1622282472271527938,@nadavwiz You’re triggering me
73,@marktenenholtz,2023-02-05 17:14:02+00:00,https://twitter.com/marktenenholtz/status/1622282415157788680,@DSaience TVH is all you need 😂
74,@marktenenholtz,2023-02-05 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1622218499346268163,Offend a data scientist in 5 words or less:
75,@marktenenholtz,2023-02-04 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1621856133421105152,"Working with messy text data used to be so annoying.

Now I just ask GPT-3 or FLAN-T5 to clean it up for me 😂"
76,@marktenenholtz,2023-02-03 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1621493765688135680,"Improving your data processing skills has an exponential relationship with the quality of the models you make.

It’s not just that you make better features, you unlock better ways of thinking about the problem."
77,@marktenenholtz,2023-02-03 03:05:57+00:00,https://twitter.com/marktenenholtz/status/1621344209855778816,@rasbt @austinvhuang @BlackHC @randal_olson How differently do the detectors that are out there work? Maybe we could ensemble them for better accuracy?
78,@marktenenholtz,2023-02-02 21:33:50+00:00,https://twitter.com/marktenenholtz/status/1621260629792153601,"My fellow Austinites: join so we laugh, drink, and be merry (about XGBoost and ChatGPT)"
79,@marktenenholtz,2023-02-02 18:09:17+00:00,https://twitter.com/marktenenholtz/status/1621209153799688194,@capetorch And don’t forget assign
80,@marktenenholtz,2023-02-02 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1621131367345172480,"I changed the way I write Pandas code and it went from feeling clunky and repetitive to expressive and very readable.

Probably the Python data package with the biggest hidden learning curve."
81,@marktenenholtz,2023-02-01 18:31:50+00:00,https://twitter.com/marktenenholtz/status/1620852442480132097,"ChatGPT Plus: $20/mo

Feels more sensible from a commercial perspective. Gonna be tough for just anybody to compete at that price point.

$20 for no more outages and speedy completions feels like a no-brainer for power users.

(But don’t worry. Free is here to stay.) https://t.co/L9wxINYpzU"
82,@marktenenholtz,2023-02-01 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1620768999780003840,"I used to read novels for fiction.

Now, I go on LinkedIn."
83,@marktenenholtz,2023-01-31 16:22:17+00:00,https://twitter.com/marktenenholtz/status/1620457450389504001,"@aakash_rewari I’m saying the best way to learn it is to use it and progressively dive deeper and deeper, learning the math you need as you go rather than upfront"
84,@marktenenholtz,2023-01-31 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1620406596273868801,"Or take this course and, by the 2nd project, you'll have built everything from statistical models on small datasets to ML models on 11M rows of real-world retail transactions data.

Start this way and the fundamental details are 100x easier to learn.

https://t.co/y0DHX1NowF"
85,@marktenenholtz,2023-01-31 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1620406593522376704,"Here's what LightGBM, transformers, and every other SOTA model have in common:

They were derived based on experimentation and intuition, not some mathematical proof.

Most ML courses drown you in technical details before you see how it comes together."
86,@marktenenholtz,2023-01-31 02:37:45+00:00,https://twitter.com/marktenenholtz/status/1620249950281146368,@Splittestingcom Welcome to Austin 🔥
87,@marktenenholtz,2023-01-30 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1620044223612596224,"Some of the most successful “AI-powered” companies sit on top of rolling averages and linear regression.

Y’all shouldn’t be surprised that GPT-3-wrapper-companies are succeeding."
88,@marktenenholtz,2023-01-29 14:31:00+00:00,https://twitter.com/marktenenholtz/status/1619704670100004864,"Data scientists that ship more often are at a massive advantage.

99% of data science projects rely on training data that doesn’t completely match real life.

The only way to figure out if you’ve really built something valuable is to actually try it out."
89,@marktenenholtz,2023-01-28 16:50:24+00:00,https://twitter.com/marktenenholtz/status/1619377362336108551,@Nils_Reimers @DavidMezzetti I always felt like this was one of the big advantages DeBERTa has
90,@marktenenholtz,2023-01-28 14:33:44+00:00,https://twitter.com/marktenenholtz/status/1619342969127206912,"Most ML accuracy metrics make little sense to use if they don’t look something like:

(Revenue generated + costs reduced) - (training cost + inference cost)

Incremental cost is incredibly important!"
91,@marktenenholtz,2023-01-28 00:24:48+00:00,https://twitter.com/marktenenholtz/status/1619129327219769348,@rasbt @ylecun Haha should we start citing Grammarly in papers for editing contributions?
92,@marktenenholtz,2023-01-27 21:01:00+00:00,https://twitter.com/marktenenholtz/status/1619078039446634496,@bentossell That’s where I am! DM’d you
93,@marktenenholtz,2023-01-27 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1618957003203940353,"1% improvements to your model come from:

• Bigger ensemble
• Different architecture
• Hyperparameter tuning

10x improvements come from

• Thorough EDA
• Robust model evaluation
• Meticulous error analysis

Minor changes get minor results."
94,@marktenenholtz,2023-01-26 21:23:13+00:00,https://twitter.com/marktenenholtz/status/1618721244362444801,@ipvkyte Only so many characters available to me
95,@marktenenholtz,2023-01-26 13:00:48+00:00,https://twitter.com/marktenenholtz/status/1618594805730414596,"If I was asked to speedrun a tabular dataset, I would:

1. Start with LightGBM
2. Create basic aggregation features
3. Tune it

Optional:

4. Create a deep learning model
5. Tune it
6. Pick one or the other or ensemble them

A well-built NN might beat a GDBT, but it's more work."
96,@marktenenholtz,2023-01-25 22:01:46+00:00,https://twitter.com/marktenenholtz/status/1618368555787116544,@zacharylipton The ideas that statistical models like ARIMA were built on are now just feature engineered into LightGBM models. The approach is still there!
97,@marktenenholtz,2023-01-25 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1618232229167271937,What's one machine learning topic that you wish was taught better in courses/books?
98,@marktenenholtz,2023-01-24 16:31:07+00:00,https://twitter.com/marktenenholtz/status/1617922959624265728,@svpino Still shows you as following me https://t.co/guiWl8tSKj
99,@marktenenholtz,2023-01-24 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1617869861342056448,"""Ha, stupid Copilot couldn't figure out how to autocomplete that function. I'll just write it myself.""

*10 minutes later*

""Uh... okay let's ask ChatGPT..."""
100,@marktenenholtz,2023-01-23 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1617507467071717376,Do this too much and you'll often find yourself stuck in local minima.
101,@marktenenholtz,2023-01-23 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1617507464290893825,"You've probably been taught to ""keep the GPU warm"" by constantly running experiments to minimize downtime.

That's a bad habit.

You end up spending more time wondering what minor tweak to make and not enough time revisiting your assumptions and trying diverse methods."
102,@marktenenholtz,2023-01-22 18:47:03+00:00,https://twitter.com/marktenenholtz/status/1617232390832668673,"This tweet appears to be quite the Rorschach test. The criticism acts like I wrote ""Kaggle quite literally teaches you everything you need to know"" which I obviously didn't say.

Thanks for perfectly proving my point 😂"
103,@marktenenholtz,2023-01-22 15:15:16+00:00,https://twitter.com/marktenenholtz/status/1617179095053352962,@ChristophMolnar Has anyone ever said that it fully prepares you?
104,@marktenenholtz,2023-01-22 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1617145067361402881,"""Kaggle doesn't translate to real life"" says the data scientist fitting models with an AutoML tool designed by a team full of Kaggle Grandmasters."
105,@marktenenholtz,2023-01-21 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1616782686441607170,What do you do to pass the time while your models train?
106,@marktenenholtz,2023-01-20 16:26:19+00:00,https://twitter.com/marktenenholtz/status/1616472200923463680,"@tunguz @atchimolajuwon Congrats, by my count there hasn't been a Croatian pope since the year 642 😂"
107,@marktenenholtz,2023-01-20 13:21:52+00:00,https://twitter.com/marktenenholtz/status/1616425782179405824,When 👏 I 👏 say 👏 XGBoost 👏 I 👏 also 👏 mean 👏 LightGBM
108,@marktenenholtz,2023-01-20 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1616420296193626112,XGBoost is the greatest low-code ML model ever created.
109,@marktenenholtz,2023-01-19 19:49:51+00:00,https://twitter.com/marktenenholtz/status/1616161031054675970,@tunguz @ArtificalNate @SmokeAwayyy @sujalyadav22007 @goth600 @xlr8harder @MichaelTrazzi @anammostarac @mezaoptimizer @Lan_Dao_ @jenny____r @juliewdesign_ @karpathy @tszzl @_akhaliq @Liff_82 @mrjonfinger @smylkshmn @archillect @EMostaque @TheMoonMidas @zaesarius @Scobleizer @rasbt @fchollet @ylecun @svpino @growing_daniel @joelgrus @NathanLands @LinusEkenstam @DrJimFan @sama 🙏
110,@marktenenholtz,2023-01-19 15:43:46+00:00,https://twitter.com/marktenenholtz/status/1616099101065592834,@amasad inb4 spam email service that auto-detects layoffs. New replit bounty? 😂
111,@marktenenholtz,2023-01-19 15:29:01+00:00,https://twitter.com/marktenenholtz/status/1616095390859665410,@machsci I like that perspective!
112,@marktenenholtz,2023-01-19 13:00:27+00:00,https://twitter.com/marktenenholtz/status/1616058002112679938,"If you're going to build an ML model, you should probably start by creating a monitoring tool for it."
113,@marktenenholtz,2023-01-19 02:35:00+00:00,https://twitter.com/marktenenholtz/status/1615900604882206720,@rasbt Always good to change up the seeds on your folds every once in a while if you’re running a ton of experiments
114,@marktenenholtz,2023-01-18 17:29:21+00:00,https://twitter.com/marktenenholtz/status/1615763285894103065,"@_OliverStanley Nope, it’s all in Pandas"
115,@marktenenholtz,2023-01-18 14:18:19+00:00,https://twitter.com/marktenenholtz/status/1615715209963835393,@svpino The satisfaction of a 10x speedup in my pipeline is like crack
116,@marktenenholtz,2023-01-18 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1615695582991126528,Also reduced the time to load the data into memory from &gt; 10 seconds to 0.3 seconds (on the exact same data) 🤯
117,@marktenenholtz,2023-01-18 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1615695579501432833,"Working on my time-series course and this is always a great feeling.

Below is before and after optimizing my DataFrame for memory. I've done this a million times and it never gets old.

(Less than 6% the original size!) https://t.co/2ep5My7spS"
118,@marktenenholtz,2023-01-17 16:16:46+00:00,https://twitter.com/marktenenholtz/status/1615382633311252480,@micsolana @antoniogm I complained about the original apple earphones for that reason. But the Pros solved that problem for me.
119,@marktenenholtz,2023-01-17 14:37:34+00:00,https://twitter.com/marktenenholtz/status/1615357668612251649,@radekosmulski @kaggle Well done Rakek!
120,@marktenenholtz,2023-01-17 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1615333129954406400,"You're better off investing in data preprocessing, feature engineering, and a debugger than bigger servers and bigger models."
121,@marktenenholtz,2023-01-16 14:50:12+00:00,https://twitter.com/marktenenholtz/status/1614998458024214534,"@dkhundley We’re not 100% sure but it will be during the business day. Don’t worry though, all of the recording will be available and you’ll have access to both me and a TA to answer your questions at all times!"
122,@marktenenholtz,2023-01-16 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1614970781842411521,I believe this will bridge it: https://t.co/y0DHX1NowF
123,@marktenenholtz,2023-01-16 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1614970778965127171,"95% of ML tutorials:

• Curated dataset
• Basic features
• Random 20% k-fold
• Hyperparams that just work

Real life:

• Messy/nonexistent dataset
• Weeks of feature engineering
• Complex cross-validation setup
• Thoroughly tuned hyperparams

We need to bridge this gap."
124,@marktenenholtz,2023-01-15 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1614608375656087557,"It doesn’t matter if your model is:

• GPT-3
• XGBoost
• Stable Diffusion
etc.

The real creativity comes in production.

The system built on top of the model (e.g. rules engines and postprocessing) are often what makes the model incredible."
125,@marktenenholtz,2023-01-15 00:08:53+00:00,https://twitter.com/marktenenholtz/status/1614414279419072514,@bhutanisanyam1 @svpino Thanks Sanyam!
126,@marktenenholtz,2023-01-14 19:28:39+00:00,https://twitter.com/marktenenholtz/status/1614343756966232064,@svpino Thanks for the shoutout!
127,@marktenenholtz,2023-01-14 14:24:10+00:00,https://twitter.com/marktenenholtz/status/1614267130182258690,"Data scientists: ""My back hurts so bad""

Also data scientists, while their models train: https://t.co/zFm18BSnVi"
128,@marktenenholtz,2023-01-13 21:47:35+00:00,https://twitter.com/marktenenholtz/status/1614016331946381341,@quaesita 😂😂😂
129,@marktenenholtz,2023-01-13 20:38:54+00:00,https://twitter.com/marktenenholtz/status/1613999050595962911,"Well, there goes my entire February/March"
130,@marktenenholtz,2023-01-13 15:44:30+00:00,https://twitter.com/marktenenholtz/status/1613924961055285254,@businessbarista Definitely a Neti pot. Saved me when I had COVID
131,@marktenenholtz,2023-01-13 13:51:41+00:00,https://twitter.com/marktenenholtz/status/1613896569039757312,"@svpino Yeah, as it turns out, knowing SQL can benefit how you think even when you’re not using it. Like when you’re using Pandas."
132,@marktenenholtz,2023-01-13 13:00:20+00:00,https://twitter.com/marktenenholtz/status/1613883648541069317,"What can you say to your ML models, but not to your partner?"
133,@marktenenholtz,2023-01-12 16:29:22+00:00,https://twitter.com/marktenenholtz/status/1613573864801402881,@gusthema @corise_ Thanks Gus! 🙏
134,@marktenenholtz,2023-01-12 15:53:01+00:00,https://twitter.com/marktenenholtz/status/1613564713832710145,@machsci Thanks Kirsten 🙏
135,@marktenenholtz,2023-01-12 13:42:51+00:00,https://twitter.com/marktenenholtz/status/1613531959304617985,@j0secyn @corise_ Financial forecasting usually relates to any sort of “top-level” forecast. In this case it will be for retail sales!
136,@marktenenholtz,2023-01-12 13:19:38+00:00,https://twitter.com/marktenenholtz/status/1613526115221479426,@svpino @corise_ Thanks Santiago! 🙏
137,@marktenenholtz,2023-01-12 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1613521255935913985,"Check it out here, with a more detailed outline: https://t.co/y0DHX1NowF"
138,@marktenenholtz,2023-01-12 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1613521253247377408,"To sum it up, you'll learn:

1. High-powered data wrangling
2. Univariate modeling
3. Model evaluation
4. Multi-series modeling
5. Hierarchical modeling
6. Ensembling
7. Deploying your models

And you won't just be listening to me. You'll be building along with real projects!"
139,@marktenenholtz,2023-01-12 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1613521250546245640,"I'll teach you about causal analysis, labor modeling, demand intelligence, and how forecasting models fit into them.

Plus, we'll tie the whole thing together and learn about building pipelines to make it easy to train and re-train your models."
140,@marktenenholtz,2023-01-12 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1613521247639605248,"7. Deploying your models

I'll touch on a bunch of tools you can use to deploy your models, but that's not the end of the story.

How do you integrate your models into your business's decision making processes?"
141,@marktenenholtz,2023-01-12 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1613521244909113345,"6. Ensembling

Data drift frequently kills your forecasting models in production.

Because of this, forecasting models are pretty much always deployed as ensembles.

There are a million ways to do it, but only a few ways that are worth your time."
142,@marktenenholtz,2023-01-12 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1613521242245718019,"5. Hierarchical modeling

Why just use univariate models or ML models when you could use them both?

Once we've built top-level univariate models and low-level ML models, I'll show you how to combine, decompose, aggregate, and reconcile them to squeeze out even more performance!"
143,@marktenenholtz,2023-01-12 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1613521239636873217,"You can't make these models work without great feature engineering.

We'll spend a lot of time understanding how to engineer the best features to get the most out of these models."
144,@marktenenholtz,2023-01-12 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1613521236843433984,"4. Multi-series modeling + feature engineering

What changes when you have many different time-series, like when you're forecasting all of the items at a store?

That's when real ML comes into play!

But, using models like XGBoost, LightGBM, and DeepAR is just the first step."
145,@marktenenholtz,2023-01-12 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1613521234062639104,"3. Model evaluation

Evaluating time-series models is really tough.

• How do you properly backtest them?
• What metrics should you use?
• How do you interpret your models?

We'll go over proper CV, avoiding bad metrics like MAPE, and how to use the best interpretation tools."
146,@marktenenholtz,2023-01-12 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1613521231353085953,"2. Single-series (univariate) modeling

Sure, there are more advanced methods. But ARIMA and friends can produce some great results out of the box.

We'll look at the right situations to use these models, the best packages for them, and also some fun tools like decomposition!"
147,@marktenenholtz,2023-01-12 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1613521228572291077,"Can you (efficiently) calculate time-series features?

Plotting... resampling... and so, so much more.

You'll come out of this module with many, many more tools in your toolbelt.

Bonus: Pandas is slow for big datasets. I'll show you what to use to squeeze out more efficiency."
148,@marktenenholtz,2023-01-12 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1613521225837576194,"1. High-powered data wrangling

It's notoriously difficult to wrangle time-series data.

Packages like Pandas have *tons* of time-series functionality, but it's usually poorly taught. Good info is hard to come by!

Do you know what .reindex() does? How about advanced indexing?"
149,@marktenenholtz,2023-01-12 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1613521223002263554,"My cohort course for time-series forecasting is now official! 🎉

It's time to bring the state-of-the-art to forecasting in a project-based, extremely approachable course.

And thanks to @corise_, you'll have way more resources than just any cohort.

Here's what you'll learn:"
150,@marktenenholtz,2023-01-11 16:05:49+00:00,https://twitter.com/marktenenholtz/status/1613205550950400001,"@BornFitness 37g of coffee beans every morning has fueled many, many hours of training ML models 😂"
151,@marktenenholtz,2023-01-11 15:40:21+00:00,https://twitter.com/marktenenholtz/status/1613199141957685248,@_ScottCondron @matplotlib I can’t help my poor reading comprehension
152,@marktenenholtz,2023-01-11 15:39:45+00:00,https://twitter.com/marktenenholtz/status/1613198988211109892,@_ScottCondron @matplotlib 😭
153,@marktenenholtz,2023-01-11 15:39:21+00:00,https://twitter.com/marktenenholtz/status/1613198887606538240,@story645 @matplotlib @kamranali305 It’s awesome that you have such a positive perspective. Folks like you are why projects like matplotlib can thrive for so long 💪
154,@marktenenholtz,2023-01-11 15:08:48+00:00,https://twitter.com/marktenenholtz/status/1613191199879299073,"@matplotlib @kamranali305 What I said:

""I have poor reading comprehension""

Where folks took it:

""Matplotlib sucks!""

:("
155,@marktenenholtz,2023-01-11 13:00:22+00:00,https://twitter.com/marktenenholtz/status/1613158879092969472,"XGBoost/LightGBM is not all you need.

…but they’re probably the fastest way to a good solution on tabular data."
156,@marktenenholtz,2023-01-11 00:00:00+00:00,https://twitter.com/marktenenholtz/status/1612962493487251457,@_ScottCondron The message within this tweet: despite the fact that I can’t remember how to use it… I still use it
157,@marktenenholtz,2023-01-10 15:53:43+00:00,https://twitter.com/marktenenholtz/status/1612840118036856834,@Suhail I keep this keyboard handy for my matplotlib code https://t.co/aXghn1ahVy
158,@marktenenholtz,2023-01-10 15:29:16+00:00,https://twitter.com/marktenenholtz/status/1612833963009097730,@ChristophMolnar ChatGPT is my business analyst intern now
159,@marktenenholtz,2023-01-10 15:28:48+00:00,https://twitter.com/marktenenholtz/status/1612833846399225859,"@Rob_Mulla I always forget little things, like is it plt.xlabel or plt.xtitle? It's dumb... but so am I"
160,@marktenenholtz,2023-01-10 13:57:25+00:00,https://twitter.com/marktenenholtz/status/1612810849265106946,@KennStack01 Forced memorization is dumb. You’ll memorize it naturally over time if it’s important enough
161,@marktenenholtz,2023-01-10 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1612796433245310977,"Hello, I've been using Matplotlib for 7+ years and I still google how to do everything except plt.plot()"
162,@marktenenholtz,2023-01-09 14:16:37+00:00,https://twitter.com/marktenenholtz/status/1612453294252896257,"Engineers usually solve problems with exact solutions, while data scientists usually solve problems with inexact solutions."
163,@marktenenholtz,2023-01-09 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1612434076799238147,"For every tree I cut down, I train a new (gradient boosted) one."
164,@marktenenholtz,2023-01-08 22:00:00+00:00,https://twitter.com/marktenenholtz/status/1612207518997962752,@svpino Was a fun topic to chat about! Great work!
165,@marktenenholtz,2023-01-08 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1612071650639642625,"My ML modeling trinity:

1. Tabular: XGBoost/LightGBM
2. Text: @huggingface Transformers
3. Vision: @wightmanr's timm

I can solve 99% of the problems I come across using these libraries."
166,@marktenenholtz,2023-01-08 00:19:44+00:00,https://twitter.com/marktenenholtz/status/1611880296038670337,@alexgraveley Wow I didn’t even know that setting existed. You just saved me.
167,@marktenenholtz,2023-01-07 14:55:28+00:00,https://twitter.com/marktenenholtz/status/1611738293917593601,"@bernhardsson I still prefer the environment management capabilities. But, in reality, I mostly use mamba because it’s way faster"
168,@marktenenholtz,2023-01-07 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1611709283305148416,"Data scientists often forget:

Your job isn't always to outperform humans.

Often, it's just to automate a boring task to free a human up to do something more productive. The human is usually the most expensive part.

Think more about utility than pure accuracy."
169,@marktenenholtz,2023-01-06 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1611346900938264576,"Going through old code is like going through a family picture album.

“What’s the story behind this code?”

“Oh… &lt;soft chuckle&gt; I remember that day. You see, back in the day, documentation for that library was much harder to find, and I had just been up for 20 hour straight…”"
170,@marktenenholtz,2023-01-05 13:00:29+00:00,https://twitter.com/marktenenholtz/status/1610984581326573570,"Check out the repo here: https://t.co/qFUw5bf82r

Thanks to the wonderful folks with CarperAI!"
171,@marktenenholtz,2023-01-05 13:00:28+00:00,https://twitter.com/marktenenholtz/status/1610984577396535300,"There’s now a Python library for RLHF called TRLX!

(The same reinforcement learning strategy used in training ChatGPT)

It works well with Hugging Face models, supports multiple RL strategies, and requires very little code! https://t.co/is1erGjHgM"
172,@marktenenholtz,2023-01-05 02:16:46+00:00,https://twitter.com/marktenenholtz/status/1610822586270720001,@karpathy How do you feel about VSCode’s debugger?
173,@marktenenholtz,2023-01-04 13:50:01+00:00,https://twitter.com/marktenenholtz/status/1610634660173778946,This is an example of a good reason:
174,@marktenenholtz,2023-01-04 13:00:39+00:00,https://twitter.com/marktenenholtz/status/1610622234955247616,"Python is 100%, no doubt, a slow language.

But, 95%+ of your ML code probably runs in C anyways.

Unless you have a really good reason for switching, you probably won't be able to make up for the huge increase in development speed that Python offers."
175,@marktenenholtz,2023-01-03 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1610259732392194048,Every ML team should maintain a Kaggle-style leaderboard for every common ML task they work on.
176,@marktenenholtz,2023-01-02 18:54:45+00:00,https://twitter.com/marktenenholtz/status/1609986571675045888,"@SilverDeGeneral There’s often many teams using the same data sources, so it depends on your org"
177,@marktenenholtz,2023-01-02 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1609897361287045122,"• Model (probably ensemble) makes predictions
• Guardrails and heuristics applied
• Output QA tests
• Predictions written to table
• Predictions served to customer

And that’s just a fairly typical (maybe even kinda simplified) inference pipeline."
178,@marktenenholtz,2023-01-02 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1609897358522978305,"What beginners think ML pipelines are:

• Data comes in
• Model makes predictions
• Done

What they actually are:

• Raw data comes in
• Goes through multiple transformation layers
• Quality checks, anomaly detection, etc
• Feature engineering

(continued)"
179,@marktenenholtz,2023-01-02 00:52:30+00:00,https://twitter.com/marktenenholtz/status/1609714216432271362,"@Suhail I’ve used it more times than I can count, feel free"
180,@marktenenholtz,2023-01-01 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1609534959248896002,"GitHub Copilot’s internals (allegedly) contain:

• Logistic regression for prompt filtering
• Complex telemetry for model evaluation
• Jaccard matching for candidate selection

All of that (and a lot more) to make GPT-3 go.

Trust me, it’ll take GPT-62 before ML jobs are gone."
181,@marktenenholtz,2023-01-01 02:20:42+00:00,https://twitter.com/marktenenholtz/status/1609374024282558472,"@MeganRisdal Whatever dish you make, I'd recommend sauteeing them and seasoning the crap out of them. You can probably completely remove the mushroom-y taste and the texture."
182,@marktenenholtz,2023-01-01 02:00:19+00:00,https://twitter.com/marktenenholtz/status/1609368893029601282,@Suhail @karpathy @johnowhitaker Its*
183,@marktenenholtz,2023-01-01 02:00:00+00:00,https://twitter.com/marktenenholtz/status/1609368813769736196,"@Suhail @karpathy @johnowhitaker PyTorch Lightning can feel like a lot of boilerplate itself, but it offers a lot more portability once you’ve written it. It’s structure is very powerful"
184,@marktenenholtz,2023-01-01 00:27:30+00:00,https://twitter.com/marktenenholtz/status/1609345537202962434,"@Splittestingcom Mostly just Apple Podcasts and Spotify. They try, but they really struggle recommending individual episodes"
185,@marktenenholtz,2022-12-31 21:11:14+00:00,https://twitter.com/marktenenholtz/status/1609296143816572929,@Splittestingcom Podcast recommender systems are honestly pretty bad in my experience. I’d use it for that.
186,@marktenenholtz,2022-12-31 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1609172600168583169,"Deep learning models in perspective:

Your brain has 86 billion neurons.

If you laid them all out, end-to-end…

You would die."
187,@marktenenholtz,2022-12-31 03:44:14+00:00,https://twitter.com/marktenenholtz/status/1609032658075492352,@radekosmulski 👀
188,@marktenenholtz,2022-12-31 01:46:00+00:00,https://twitter.com/marktenenholtz/status/1609002902613331969,"@tunguz A lot of people are complaining about me saying that Word2Vec doesn’t care about context.

I was trying to ELI5, but really it’s that it can’t flexibly incorporate context. CBOW kinda does that, but not in any way that adapts to the sentence at hand"
189,@marktenenholtz,2022-12-30 20:48:13+00:00,https://twitter.com/marktenenholtz/status/1608927964590374913,@alexgraveley @marcosvgjac Haha yes. Just mean pool the token embeddings and slap XGBoost/SVM on that vector
190,@marktenenholtz,2022-12-30 20:46:00+00:00,https://twitter.com/marktenenholtz/status/1608927406261440513,"@alexgraveley @marcosvgjac All sorts of text classification tasks. It’s great because it doesn’t require the underlying transformer to be finetuned for the task at hand, as long as it provides decent embeddings.

Works for image classification too."
191,@marktenenholtz,2022-12-30 20:29:57+00:00,https://twitter.com/marktenenholtz/status/1608923364328079361,"@marcosvgjac @alexgraveley Yes, but I misunderstood him at first. Alex is actually suggesting that you use ChatGPT to help with things like feature engineering rather than a traditional boosting approach"
192,@marktenenholtz,2022-12-30 19:46:27+00:00,https://twitter.com/marktenenholtz/status/1608912418633306114,@alexgraveley I’ve been mulling around the interactions between GPT and tabular data for a bit now. Seems like an interesting challenge to get it to work well with wide datasets
193,@marktenenholtz,2022-12-30 19:17:22+00:00,https://twitter.com/marktenenholtz/status/1608905100172595201,"@alexgraveley Not personally, but I’ve used XGBoost as a boosting model on top of transformers many times.

Actually, I sometimes get better results using SVMs (using cuML) on top of transformers."
194,@marktenenholtz,2022-12-30 19:12:36+00:00,https://twitter.com/marktenenholtz/status/1608903899930558464,@tunguz 😇
195,@marktenenholtz,2022-12-30 19:11:13+00:00,https://twitter.com/marktenenholtz/status/1608903553380413442,"@tunguz Extra details:

1. “Self-attention” means the context is the sentence the word is in. The other attention uses context from a different sentence

2. The relevant context is a weighted average of all the words in the context, weighted by their similarity to the current word"
196,@marktenenholtz,2022-12-30 19:09:05+00:00,https://twitter.com/marktenenholtz/status/1608903014672379905,"@tunguz Using attention, which goes something like this:

1. Look at a word’s representation
2. Look at the context around that word (i.e. the sentence it’s in)
3. Grab the most relevant part of the sentence and dot product it with the word’s vector"
197,@marktenenholtz,2022-12-30 18:35:03+00:00,https://twitter.com/marktenenholtz/status/1608894449547104258,"@tunguz Word2Vec has one representation for every word, and doesn’t care about the context surrounding the word (that’s up to the user).

Transformers learn how to mix up/pass the context of the sentence around to each word. Each word essentially becomes enriched with context."
198,@marktenenholtz,2022-12-30 18:29:17+00:00,https://twitter.com/marktenenholtz/status/1608893000440250368,@amasad Implying that OpenAI has made zero innovations is… quite something
199,@marktenenholtz,2022-12-30 18:00:10+00:00,https://twitter.com/marktenenholtz/status/1608885671120482305,Link here: https://t.co/2cAG2HVUPd
200,@marktenenholtz,2022-12-30 18:00:09+00:00,https://twitter.com/marktenenholtz/status/1608885666838114310,"GitHub Copilot is a genuinely brilliant piece of engineering sitting on top of GPT-3.

Here's how it works, courtesy of @parth007_96.

Huge shoutout to @alexgraveley and the Copilot team. Well done 👏 https://t.co/NJJOOcmVii"
201,@marktenenholtz,2022-12-30 16:28:09+00:00,https://twitter.com/marktenenholtz/status/1608862513109815299,"@svpino Agreed. I would instead say “know what a derivative is and know some basic linear algebra.” 

But I would still say you should toy around with them first so that you can figure out what you don’t know before you go on aimlessly learning math."
202,@marktenenholtz,2022-12-30 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1608810170917982210,"Feature interactions is a bit of a misleading term in this sense.

One of the most valuable methods of feature engineering for GBDT models are difference features, multiplicative features, aggregated features, etc. for exactly this reason."
203,@marktenenholtz,2022-12-30 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1608810168250437635,"Common XGBoost misconception:

Incorrect: ""XGBoost handles all feature interactions for you!""

Correct: It handles *some* for you.

XGBoost DOES know what to do when, say, feature_1 &gt; 30 and feature_2 &lt; 10.

But it CAN'T tell you what to do when feature_1 - feature_2 &gt; 100."
204,@marktenenholtz,2022-12-29 23:03:51+00:00,https://twitter.com/marktenenholtz/status/1608599708397887493,@Aella_Girl Compounded by your lack of credentials (stupid reason) and that they don’t like you (another stupid reason)
205,@marktenenholtz,2022-12-29 23:02:57+00:00,https://twitter.com/marktenenholtz/status/1608599483579002882,"@Aella_Girl They’re critiquing it because there’s social capital in the stats/data science world associated with tearing down (or at least doubting) others’ work.

Getting a statistician to say “hmm, that looks maybe okay” is a glowing endorsement."
206,@marktenenholtz,2022-12-29 19:45:43+00:00,https://twitter.com/marktenenholtz/status/1608549847283695616,Me checking my water cooler after running my 1578332nd stable diffusion prompt of the day  https://t.co/CrI3JLUjCW
207,@marktenenholtz,2022-12-29 17:49:30+00:00,https://twitter.com/marktenenholtz/status/1608520598870986752,@chrisalbon Remember when data science was the cool term?
208,@marktenenholtz,2022-12-29 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1608447769425104899,What is XGBoost? Wrong answers only.
209,@marktenenholtz,2022-12-29 02:18:49+00:00,https://twitter.com/marktenenholtz/status/1608286383939919880,@tunguz @RampCapitalLLC I tried it for 5 minutes and... that was painful
210,@marktenenholtz,2022-12-28 17:37:54+00:00,https://twitter.com/marktenenholtz/status/1608155291874635784,Me training models in Tensorflow vs. PyTorch https://t.co/0vOpgb8SiV
211,@marktenenholtz,2022-12-28 17:03:38+00:00,https://twitter.com/marktenenholtz/status/1608146669991763968,@amasad You mean the “how well did you overfit on ImageNet” system?
212,@marktenenholtz,2022-12-28 14:36:42+00:00,https://twitter.com/marktenenholtz/status/1608109693641031686,@tunguz You take that back
213,@marktenenholtz,2022-12-28 13:00:22+00:00,https://twitter.com/marktenenholtz/status/1608085448316059655,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/3zQt1utof9"
214,@marktenenholtz,2022-12-28 13:00:21+00:00,https://twitter.com/marktenenholtz/status/1608085445648449538,"Incorporating those 9 tips has been a game-changer for me. I hope you find value in them!

Follow me @marktenenholtz to get more high-signal ML content!"
215,@marktenenholtz,2022-12-28 13:00:21+00:00,https://twitter.com/marktenenholtz/status/1608085442892795905,"9. Pipelines &gt; models

Most of the code that goes into training ML models is written either for getting the data to the model or getting the predictions out.

You should write convenience functions/pipeline steps that transform your data or model outputs as often as possible."
216,@marktenenholtz,2022-12-28 13:00:20+00:00,https://twitter.com/marktenenholtz/status/1608085440145551360,"8. Refactor, refactor, refactor

ML code can get out of hand really quickly.

With rapid iteration comes a rapid mess.

Don't be afraid to spend 30 minutes cleaning up your code instead of training models."
217,@marktenenholtz,2022-12-28 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1608085437406662658,"7. Create your own framework

99% of my projects follow the exact same structure.

Down to the names of the files, what goes in them, and when they are created. I make heavy use of copy-paste.

I find I work infinitely faster in a codebase that I understand inside-out."
218,@marktenenholtz,2022-12-28 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1608085434583715842,"6. Reuse old Kaggle solutions

Kaggle solutions are like a database for battle-tested solutions.

If you have a similar enough problem to an old competition, those solution writeups are an absolute goldmine.

My first stop on any problem for ""literature review"" is Kaggle."
219,@marktenenholtz,2022-12-28 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1608085431752757248,"5. Try one thing at a time

Every experiment you run should be a single change to your previous runs.

If you change two things at once and it improves your model, you can't really tell what caused the improvement.

This is a very common source of wasted GPU-hours."
220,@marktenenholtz,2022-12-28 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1608085429072564225,"If I use a model like a random forest or DistilBERT, it's either because I have a really good reason (i.e. downstream deployment), or because I'm just debugging my pipeline.

Other than that, just use the strong, reliable ones."
221,@marktenenholtz,2022-12-28 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1608085426300145669,"I realized, though, it just distracts me from the important data work.

Random forests aren't much less hyperparameter-dependent than XGBoost (if you know how to use it).

Many feature engineering strategies on smaller language models don't work on the larger variants."
222,@marktenenholtz,2022-12-28 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1608085423527718912,"4. Skip to the good models

Despite my obvious love for XGBoost, DeBERTa, and other state-of-the-art models, I still sorta bought into the idea of using simpler baseline models like Random Forests.

(Obviously excluding specific applications like regression analysis)"
223,@marktenenholtz,2022-12-28 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1608085420738506752,"3. Can't use a GPU? Use a better DataFrame.

Polars was a recent addition of mine that I've come to really like.

It's fast, multithreaded, memory efficient, and has a lovable API!

Check it out here: https://t.co/FEkx8d2Ai0"
224,@marktenenholtz,2022-12-28 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1608085418033164290,"2. Use a GPU wherever you can

@RAPIDSai is an absolute blessing.

For 95% of tabular problems, I can change 2 lines of code to make the whole thing run on my GPU.

Everything from feature engineering to model training.

I've seen instances where this speeds pipelines up by 50x."
225,@marktenenholtz,2022-12-28 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1608085415319437312,"I realized that having a reliable CV setup *before* even setting up the pipeline can also help you debug your evaluation metrics at the same time.

It's hard to know if your pipeline is broken if you can't trust your evaluation metrics!"
226,@marktenenholtz,2022-12-28 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1608085412601548801,"1. Model evaluation is the best time saver

Experiments done with a bad CV setup are a waste of time.

Previously, I focused on getting a pipeline together before worrying about it.

Of course, that would lead to running a few experiments with poorly conceived evaluation."
227,@marktenenholtz,2022-12-28 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1608085409866866688,"I've been doing a year-end review of my completed data science projects.

This time, my goal was to find the tools and methods I added to work more efficiently.

Here's what I changed to work 10x more efficiently:"
228,@marktenenholtz,2022-12-27 21:04:35+00:00,https://twitter.com/marktenenholtz/status/1607844918457950209,@realGeorgeHotz And why didn't he use the elder wand to repair his own wand?
229,@marktenenholtz,2022-12-27 18:37:57+00:00,https://twitter.com/marktenenholtz/status/1607808015062663173,"@dvassallo I leave the predictions to my models. 

The more predictions I make about the world, the more biased I become towards those outcomes actually happening."
230,@marktenenholtz,2022-12-27 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1607723023720943622,"Plus, in the modern environment, there is so much access to the basics (which is what a Master’s teaches you) that it’s almost nauseating.

I’d rather teach that myself or on the job.

Working with a skeptical stakeholder is a much better early career teacher."
231,@marktenenholtz,2022-12-27 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1607723020797476872,"Requiring an advanced degree for applied ML jobs is doubly counterproductive.

The learning curve is actually in trying and failing to produce something that provides business value.

Forcing people to get an advanced degree first just pushes that further down the line."
232,@marktenenholtz,2022-12-26 13:13:36+00:00,https://twitter.com/marktenenholtz/status/1607364002820022274,"@mattvvolf Haha more like try, fail, learn 20 times before you succeed"
233,@marktenenholtz,2022-12-26 13:12:14+00:00,https://twitter.com/marktenenholtz/status/1607363661307207680,"@Abdel_Bentorcha Yes, but I can tell you it’s easier said than done.

You have to make many assumptions along the way which get baked into your pipeline. Those assumptions can often be wrong/misguided. 

Plus, rigid pipelines can cause you to miss other, quite different solutions."
234,@marktenenholtz,2022-12-26 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1607360599083880449,"ML is all about slogging through messy data, inexact solutions, crazy difficult model evaluation, and gloriously complex code

only to realize 3 months later that your model actually sucks, at which point you build one 10x more performant and you’re permanently better off for it."
235,@marktenenholtz,2022-12-26 02:18:53+00:00,https://twitter.com/marktenenholtz/status/1607199240291323904,@tunguz Holy crap
236,@marktenenholtz,2022-12-26 00:07:52+00:00,https://twitter.com/marktenenholtz/status/1607166266766155778,@Aella_Girl I feel like it’s always WSW or ESE
237,@marktenenholtz,2022-12-25 18:23:29+00:00,https://twitter.com/marktenenholtz/status/1607079598721843201,@Mlbot4 😂 lucidrains is a legend
238,@marktenenholtz,2022-12-25 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1606998246651760646,Check out the repo here: https://t.co/tL768IevAq
239,@marktenenholtz,2022-12-25 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1606998242491002880,"And finally, putting it all together: https://t.co/BRVueKC6Xp"
240,@marktenenholtz,2022-12-25 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1606998231803887618,"Next, here’s where you train the reward model: https://t.co/UyWaGbtFdC"
241,@marktenenholtz,2022-12-25 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1606998220915486720,"Someone implemented a framework to allow you to train PaLM (Google’s 540B parameter LM) using the same reinforcement learning strategy as ChatGPT!

Open source strikes again.

To start, here’s the part where you initially pre-train the model: https://t.co/leJZoOw0mh"
242,@marktenenholtz,2022-12-24 15:03:27+00:00,https://twitter.com/marktenenholtz/status/1606666872220520448,@meetdliu Follow @radekosmulski
243,@marktenenholtz,2022-12-24 14:57:03+00:00,https://twitter.com/marktenenholtz/status/1606665260638797824,"Right now, there’s tons of roundabout methods for solving it:

1. Bi-encoders and pairwise distance
2. Candidate re-rank models
3. Cross encoders
etc.

These methods are either very cumbersome to maintain or pretty finicky to tune."
244,@marktenenholtz,2022-12-24 14:54:16+00:00,https://twitter.com/marktenenholtz/status/1606664559695171584,@GaryMarcus The tweet is a joke
245,@marktenenholtz,2022-12-24 14:52:49+00:00,https://twitter.com/marktenenholtz/status/1606664198255046658,"Everyone is talking about search, but IMO the next field to be disrupted is simply recommender systems in general."
246,@marktenenholtz,2022-12-24 13:00:20+00:00,https://twitter.com/marktenenholtz/status/1606635890511532032,https://t.co/tRd465HJGr
247,@marktenenholtz,2022-12-24 02:03:43+00:00,https://twitter.com/marktenenholtz/status/1606470645008814080,"@tszzl The worry in my mind is, in the case that it doesn’t lead to AGI in the short term, we’ve wasted a shitload of time progressing on getting, say, a powerful forecast in every business’ hands.

That itself could save trillions of $$$ in retail alone."
248,@marktenenholtz,2022-12-23 21:42:46+00:00,https://twitter.com/marktenenholtz/status/1606404974690406402,@jleplat Tell me how many people like Rust and I'll tell you
249,@marktenenholtz,2022-12-23 21:06:51+00:00,https://twitter.com/marktenenholtz/status/1606395937529765889,@tunguz Oh lawd
250,@marktenenholtz,2022-12-23 20:37:01+00:00,https://twitter.com/marktenenholtz/status/1606388427867492354,"@tszzl investor: ""okay okay these look good. but how about your rizz?"""
251,@marktenenholtz,2022-12-23 20:27:55+00:00,https://twitter.com/marktenenholtz/status/1606386140990345229,@realGeorgeHotz Twitter's recsys team simply found out how to sort by tweet embedding similarity to a hellscape. Genius
252,@marktenenholtz,2022-12-23 19:43:25+00:00,https://twitter.com/marktenenholtz/status/1606374939887652864,"@levelsio They don’t realize just how many of the tools they use are already powered by that “AI that’s not good enough.” 

Just because it’s not “one click and go” to deploy doesn’t mean smart people haven’t figured out how to build guardrails."
253,@marktenenholtz,2022-12-23 18:15:27+00:00,https://twitter.com/marktenenholtz/status/1606352802204221445,JavaScript could have been the language of ML in another universe send tweet
254,@marktenenholtz,2022-12-23 18:00:06+00:00,https://twitter.com/marktenenholtz/status/1606348940017733632,99% of you only like the programming language you use because you already know it send tweet
255,@marktenenholtz,2022-12-23 17:15:50+00:00,https://twitter.com/marktenenholtz/status/1606337801578070016,@PWengerden @andrey_kurenkov Don’t encourage me
256,@marktenenholtz,2022-12-23 16:56:08+00:00,https://twitter.com/marktenenholtz/status/1606332843092631552,"@gdb I’ve been trying to learn more things first by starting top-down (i.e. end-to-end) and then drilling down.

Then once I’ve done that, go the other direction. Start from the smallest building blocks and work my way back up.

It’s like a superpower!"
257,@marktenenholtz,2022-12-23 13:04:12+00:00,https://twitter.com/marktenenholtz/status/1606274474445115394,@avikumart_ print is the greatest debugger ever released
258,@marktenenholtz,2022-12-23 13:03:31+00:00,https://twitter.com/marktenenholtz/status/1606274302520659968,"“I’ll use the debugger next time”

“…okay one more time”

“Oh wait I know what it was… nope one more time”"
259,@marktenenholtz,2022-12-23 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1606273497390403584,"Do I have access to a debugger?

Yes.

Do I know how to use a debugger?

Absolutely.

So how do I debug my code?

print('here')
...
print('here2')
...
print('here3')
...
print('here4')"
260,@marktenenholtz,2022-12-22 22:49:33+00:00,https://twitter.com/marktenenholtz/status/1606059394919063552,@Kmat67916008 Congrats! You’re truly dominant in video competitions
261,@marktenenholtz,2022-12-22 18:18:16+00:00,https://twitter.com/marktenenholtz/status/1605991124090253312,"How to learn Python and ML in 4 weeks:

Step 1: Slow the Earth's rotation by 26x
Step 2: Spend 4 weeks learning Python and ML

Congrats! You learned it in 2 years. Like everyone else!"
262,@marktenenholtz,2022-12-22 17:51:55+00:00,https://twitter.com/marktenenholtz/status/1605984491901579265,@tunguz Someday 🥲 https://t.co/R6F3PLMQnm
263,@marktenenholtz,2022-12-22 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1605911104235765760,"I hope this list inspired you!

Follow me @marktenenholtz for more high-signal ML content.

I'm also relaunching my newsletter so I can do more long-form content again. I miss it!

Subscribe to it here: https://t.co/3zQt1uKrh9"
264,@marktenenholtz,2022-12-22 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1605911101706682368,"This is by no means a comprehensive list.

Got something else? Reply to the first tweet and I’ll add it to the list."
265,@marktenenholtz,2022-12-22 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1605911099127132161,"Stacking is certainly a form of feature engineering!

You can add predictions from a baseline model (like linear regression).

A common technique in RecSys is adding similarity features even from a neural network-type model like Word2Vec."
266,@marktenenholtz,2022-12-22 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1605911096572858369,"Try running PCA on your data and either building a model on those features or adding the PCA features to your original dataset.

(or any other dimensionality-reduction technique)"
267,@marktenenholtz,2022-12-22 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1605911094060408833,Try running clustering like k-means on your data and adding each row’s cluster label as a feature.
268,@marktenenholtz,2022-12-22 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1605911091132764160,"Have group fields like product hierarchy data, dates, cities, etc.?

Try creating aggregations on those groups (mean, median, stddev, etc.)

This is often where most of your opportunities lie. You can slice-and-dice grouped data in many ways."
269,@marktenenholtz,2022-12-22 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1605911088301686786,"Create interaction terms between columns, such as subtracting or multiplying them.

Tree-based models have a hard time picking up on these sorts of interactions."
270,@marktenenholtz,2022-12-22 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1605911085487230976,"Try splitting categorical data into multiple fields, or combining multiple categorical fields into one."
271,@marktenenholtz,2022-12-22 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1605911082878451712,"If your data has a time-series aspect, try creating rolling window calculations (usually mean and standard deviation are sufficient) as well as lag features."
272,@marktenenholtz,2022-12-22 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1605911080126910464,"If you have a date column, extract all of the useful components, like day of week, week of year, hour of day, etc.

Similarly, addresses can be decomposed into state, city, latitude, longitude, etc."
273,@marktenenholtz,2022-12-22 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1605911077434167296,"Now let’s talk about creating features.

This is a bit less structured, and a lot more about having a large arsenal of ideas and deploying them as necessary."
274,@marktenenholtz,2022-12-22 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1605911074649227264,"For continuous data:

If you’re using a tree-based model, you probably don’t need to do anything unless you're linear boosting.

Otherwise, try standardizing or normalizing.

If a variable has a weird distribution, try a log or sqrt transformation, or even something like Box-Cox."
275,@marktenenholtz,2022-12-22 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1605911071998386177,"For categorical data:

Using a GBDT/RF?  Try label encoding.

Linear model/SVM/similar model? Try dummy encoding.

Using a neural net? Try label encoding and using embedding layers."
276,@marktenenholtz,2022-12-22 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1605911069121052672,"Once you’ve imputed, consider adding a corresponding binary column that indicates whether the other column was missing in that row.

Now let’s talk transforming values:"
277,@marktenenholtz,2022-12-22 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1605911066482905088,"Option #3: Fall back to a default method

For continuous features:
• Filling with the mean
• Filling with the median
• Filling with zero

For categorical features:
• Filling with the mode
• Filling with a negative value (for tree models)
• Frequency encoding"
278,@marktenenholtz,2022-12-22 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1605911063462977536,"Option #2: Let your model handle it!

Many popular gradient-boosting libraries like XGBoost or LightGBM can handle NaN’s, so you can try that too.

This often provides the best performance."
279,@marktenenholtz,2022-12-22 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1605911060673810432,"Option #1: Fill them with a value that makes sense

If ""NaN sales"" means there was simply an absence of transactions, then filling with zero makes perfect sense."
280,@marktenenholtz,2022-12-22 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1605911057859428354,"Note that the most important thing is to always start with what makes sense from your EDA and error analysis.

Also, make sure to cross-validate anything you try here to see if it actually helps!

Let’s get started with NaN handling:"
281,@marktenenholtz,2022-12-22 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1605911055170822145,"Feature engineering is the most important part of building great models for tabular data.

I revisited dozens of tabular ML projects I worked on in the past and distilled the techniques I used down to repeatable, powerful processes.

Here's what I found:"
282,@marktenenholtz,2022-12-21 21:32:45+00:00,https://twitter.com/marktenenholtz/status/1605677679306412032,"@svpino I’ve always felt like take-homes give the best chance for candidates to set themselves apart.

I’ve never heard of them taking more than a couple of hours."
283,@marktenenholtz,2022-12-21 20:45:34+00:00,https://twitter.com/marktenenholtz/status/1605665807068979201,"@amasad The problem most orgs have is that they require the selling to be done at the engineer level and not at the Chief Architect level.

ML can often be a bit different, though. Most model evaluation processes should be able to work themselves into an ROI calculation."
284,@marktenenholtz,2022-12-21 20:34:20+00:00,https://twitter.com/marktenenholtz/status/1605662980514799616,The year is 2022 and it's still not 100% clear whether it's legal for models pre-trained on ImageNet (released in 2010) to be used commercially.
285,@marktenenholtz,2022-12-21 18:30:04+00:00,https://twitter.com/marktenenholtz/status/1605631703749468160,@GergelyOrosz You don’t think he’s worked on a complex system before?
286,@marktenenholtz,2022-12-21 18:22:57+00:00,https://twitter.com/marktenenholtz/status/1605629914291412994,"@GergelyOrosz I think his complaint was that the code base was swimming in tech debt, not that it was hard to learn"
287,@marktenenholtz,2022-12-21 18:00:13+00:00,https://twitter.com/marktenenholtz/status/1605624195810938881,"If you don't believe me, read Effective Pandas by @__mharrison__"
288,@marktenenholtz,2022-12-21 18:00:13+00:00,https://twitter.com/marktenenholtz/status/1605624193281712129,"Almost every data science tool needs to focus less on beginner tutorials and more on examples of how power-users make use of it.

99% of folks using them find the first way of making something work and stick to it forever (because they don't know better solutions exist)."
289,@marktenenholtz,2022-12-21 17:42:44+00:00,https://twitter.com/marktenenholtz/status/1605619795382046720,@tunguz @DSaience NAME NAMES
290,@marktenenholtz,2022-12-21 17:30:59+00:00,https://twitter.com/marktenenholtz/status/1605616835545669633,"@DSaience @tunguz When I was in mechanical engineering, that was all of the software. It was unbearable"
291,@marktenenholtz,2022-12-21 17:27:06+00:00,https://twitter.com/marktenenholtz/status/1605615859946307584,@tunguz Actually I’d say working with proprietary software that’s poorly documented and not easily googleable is the worst
292,@marktenenholtz,2022-12-21 15:41:22+00:00,https://twitter.com/marktenenholtz/status/1605589251017494529,"@sh_reya To me, it seems like his point is “if you use someone else’s API, you remove a lot of tech debt.”

Hasn’t that always been the case? I don’t think anything has changed in that regard. The focus of the paper was developing ML systems.

Even FMs usually need to be finetuned."
293,@marktenenholtz,2022-12-21 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1605548702700957698,The world if data scientists used 10% less technical jargon https://t.co/DvxTewC0AL
294,@marktenenholtz,2022-12-21 02:38:36+00:00,https://twitter.com/marktenenholtz/status/1605392261797535744,@growing_daniel Looks like he had a wild night testing it
295,@marktenenholtz,2022-12-20 20:04:07+00:00,https://twitter.com/marktenenholtz/status/1605292985914216477,"@svpino @ylecun I think something like a chat librarian that goes and find sources for you would be a much better interface for the foreseeable future (which is a relatively small UX change to Google). 

I think we overestimate how often there is a “right answer” to the questions we’re asking."
296,@marktenenholtz,2022-12-20 17:55:56+00:00,https://twitter.com/marktenenholtz/status/1605260729224929289,@PiazzaRobert I'll just ask ChatGPT what my parameters should be
297,@marktenenholtz,2022-12-20 17:55:25+00:00,https://twitter.com/marktenenholtz/status/1605260598450724873,"@TrungTPhan ""hey Tom, we've got this massive exoskeleton for you to wear. But it's really heavy.""

""idk man...""

""it'll make you look taller""

""say no more fam"""
298,@marktenenholtz,2022-12-20 17:46:45+00:00,https://twitter.com/marktenenholtz/status/1605258416481509387,"@Scobleizer brb, buying the domain https://t.co/yhA6Br9CEY"
299,@marktenenholtz,2022-12-20 17:42:25+00:00,https://twitter.com/marktenenholtz/status/1605257325568294913,@tunguz those gol' dern free-range algorithms
300,@marktenenholtz,2022-12-20 17:41:41+00:00,https://twitter.com/marktenenholtz/status/1605257139735388160,"@ylecun What I don't understand is why we're acting like ChatGPT and Google Search perform the same function.

When I want to search for something, I want options and I want to be able to see context and browse around where it came from. ChatGPT is really doing something else."
301,@marktenenholtz,2022-12-20 17:37:24+00:00,https://twitter.com/marktenenholtz/status/1605256063237890061,@TheZachMueller Is there an advantage to dataclasses over something like SimpleNamespace?
302,@marktenenholtz,2022-12-20 17:35:53+00:00,https://twitter.com/marktenenholtz/status/1605255680885415937,@rosikand Interesting. Have you ever used SimpleNamespace?
303,@marktenenholtz,2022-12-20 17:33:16+00:00,https://twitter.com/marktenenholtz/status/1605255025034420233,What is everyone using for configuration files nowadays?
304,@marktenenholtz,2022-12-20 14:37:12+00:00,https://twitter.com/marktenenholtz/status/1605210715467534340,"@rasbt Anyone who thinks the problem is with search itself, try using another search engine like @YouSearchEngine. 

There are plenty of things to be done to make search itself way better than ChatGPT, and making those commercially viable is easier than making LLMs trustworthy (for now)"
305,@marktenenholtz,2022-12-20 14:33:19+00:00,https://twitter.com/marktenenholtz/status/1605209736139444225,@tunguz You mean jax_prod_tools?
306,@marktenenholtz,2022-12-20 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1605186286800621570,"Our tools for actually defining/training the models have gotten pretty good.

The current set of tools in progress are making the developer experience much better.

And the next set of tools will bring the whole thing together."
307,@marktenenholtz,2022-12-20 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1605186283852029952,"Poor CI/CD probably holds back 95% of ML teams.

Often, the two biggest technical limiters of ML:

1. How long it takes to build
2. How error prone the pipelines are

The closer you are to ""green check == good to go"" the more adoption you'll see."
308,@marktenenholtz,2022-12-20 02:29:13+00:00,https://twitter.com/marktenenholtz/status/1605027512458924035,@Scobleizer @elonmusk @lhamtil They maybe only matter in something like commodities that require economies of scale
309,@marktenenholtz,2022-12-19 21:29:21+00:00,https://twitter.com/marktenenholtz/status/1604952049908469760,@realGeorgeHotz You're not excused until you fix search
310,@marktenenholtz,2022-12-19 17:30:12+00:00,https://twitter.com/marktenenholtz/status/1604891863424847872,@TheDSGuru Check yo privilege 😂
311,@marktenenholtz,2022-12-19 17:29:45+00:00,https://twitter.com/marktenenholtz/status/1604891752590348288,@vsbd I haven’t seen the same kind of speedup that I get from XGBoost. But it certainly works
312,@marktenenholtz,2022-12-19 13:19:12+00:00,https://twitter.com/marktenenholtz/status/1604828697550266369,@leassis91 Probably a tuning issue. Results (generally) should be quite similar
313,@marktenenholtz,2022-12-19 13:18:30+00:00,https://twitter.com/marktenenholtz/status/1604828523641856000,@adkravetz Try them both!
314,@marktenenholtz,2022-12-19 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1604823891142070272,"Here’s my general recommendation for using LightGBM vs. XGBoost vs. CatBoost.

Use:

• LightGBM when you’re only using a CPU
• XGBoost when you have a GPU
• CatBoost with a lot of categorical features

Obviously there’s plenty of wiggle room in there, but a solid rule of thumb"
315,@marktenenholtz,2022-12-19 12:56:39+00:00,https://twitter.com/marktenenholtz/status/1604823023759921152,"@PrasoonPratham Yeah the more I try it, the more I’m glad Elon is considering stepping down 😂"
316,@marktenenholtz,2022-12-18 23:38:15+00:00,https://twitter.com/marktenenholtz/status/1604622099263496192,@Suhail Everyone should have their own personal board of directors
317,@marktenenholtz,2022-12-18 22:02:14+00:00,https://twitter.com/marktenenholtz/status/1604597935282737152,@DSaience Replied to the original post with the link
318,@marktenenholtz,2022-12-18 21:57:50+00:00,https://twitter.com/marktenenholtz/status/1604596827763879936,Substack link here: https://t.co/XFLkNLKKLW
319,@marktenenholtz,2022-12-18 21:57:49+00:00,https://twitter.com/marktenenholtz/status/1604596824186261505,"Well, not sure what the social future holds for me. 

I’m not leaving Twitter… but…

Find me on: 

• LinkedIn
• Substack 
• same username @ s igmoid social"
320,@marktenenholtz,2022-12-18 21:42:56+00:00,https://twitter.com/marktenenholtz/status/1604593076579745795,"@janseben @svpino LinkedIn is next up, probably"
321,@marktenenholtz,2022-12-18 21:27:08+00:00,https://twitter.com/marktenenholtz/status/1604589102782971906,@svpino I’m a long term twitter optimist but damn this one is not a good look
322,@marktenenholtz,2022-12-18 17:45:37+00:00,https://twitter.com/marktenenholtz/status/1604533356850561030,Mbappe 😱 extra time just gave me a heart attack. If he had finished it there…
323,@marktenenholtz,2022-12-18 17:33:12+00:00,https://twitter.com/marktenenholtz/status/1604530231703965699,Messi. Holy. Crap.
324,@marktenenholtz,2022-12-18 16:48:08+00:00,https://twitter.com/marktenenholtz/status/1604518889920122882,@tunguz AGI
325,@marktenenholtz,2022-12-18 16:45:36+00:00,https://twitter.com/marktenenholtz/status/1604518251446374401,"@machsci @modal_labs @BananaDev_ The caveat, in my mind: if you work at a startup, you’d better know how"
326,@marktenenholtz,2022-12-18 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1604461497584742401,"Data scientists don’t need to know how to deploy their models, but you *probably* should learn anyway.

I’d recommend starting off with something like @modal_labs or @BananaDev_ (if you want a GPU) and deploy something.

Then, try to do it on your own on AWS/Azure/GCP."
327,@marktenenholtz,2022-12-17 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1604099125980459008,"Which brings up another question... why is GPT-3 decoder-only?

I could be wrong, but IIRC the latest research on LLMs shows that encoder-decoder performance tends to be superior.

Then again... I've never trained a 175B parameter model 😂"
328,@marktenenholtz,2022-12-17 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1604099123128344579,"It's great that OpenAI is upgrading their embeddings capabilities, but this benchmark from @Nils_Reimers is worth remembering.

It shouldn't be surprising the decoder-only models lose to encoder-only and encoder-decoder models.

Interested to see how this offering stacks up. https://t.co/rJhPPBT85Z"
329,@marktenenholtz,2022-12-17 00:44:50+00:00,https://twitter.com/marktenenholtz/status/1603914079927762945,@Suhail Skip the step of calling a friend and figuring it out within the first few seconds and be your own friend 😂
330,@marktenenholtz,2022-12-16 22:41:36+00:00,https://twitter.com/marktenenholtz/status/1603883066765844483,@tunguz Congrats Bojan! Here’s to 75k more!
331,@marktenenholtz,2022-12-16 18:00:07+00:00,https://twitter.com/marktenenholtz/status/1603812229781954560,There are few rushes quite like creating a new engineered feature that drastically improves your model.
332,@marktenenholtz,2022-12-16 17:11:35+00:00,https://twitter.com/marktenenholtz/status/1603800014723203086,"@Austen I think there's a fine line. I at least like that the person steering the ship is a very active user of the platform. 

Previously, it felt like the development of the platform was only a 60-70% match to the problems users actually faced on the platform."
333,@marktenenholtz,2022-12-16 16:39:31+00:00,https://twitter.com/marktenenholtz/status/1603791946509606913,You’ll learn 10x more about a modeling technique when you need to make it work in a user-facing product.
334,@marktenenholtz,2022-12-16 16:33:27+00:00,https://twitter.com/marktenenholtz/status/1603790420030681088,@svpino @OpenAI “Revenue” is alright. But I wonder about the profit.
335,@marktenenholtz,2022-12-16 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1603736724462456832,"I can’t count how many folks doubt the applications of LLMs while:

• using search engines power by them
• autocompleting emails with them
• using apps created with Copilot
• reading ads created by them

You don’t have to have an opinion on everything 😂"
336,@marktenenholtz,2022-12-16 03:09:27+00:00,https://twitter.com/marktenenholtz/status/1603588083894583297,@TanweerMHasan1 @rustlang Great library!
337,@marktenenholtz,2022-12-16 00:28:16+00:00,https://twitter.com/marktenenholtz/status/1603547522303229954,@dnn_sid The hardware side of images is so underrated. I want to make a pet tracker for my dog 😂
338,@marktenenholtz,2022-12-15 23:01:00+00:00,https://twitter.com/marktenenholtz/status/1603525560227037184,Bonus internet points for telling me why
339,@marktenenholtz,2022-12-15 22:59:15+00:00,https://twitter.com/marktenenholtz/status/1603525120131305474,You have the most fun creating models using ___ data:
340,@marktenenholtz,2022-12-15 19:52:31+00:00,https://twitter.com/marktenenholtz/status/1603478128025804800,@ecastillomon Coming up with a system to snapshot/version your datasets can be useful
341,@marktenenholtz,2022-12-15 18:26:19+00:00,https://twitter.com/marktenenholtz/status/1603456435068801042,@svpino That third one is the gift that keeps on giving 😂
342,@marktenenholtz,2022-12-15 18:00:07+00:00,https://twitter.com/marktenenholtz/status/1603449842574839808,"The first jarring part of working in industry is that datasets are rarely curated for you.

Eventually, you get comfortable with that.

But then the second jarring part is learning that the dataset you curated is constantly changing."
343,@marktenenholtz,2022-12-15 17:02:18+00:00,https://twitter.com/marktenenholtz/status/1603435291951464448,@tunguz Was thinking about this the other day. There’s probably a network effect (no pun intended) required to accomplish mass adoption that has never quite hit yet
344,@marktenenholtz,2022-12-15 13:40:00+00:00,https://twitter.com/marktenenholtz/status/1603384381191720962,@TheRandomMtrix Not claiming it’s incredible. I still will use something like XGBoost or a more representative stat model. But it’s pretty effective for how easy it is to use.
345,@marktenenholtz,2022-12-15 13:22:37+00:00,https://twitter.com/marktenenholtz/status/1603380006335533058,@BenOgorek No doubt about that
346,@marktenenholtz,2022-12-15 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1603374333421756418,"One of my favorite parts about time-series modeling is that there are some statistical models that are still competitive and relevant.

ARIMA is probably the poster child.

Sure, eventually I'll end up on LightGBM, but it's good to have a strong baseline in the meantime."
347,@marktenenholtz,2022-12-14 18:13:10+00:00,https://twitter.com/marktenenholtz/status/1603090737628942336,"@__mharrison__ @svpino It looks like the variance in my engagement/impressions is much higher. When one takes off, it really takes off. Other than that it kinda flops."
348,@marktenenholtz,2022-12-14 18:12:30+00:00,https://twitter.com/marktenenholtz/status/1603090570041511936,@DatosNinja @__mharrison__ @svpino Yeah unfortunately this doesn't surprise me
349,@marktenenholtz,2022-12-14 17:27:25+00:00,https://twitter.com/marktenenholtz/status/1603079223777845256,@svpino @__mharrison__ Same here
350,@marktenenholtz,2022-12-14 14:35:54+00:00,https://twitter.com/marktenenholtz/status/1603036062749970434,@roydanroy 👆correct answer 😂
351,@marktenenholtz,2022-12-14 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1603011984919695360,"You're given a non-anonymized tabular dataset and limited access to your stakeholder. Your goal is to produce a model with the best possible accuracy in the shortest amount of time.

In 2 tweets or less, what is your approach?"
352,@marktenenholtz,2022-12-13 16:41:10+00:00,https://twitter.com/marktenenholtz/status/1602705195628118018,"@charles_irl It's the self-driving cars question -- how much more safe than human drivers do they have to be before humans will trust them?

1.1x? 5x? 10x? 100+x?"
353,@marktenenholtz,2022-12-13 15:55:32+00:00,https://twitter.com/marktenenholtz/status/1602693713062498306,@charles_irl Spot on. Acting like we need AGI-level perfection before releasing anything into the world is why so many organizations hate how little value they get out of their data scientists 😂
354,@marktenenholtz,2022-12-13 14:08:06+00:00,https://twitter.com/marktenenholtz/status/1602666678684471299,@shubham12et1062 Yeah I’ve come to like Spark more and more
355,@marktenenholtz,2022-12-13 14:07:00+00:00,https://twitter.com/marktenenholtz/status/1602666397829578754,"@tunguz Nope, how does it’s performance compare to other data frame libraries?"
356,@marktenenholtz,2022-12-13 14:06:26+00:00,https://twitter.com/marktenenholtz/status/1602666258335518720,@voskresenskiia Never heard of it. I’ll check it out
357,@marktenenholtz,2022-12-13 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1602649575759568896,"Pandas is a great library, but it's very slow for some common time-series operations.

Most notably, rolling features.

If your feature engineering is a bottleneck, try out something like Spark, Dask, or Polars."
358,@marktenenholtz,2022-12-12 14:15:49+00:00,https://twitter.com/marktenenholtz/status/1602306231917477893,@tng_konrad You’re right not to trust me 😂
359,@marktenenholtz,2022-12-12 14:02:46+00:00,https://twitter.com/marktenenholtz/status/1602302945898414082,@tng_konrad I have loss function in there but I added a note about learning rate
360,@marktenenholtz,2022-12-12 14:02:20+00:00,https://twitter.com/marktenenholtz/status/1602302838067044352,I didn’t include learning rate because you can usually just set it to the default or 0.05 and you’ll be fine.
361,@marktenenholtz,2022-12-12 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1602287193937960961,"99% of XGBoost/LightGBM models only need to tune:

• Objective function
• num_leaves/max_depth
• feature_fraction
• bagging_fraction
• min_child_samples

They're generally not as hard to tune as you've probably been lead to believe."
362,@marktenenholtz,2022-12-11 15:44:18+00:00,https://twitter.com/marktenenholtz/status/1601966110236188675,@levelsio Ubuntu is perfect to use. I use it for all of my deep learning workloads
363,@marktenenholtz,2022-12-11 14:21:01+00:00,https://twitter.com/marktenenholtz/status/1601945150791749633,@johnleuner Blazing fast
364,@marktenenholtz,2022-12-11 14:20:49+00:00,https://twitter.com/marktenenholtz/status/1601945102561189890,@4ndr3aR @RAPIDSai cuDF is also great but sometimes I don’t have enough GPU RAM (or a GPU at all)!
365,@marktenenholtz,2022-12-11 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1601924786241736705,Check it out here: https://t.co/FEkx8djDk0
366,@marktenenholtz,2022-12-11 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1601924783553187840,"I've been checking out a new DataFrame library lately: Polars!

It's written in Rust, multi-threaded, and way faster than Pandas.

It supports lazy execution with query optimization and streaming larger-than memory datasets.

If you like the PySpark API, it feels very similar!"
367,@marktenenholtz,2022-12-10 18:28:37+00:00,https://twitter.com/marktenenholtz/status/1601645074718527488,@Ryan_Lanham I think I prefer this trajectory to flying cars
368,@marktenenholtz,2022-12-10 18:25:16+00:00,https://twitter.com/marktenenholtz/status/1601644232782905350,"It’s crazy that just a couple of decades ago, random forests were invented.

And now we’re talking about how many different DL models you can use to redecorate your house or see what your head would look like on a donkey’s body."
369,@marktenenholtz,2022-12-10 15:39:48+00:00,https://twitter.com/marktenenholtz/status/1601602588335824905,"@mervenoyann “Google is going to be dead in the next 3 years”

— person who was intro’d to LLMs by ChatGPT"
370,@marktenenholtz,2022-12-10 15:34:33+00:00,https://twitter.com/marktenenholtz/status/1601601267566268416,@littiecheese Don’t use an NLP model for that which can be solved by a regex
371,@marktenenholtz,2022-12-10 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1601562397252018177,"One of the beauties of APIs like GPT-3 is that they let you experiment with something without a long development cycle.

Don’t underestimate the value of trialing an imperfect solution before you spend months on a better one."
372,@marktenenholtz,2022-12-10 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1601562394680958976,"You can’t build a great model until you can deeply empathize with the problem.

Most models only *slightly* solve the problem.

Maybe the output isn’t really what’s needed.

Maybe it’s not optimized for the right task.

Maybe you didn’t even need a model in the first place!"
373,@marktenenholtz,2022-12-09 17:50:40+00:00,https://twitter.com/marktenenholtz/status/1601273136594694144,@tunguz Hrvatska!!!
374,@marktenenholtz,2022-12-09 13:57:05+00:00,https://twitter.com/marktenenholtz/status/1601214352610103301,@tunguz @nvidia Congrats Bojan! Here’s to more years of making the models go brrrr
375,@marktenenholtz,2022-12-09 13:54:21+00:00,https://twitter.com/marktenenholtz/status/1601213665029390336,@SeguraAndres7 Python even more so!
376,@marktenenholtz,2022-12-09 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1601200013383589888,"If you're doomscrolling on ML Twitter, you may be worried that you're falling behind.

Diffusion models, ChatGPT, you name it.

But most companies with ML functions don't even use a single deep learning model, let alone have datasets to train them on.

You're still early!"
377,@marktenenholtz,2022-12-08 14:02:50+00:00,https://twitter.com/marktenenholtz/status/1600853410076454912,@svpino I ran hundreds of experiments during the Kaggle competition I medaled in a few days ago. Couldn’t have managed it without automatic logging.
378,@marktenenholtz,2022-12-08 13:53:59+00:00,https://twitter.com/marktenenholtz/status/1600851184855785478,@dk21 What sort of interesting tasks do you think will be automated?
379,@marktenenholtz,2022-12-08 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1600837640906895364,"Your data science job isn’t going to get automated away, but the boring tasks will be.

It’s hard to say what those tasks are. But, some pretty safe qualities to have:

• Adaptability
• Communication
• Creative problem solving

Once those are solved… well, that’s AGI."
380,@marktenenholtz,2022-12-08 02:53:46+00:00,https://twitter.com/marktenenholtz/status/1600685036952748032,@svpino @MathNerdJeremy k-nearest neighbors is also very intuitive
381,@marktenenholtz,2022-12-07 16:37:33+00:00,https://twitter.com/marktenenholtz/status/1600529960602685440,@DSaience Don't tempt me
382,@marktenenholtz,2022-12-07 16:23:07+00:00,https://twitter.com/marktenenholtz/status/1600526328511729665,@bernhardsson I picked the right time to snatch a 4090
383,@marktenenholtz,2022-12-07 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1600475290924302337,"I probably repeated myself a couple of times, but that's just because I thought it was important!

Follow me @marktenenholtz for more high-signal ML content!"
384,@marktenenholtz,2022-12-07 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1600475288357347328,"20. Successful academics think innovative methods are cool (nothing wrong with that). Successful applied data scientists think the simplest effective solution is cool.

21. Keep a running log of the assumptions you're making while you run your experiments."
385,@marktenenholtz,2022-12-07 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1600475285857574913,"18. 5% of your job is actually training the models, and the other 95% is working with data and managing stakeholders. Plan accordingly.

19. Models that have their performance quantified in $$$ are infinitely more likely to be used."
386,@marktenenholtz,2022-12-07 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1600475283370307586,"16. Data collection and curation are the most underrated skills out there. Very rarely are you just given a formed dataset.

17. If you've read enough Kaggle competition solutions, you'll have a solid starting point on 95% of real-world problems."
387,@marktenenholtz,2022-12-07 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1600475280862109697,"14. The best applied data scientists are the ones that understand a decent amount of data engineering (and vice-versa).

15. Every data scientist should understand the phrase ""genchi genbutsu"" (go look it up)"
388,@marktenenholtz,2022-12-07 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1600475278416805889,"12. Always be willing to learn new tools. Sometimes, a different tool has out-of-the-box functionality perfect for what you're working on.

13. Data scientists like global views, such as aggregate metrics. Business people like granular, specific, tangible examples."
389,@marktenenholtz,2022-12-07 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1600475275938054144,"11. If you work for a medium+ sized company, go buy someone from every department lunch and ask them about their day-to-day problems. It's easy to get disconnected from reality in a DS role."
390,@marktenenholtz,2022-12-07 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1600475273530515457,"10. Work on your coding skills. You're always going to have to change your pipelines, and spaghetti code makes this take 100x longer."
391,@marktenenholtz,2022-12-07 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1600475270984527872,"8. Keep technical jargon out of your conversations as much as possible. You'll even confuse the data scientists you're talking to.

9. Work on your writing skills. Clear communication with your stakeholders builds trust. People fear what they don't understand."
392,@marktenenholtz,2022-12-07 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1600475268480499718,"6. Put together a demo before you finish your model/pipeline. It'll help you understand what to focus on.

7. The first priority in any data science project is agreeing on success criteria. There are a million ways to define value in the business world."
393,@marktenenholtz,2022-12-07 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1600475265980694528,"4. The best way to check if you have target leakage is to actually truncate your data. QA'ing your code won't always catch it.

5. Compute hours and dollars saved are not the only value metrics. Data scientist hours are expensive, so don't overcomplicate your models."
394,@marktenenholtz,2022-12-07 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1600475263418085376,"2. It's better to get a simple model (even with manual steps) out early to see if it's useful than trying to perfect a complicated one.

3. Every deployment should start with a human-in-the-loop phase. Most ML pipelines are too error-prone to avoid it."
395,@marktenenholtz,2022-12-07 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1600475260838502400,"21 thoughts any data scientist should read to succeed in the real world (that I learned the hard way):

1. Go talk to the person that will use the model before you start building it. You'll waste a lot less time.

(read on)"
396,@marktenenholtz,2022-12-06 23:03:45+00:00,https://twitter.com/marktenenholtz/status/1600264763241820160,@rasbt Congrats! Can’t wait to check this out
397,@marktenenholtz,2022-12-06 14:52:17+00:00,https://twitter.com/marktenenholtz/status/1600141082473267201,@AhmadMustafaAn1 Haha I just zoomed out
398,@marktenenholtz,2022-12-06 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1600112865150525441,"I'm putting together material for my upcoming course and wanted to make sure I was doing ARIMA justice in nice, simple terms.

So naturally, I turned to ChatGPT for help.

ChatGPT is going to fuel the next generation of learning tools 🤯 https://t.co/vZCfBILZrL"
399,@marktenenholtz,2022-12-05 13:19:37+00:00,https://twitter.com/marktenenholtz/status/1599755374051360768,@rasbt This is a Texas-based pour-over!
400,@marktenenholtz,2022-12-05 13:17:36+00:00,https://twitter.com/marktenenholtz/status/1599754865739108352,@rasbt Too safe. Pour-over is clearly superior https://t.co/z6JUSlEFxq
401,@marktenenholtz,2022-12-05 13:15:26+00:00,https://twitter.com/marktenenholtz/status/1599754321691742208,@melkebir1 🥲
402,@marktenenholtz,2022-12-05 13:15:15+00:00,https://twitter.com/marktenenholtz/status/1599754271737622530,@Aplamis1 It always surprises me with how fast it is
403,@marktenenholtz,2022-12-05 13:14:57+00:00,https://twitter.com/marktenenholtz/status/1599754197355810817,@nicolasvelezb I never could wrap my head around Pyro but it always seemed like a good library
404,@marktenenholtz,2022-12-05 13:13:40+00:00,https://twitter.com/marktenenholtz/status/1599753874214031360,@alsarasua And the best of friends at that
405,@marktenenholtz,2022-12-05 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1599750462890123266,What is your favorite ML library and why?
406,@marktenenholtz,2022-12-04 13:55:52+00:00,https://twitter.com/marktenenholtz/status/1599402105512476672,@arthurbmello Depends on the problem
407,@marktenenholtz,2022-12-04 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1599388070310137857,"For every forecasting model I make, I plot:

• Error by length of horizon
• Error during holidays (events)
• Error by seasonality (i.e. day of week)
• Feature importance over different periods

Depending on my use-case I also look at bias, trends, reactiveness, etc."
408,@marktenenholtz,2022-12-03 14:17:05+00:00,https://twitter.com/marktenenholtz/status/1599045059781226496,@Amedprof @The0Viel @CroDoc @dk21 @bhutanisanyam1 @ZbyHP Congrats!!!
409,@marktenenholtz,2022-12-03 14:10:10+00:00,https://twitter.com/marktenenholtz/status/1599043316368035840,@PrasoonPratham Another great option!
410,@marktenenholtz,2022-12-03 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1599025710626050048,You can check it out here: https://t.co/TVgZjEOaHO
411,@marktenenholtz,2022-12-03 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1599025707207647234,"The best way to grow your ML skills is project-based learning.

If you can't scrape your own data, you'll need interesting datasets.

Google Research has released 110 datasets spanning text to images to time series data.

Want to try a new method? Use one of these as a basis! https://t.co/zrTkb2WNUR"
412,@marktenenholtz,2022-12-03 03:15:18+00:00,https://twitter.com/marktenenholtz/status/1598878515826286592,"@goodside Sliding window works surprisingly well as long as you tune the overlap length so it wouldn’t surprise me. 

Maybe test a longer convo which requests info from the second or third prompt"
413,@marktenenholtz,2022-12-03 03:08:27+00:00,https://twitter.com/marktenenholtz/status/1598876792865304577,@goodside How do you think they manage it? Sliding window?
414,@marktenenholtz,2022-12-02 22:32:18+00:00,https://twitter.com/marktenenholtz/status/1598807294984298506,@amasad Brilliant ideas can now have 1 (crucial) fewer degree of separation from the idea-haver
415,@marktenenholtz,2022-12-02 22:30:53+00:00,https://twitter.com/marktenenholtz/status/1598806938489221120,"@amasad Instead of non-technical founders needing a junior dashboarder/analyst/designer attached to their hip, they can just do it themselves.

Businesses can get smaller and run leaner."
416,@marktenenholtz,2022-12-02 16:00:56+00:00,https://twitter.com/marktenenholtz/status/1598708805394653186,"@paulg The key that unlocks the future of AI is getting as many builders into the fold as possible.

The GPT-3 APIs have been instrumental in that."
417,@marktenenholtz,2022-12-02 14:54:11+00:00,https://twitter.com/marktenenholtz/status/1598692006267473920,"@janseben The deep, sinking regret you feel when you hit enter and realize you fat fingered something, but won’t be able to fix it for 15 minutes"
418,@marktenenholtz,2022-12-02 14:35:24+00:00,https://twitter.com/marktenenholtz/status/1598687280125861890,@chrisalbon We're going to quickly find out that GPT-3 is a better guru than most gurus
419,@marktenenholtz,2022-12-02 14:34:16+00:00,https://twitter.com/marktenenholtz/status/1598686993868898305,"@svpino It's probably good for them. The more growth there is, the more there is to complain about 😂"
420,@marktenenholtz,2022-12-02 14:33:04+00:00,https://twitter.com/marktenenholtz/status/1598686691698450432,"How I work with Excel:

Step 1: ""I need to do some calculations quickly.""

Step 2: ""I'll just do it in Excel!""

Step 3: ""Wow, Excel is an amazing tool. Very underrated.""

Step 4: ""Sh*t. This calculation is useful. I have to maintain it. Why the f*ck did I do it in Excel?"""
421,@marktenenholtz,2022-12-02 13:08:57+00:00,https://twitter.com/marktenenholtz/status/1598665523859955712,@AiSimonThompson Granular monitoring is extremely important!
422,@marktenenholtz,2022-12-02 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1598663290946695168,"The last wave of ML tools was making more accurate models.

The next wave of ML tools is making ML easier to use and develop."
423,@marktenenholtz,2022-12-01 17:08:21+00:00,https://twitter.com/marktenenholtz/status/1598363383685857283,@svpino @tunguz Modric was the player that introduced me to the beauty of midfield play
424,@marktenenholtz,2022-12-01 15:04:19+00:00,https://twitter.com/marktenenholtz/status/1598332171214852099,@rasbt There was a Kaggle competition a while ago where we all had a collective heart attack realizing that Pandas and NumPy do it differently 😂
425,@marktenenholtz,2022-12-01 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1598300951307120641,"I hope you enjoyed!

Let me know what you're building with GPT-3 and what new applications you think this opens up. I know I want to get back in the GPT-3 groove.

Follow me @marktenenholtz for more high-signal ML content!"
426,@marktenenholtz,2022-12-01 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1598300948392464384,And here's a nice image that encapsulates the training process nicely: https://t.co/aetf6obVCD
427,@marktenenholtz,2022-12-01 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1598300939542089728,This is mostly a summary from OpenAI's blog post. Check it out here: https://t.co/NvaSLX0E4u
428,@marktenenholtz,2022-12-01 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1598300936991961088,"There are some limitations with this approach, of course.

For example, RL training makes it hard to give the model access to ""truthful"" answers. It's just trained to give the kind of answer it thinks a human would want."
429,@marktenenholtz,2022-12-01 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1598300934395768832,"After running that for some iterations, they initiate the full PPO process by:

1. Sampling a new conversational prompt
2. Getting the output from the student model
3. Judging the quality of the output using the reward model
4. Updating the student with the feedback"
430,@marktenenholtz,2022-12-01 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1598300931765915648,"In PPO, the idea is to train a ""reward"" model that gives feedback to a student model.

Since the reward model was trained directly on the human feedback (ranked responses), it theoretically can pass that feedback onto the student model at much larger scale, on many more prompts."
431,@marktenenholtz,2022-12-01 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1598300929056382976,"Once the first stage was complete, they moved into RL.

Human trainers were given a set of GPT-3 completions for each prompt and were asked to rank them

Then, they used Proximal Policy Optimization (PPO) to kick off the reinforcement learning process."
432,@marktenenholtz,2022-12-01 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1598300926523019264,"The specialized training data for this model is in conversational form, but it's trained in 2 stages.

First, the model was trained in the same supervised, causal language model way, where the human played both sides of the conversation."
433,@marktenenholtz,2022-12-01 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1598300923918311425,"Previous versions of GPT-3 saw a huge boost when human feedback was incorporated into the training process.

Continuing with those advances, GPT-3 is trained using a method called ""Reinforcement Learning from Human Feedback."" (RLHF)"
434,@marktenenholtz,2022-12-01 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1598300921301065730,"GPT-3 could always correct based on your feedback, since it's autoregressive.

BUT, despite the fact that feedback is a very common part of the workflow with GPT-3, it wasn't actually optimized for that.

Now, ChatGPT is!

Here's how it was trained:"
435,@marktenenholtz,2022-12-01 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1598300918650269696,"To give you a taste of why this is so powerful, take a look at the picture in the first tweet.

When working with GPT-3, it's common to get a tangentially correct answer, but not quite what you're looking for."
436,@marktenenholtz,2022-12-01 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1598300915454210048,"GPT-3.5 has been released 🎉

It's called ""ChatGPT"" and it was trained to interact with you in a conversational manner.

Because of this, it's much easier to give GPT-3 feedback to get the answer you're looking for.

Here's how it was trained and what it can do: https://t.co/A3E8b1MozM"
437,@marktenenholtz,2022-11-30 18:00:07+00:00,https://twitter.com/marktenenholtz/status/1598014023168974849,"When you're splitting your data, you should pretty much always be stratifying your splits, not just splitting randomly.

(Unless you're working with a time-series)

The more data you have, though, the less this matters."
438,@marktenenholtz,2022-11-30 16:29:49+00:00,https://twitter.com/marktenenholtz/status/1597991296656105474,"@LukasValatka Hmm, not sure I totally agree. Some companies have had extremely good success pairing shoppers with humans that help them shop based on their style preferences.

The value to someone like Amazon is mostly in out-scaling those humans."
439,@marktenenholtz,2022-11-30 14:24:21+00:00,https://twitter.com/marktenenholtz/status/1597959724628344833,@Deepalis1612 Humans are expensive. Saving their time can be more valuable than accuracy (unless accuracy saves more of their time!)
440,@marktenenholtz,2022-11-30 14:11:59+00:00,https://twitter.com/marktenenholtz/status/1597956613695950849,"The main goal of ML models usually isn’t to do a task more accurately than humans.

It’s to do it faster and at a much larger scale (at comparable accuracy)."
441,@marktenenholtz,2022-11-30 01:57:02+00:00,https://twitter.com/marktenenholtz/status/1597771653655642112,@colum2131 Likewise! Can’t wait to see what you put together
442,@marktenenholtz,2022-11-30 01:20:59+00:00,https://twitter.com/marktenenholtz/status/1597762581879721984,@colum2131 I have the same dilemma. Got a silver but hadn’t submitted to the competition in a month so I lost my momentum 😂
443,@marktenenholtz,2022-11-29 20:55:52+00:00,https://twitter.com/marktenenholtz/status/1597695864604360704,@TheShadeEngine I love my deep learning models too. But I’m in an XGBoost kick 😂
444,@marktenenholtz,2022-11-29 18:00:04+00:00,https://twitter.com/marktenenholtz/status/1597651624272400385,"Linear regression (and its friends) are incredibly powerful for the right analysis.

But if you don't specifically need it, just use XGBoost."
445,@marktenenholtz,2022-11-29 13:46:15+00:00,https://twitter.com/marktenenholtz/status/1597587747489091584,@Mlondon83 I expect nothing less
446,@marktenenholtz,2022-11-29 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1597576144580014080,"When writing code, I try to document it by leaving helpful comments.

For instance:

• ""Sorry about this one""
• ""Remove this at your own peril""
• ""Can't figure out why this works""

Anything to help a future developer."
447,@marktenenholtz,2022-11-28 19:41:25+00:00,https://twitter.com/marktenenholtz/status/1597314740342370304,@charles_irl Invitation is always open!
448,@marktenenholtz,2022-11-28 19:32:34+00:00,https://twitter.com/marktenenholtz/status/1597312515138281472,@charles_irl Come to Austin
449,@marktenenholtz,2022-11-28 15:45:37+00:00,https://twitter.com/marktenenholtz/status/1597255398423400455,@rasbt I’ll have a new PC built at the end of this week with a 4090 in it. Would love to see what I can get speeds down to
450,@marktenenholtz,2022-11-28 14:12:01+00:00,https://twitter.com/marktenenholtz/status/1597231845850386433,@pksivasubraman1 That’s why we should all speak in terms of $$$ and not percentages
451,@marktenenholtz,2022-11-28 13:37:01+00:00,https://twitter.com/marktenenholtz/status/1597223037585063936,"Everyone says ""1% more accuracy is a waste of time.""

...without realizing that 1% more accuracy at scale is worth tens of millions of $$$ (at least)."
452,@marktenenholtz,2022-11-28 01:11:03+00:00,https://twitter.com/marktenenholtz/status/1597035307949764608,@QuantPrincess Yes. And @tunguz is the CEO
453,@marktenenholtz,2022-11-28 00:45:04+00:00,https://twitter.com/marktenenholtz/status/1597028770543677440,"@Pehdrew_ Lmao oh sorry, did I get the part about turning water into wine wrong?"
454,@marktenenholtz,2022-11-27 22:51:12+00:00,https://twitter.com/marktenenholtz/status/1597000113334890496,"Rumor has it GPT-4 will:

• Turn water to wine
• Transform any metal to gold
• Still get beaten by XGBoost on tabular data"
455,@marktenenholtz,2022-11-27 21:49:21+00:00,https://twitter.com/marktenenholtz/status/1596984546549207040,@pksivasubraman1 Entering the field from a different one gives you a wonderful perspective
456,@marktenenholtz,2022-11-27 21:48:46+00:00,https://twitter.com/marktenenholtz/status/1596984400306405376,@bleuisfrench Tools aren't nearly as important as your effort. The only guidance I'd give is probably go with Python.
457,@marktenenholtz,2022-11-27 21:48:12+00:00,https://twitter.com/marktenenholtz/status/1596984260186865664,@PsycheNerd 10 years is a bit excessive. You can become very productive within a couple years depending on how much effort you put in
458,@marktenenholtz,2022-11-27 21:47:24+00:00,https://twitter.com/marktenenholtz/status/1596984055551008768,"@richdatasci I'm even deeper, am the co-founder of a startup, and I still get imposter syndrome."
459,@marktenenholtz,2022-11-27 21:23:33+00:00,https://twitter.com/marktenenholtz/status/1596978054340845569,"There are too many needless barriers to enter machine learning.

But telling someone they can ""learn Python and ML"" in a week is a straight up lie."
460,@marktenenholtz,2022-11-27 14:34:35+00:00,https://twitter.com/marktenenholtz/status/1596875134161195008,@PsycheNerd Good luck ever training the model if you do that
461,@marktenenholtz,2022-11-27 14:34:05+00:00,https://twitter.com/marktenenholtz/status/1596875010836418560,@MindfulDataPath It usually ends up going that way
462,@marktenenholtz,2022-11-27 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1596851367586340864,The most reliable way to improve your models is to improve your data wrangling skills.
463,@marktenenholtz,2022-11-27 01:56:31+00:00,https://twitter.com/marktenenholtz/status/1596684360706842625,@tszzl We’re back baby
464,@marktenenholtz,2022-11-27 00:10:27+00:00,https://twitter.com/marktenenholtz/status/1596657669984641024,@tunguz @TiskTusk Sup duded
465,@marktenenholtz,2022-11-26 19:28:30+00:00,https://twitter.com/marktenenholtz/status/1596586712620154880,"@machsci I want more folks who have “been there, done that” in the real world talking about the stuff you can only know if you’ve deployed models at scale.

The sort of black magic that you need to make ML work in practice."
466,@marktenenholtz,2022-11-26 19:18:04+00:00,https://twitter.com/marktenenholtz/status/1596584090396135424,@EMostaque The uncentered pupil is nightmare stuff 😂
467,@marktenenholtz,2022-11-26 19:10:11+00:00,https://twitter.com/marktenenholtz/status/1596582102820356096,@cata94_cata Everyone who I’ve seen “leave” is at least 50% back
468,@marktenenholtz,2022-11-26 19:09:26+00:00,https://twitter.com/marktenenholtz/status/1596581913988587520,"@machsci You’re right, that whole story has been a black hole of content. But I’m less looking for the same creators making the same stuff and new folks switching it up. Haven’t seen that in a while."
469,@marktenenholtz,2022-11-26 19:04:00+00:00,https://twitter.com/marktenenholtz/status/1596580548725460992,@Austen Member of congress vs. GPT-3. Choose your misinformation fighter
470,@marktenenholtz,2022-11-26 19:02:35+00:00,https://twitter.com/marktenenholtz/status/1596580191102730241,@unmeshmali @radekosmulski Radek is great. Love all of my interactions with him.
471,@marktenenholtz,2022-11-26 19:02:06+00:00,https://twitter.com/marktenenholtz/status/1596580072584085504,I’m trying to find the next set of great ML builders and creators to spice things up
472,@marktenenholtz,2022-11-26 19:00:28+00:00,https://twitter.com/marktenenholtz/status/1596579661441822720,"@DSaience Maybe a little, but I don’t expect it to last"
473,@marktenenholtz,2022-11-26 18:58:19+00:00,https://twitter.com/marktenenholtz/status/1596579118170128384,@Scobleizer Yeah but I already follow you 😂
474,@marktenenholtz,2022-11-26 18:53:21+00:00,https://twitter.com/marktenenholtz/status/1596577870180016129,ML Twitter is so stale right now. Who should I be following?
475,@marktenenholtz,2022-11-26 18:02:14+00:00,https://twitter.com/marktenenholtz/status/1596565004027039744,"@HamelHusain I think it’s simply the combo of complexity + cost + benefit not stacking up well enough.

That being said, as with all things ML, it just takes one successful application to convince everyone."
476,@marktenenholtz,2022-11-26 17:56:06+00:00,https://twitter.com/marktenenholtz/status/1596563463035588609,@tunguz It’s not worth trying to recreate it. Make something different. Tech needs it.
477,@marktenenholtz,2022-11-26 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1596488984846225409,4) o
478,@marktenenholtz,2022-11-26 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1596488982375694336,3) B
479,@marktenenholtz,2022-11-26 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1596488979896881153,2) G
480,@marktenenholtz,2022-11-26 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1596488977439019009,1) X
481,@marktenenholtz,2022-11-26 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1596488974855376897,A step-by-step guide I used to build my latest ML model (that anyone can follow):
482,@marktenenholtz,2022-11-25 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1596126577531637760,"Your model will only ever be as good as your data.

Throw it all at your model and you’ll spend 100 GPU hours to get bad results.

Curate it and you’ll spend 1 hour to get state-of-the-art performance."
483,@marktenenholtz,2022-11-24 18:00:05+00:00,https://twitter.com/marktenenholtz/status/1595839689394556929,"On this Thanksgiving, I am thankful for XGBoost."
484,@marktenenholtz,2022-11-24 15:29:48+00:00,https://twitter.com/marktenenholtz/status/1595801866394288128,@TomRampley Definitely Hugging Face!
485,@marktenenholtz,2022-11-24 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1595764223367225344,"You can build a great transformer for NLP with just:

• A single GPU
• ~100-1000 well-curated samples
• A solid pre-trained model (like DeBERTa)

The power of pre-trained transformers is not that they’re a silver bullet.

It’s that they reduce your training complexity."
486,@marktenenholtz,2022-11-23 14:33:52+00:00,https://twitter.com/marktenenholtz/status/1595425402566201344,@svpino Congrats!
487,@marktenenholtz,2022-11-23 14:28:34+00:00,https://twitter.com/marktenenholtz/status/1595424068832043013,"@rasbt @ylecun As long as you’re willing to be constructive, I’d agree"
488,@marktenenholtz,2022-11-23 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1595401800323997696,"If you feel like you got everything you possibly could get out of your model, you probably delivered it too late."
489,@marktenenholtz,2022-11-22 13:39:24+00:00,https://twitter.com/marktenenholtz/status/1595049309782904833,"@TheDSGuru If you die while training XGBoost, you go to Valhalla"
490,@marktenenholtz,2022-11-22 13:38:35+00:00,https://twitter.com/marktenenholtz/status/1595049103813218307,@tunguz Yeah that’s spot on
491,@marktenenholtz,2022-11-22 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1595039427448827904,"99.9% of recent research on deep learning for tabular data doesn’t work in practice.

But!

I seriously appreciate that it exists and the work is being done.

Tabular data has otherwise been left behind."
492,@marktenenholtz,2022-11-22 01:05:41+00:00,https://twitter.com/marktenenholtz/status/1594859630886522880,@svpino Better data == better models :D
493,@marktenenholtz,2022-11-21 13:54:44+00:00,https://twitter.com/marktenenholtz/status/1594690779804536835,@TheDSGuru Not bad to understand what’s happening behind the curtain of the built-in!
494,@marktenenholtz,2022-11-21 13:00:22+00:00,https://twitter.com/marktenenholtz/status/1594677097556979712,"You'll learn ML methods 10x faster by stepping through code, line-by-line, than you will by staring at diagrams or complex formulas.

Even better: step through code with an accompanying diagram."
495,@marktenenholtz,2022-11-20 22:24:14+00:00,https://twitter.com/marktenenholtz/status/1594456610742706177,@Aella_Girl One thing I didn’t completely expect about that dynamic is that taking away moderation capabilities as a status symbol makes a huge difference
496,@marktenenholtz,2022-11-20 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1594314638149099520,"Behind every decision powered by an ML model is a real human that is affected by it.

You’ll make a lot more headway by getting out of your bubble and spending time with those people than you will by running yet another hyperparameter sweep."
497,@marktenenholtz,2022-11-19 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1593952262912475137,"There’s a 99% chance standard ML metrics are a really poor judge of value to your business.

Money usually isn’t made by improving your RMSE.

It’s actually made by:

• Saving shifts
• Reducing inventory
• Increasing conversions

Evaluate your models on those metrics."
498,@marktenenholtz,2022-11-18 20:12:20+00:00,https://twitter.com/marktenenholtz/status/1593698643176980488,@bentossell Here’s a list of some useful ones I’ve shared:
499,@marktenenholtz,2022-11-18 18:00:11+00:00,https://twitter.com/marktenenholtz/status/1593665384703401984,"I plan on rejuvenating it with some great content here soon… and maybe migrating to Substack.

I’m also going to start posting on LinkedIn! Follow me here: https://t.co/2WnncSorgq"
500,@marktenenholtz,2022-11-18 18:00:10+00:00,https://twitter.com/marktenenholtz/status/1593665382132301824,"I have no plans to leave Twitter, and I’m not really worried about its future.

But this has reminded me that it’s fun to post elsewhere and write longer-form content.

Join 4,250+ others and subscribe to my newsletter where I do that kinda thing. https://t.co/34sBfoLpMx"
501,@marktenenholtz,2022-11-18 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1593589880222732291,"Data scientists want to automate so much of other folks’ work, but get so upset when you automate any part of their work.

(Myself included)"
502,@marktenenholtz,2022-11-17 21:25:35+00:00,https://twitter.com/marktenenholtz/status/1593354689373937666,"@tunguz Then, no offense, you must be an idiot to have assumed you’d need an LLM"
503,@marktenenholtz,2022-11-17 20:51:04+00:00,https://twitter.com/marktenenholtz/status/1593346000780918790,"@bentossell It’s becoming more common. Mostly in the form of grants and free compute tokens. For example, the TPU Research Cloud from Google gives free access to TPUs."
504,@marktenenholtz,2022-11-17 17:54:20+00:00,https://twitter.com/marktenenholtz/status/1593301527279685632,"@xtinacomputes It gets interesting for a second when someone comes along with a novel idea and then we’re quickly slapped across the face as 10,000 content creators immediately copy it."
505,@marktenenholtz,2022-11-17 17:47:40+00:00,https://twitter.com/marktenenholtz/status/1593299846349348866,@Kurmapu_Loki As any good tweet should be
506,@marktenenholtz,2022-11-17 17:47:20+00:00,https://twitter.com/marktenenholtz/status/1593299764661157888,@njgroene Honest question: why?
507,@marktenenholtz,2022-11-17 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1593227472392200193,"You can spend weeks trying all of the models in the world.

Or you can just use XGBoost and focus on the data (which is what really matters)."
508,@marktenenholtz,2022-11-17 01:42:15+00:00,https://twitter.com/marktenenholtz/status/1593056891206537216,@tszzl 10 TPU pods were incinerated when I fed this tweet into GPT-4
509,@marktenenholtz,2022-11-16 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1592865092936138752,"YouTube link: https://t.co/3riJEfBFFS

If you're a UMich alum like me, you can view an even more up-to-date version here: https://t.co/ILP3onLlPt"
510,@marktenenholtz,2022-11-16 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1592865089790410754,"Computer vision is coming back into the forefront with Stable Diffusion.

But if you're totally new to CV, you've gotta get started somewhere.

No matter your skill level, here’s my favorite computer vision course.

(And, of course, it’s 100% free from University of Michigan!) https://t.co/1gkFOWEjD3"
511,@marktenenholtz,2022-11-16 01:40:56+00:00,https://twitter.com/marktenenholtz/status/1592694173077237760,@lgiqnna @tszzl Ban this fool @elonmusk
512,@marktenenholtz,2022-11-16 01:39:16+00:00,https://twitter.com/marktenenholtz/status/1592693754313723905,"@tszzl You: “what did you have for breakfast”

Me: “bacon egg and cheese”

You: “shit this is the future where SBF bought twitter”"
513,@marktenenholtz,2022-11-15 15:49:03+00:00,https://twitter.com/marktenenholtz/status/1592545221291933696,@JagersbergKnut The problem is that too many high level APIs are far too inflexible for that. A good one lets you write one small little subclass to change anything you want. Shouldn’t be more than 20 lines of code.
514,@marktenenholtz,2022-11-15 13:23:53+00:00,https://twitter.com/marktenenholtz/status/1592508690287931392,@DThompsonDev https://t.co/jF2ckmws3V
515,@marktenenholtz,2022-11-15 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1592502723932352512,"The best ""high-level"" ML APIs are really intermediate-level APIs with a few convenience routines stitched on top.

Most high-level APIs sound great until you realize that they are terribly inflexible and won't solve many problems different from their Colab examples."
516,@marktenenholtz,2022-11-15 02:37:22+00:00,https://twitter.com/marktenenholtz/status/1592345987246608384,"@xamat It is against FTC rules in the US to lie about your opinions of a product as an influencer, but I can’t really decide if they can pin anything on the influencers here (in general)."
517,@marktenenholtz,2022-11-14 13:45:06+00:00,https://twitter.com/marktenenholtz/status/1592151638701395969,"@svpino scikit-learn’s GroupKFold is your best friend! The newly-added StratifiedGroupKFold is even better.

Also love that we both posted on model evaluation today 😂"
518,@marktenenholtz,2022-11-14 13:40:01+00:00,https://twitter.com/marktenenholtz/status/1592150363033174016,"@rasbt I’ll check it out, thanks!"
519,@marktenenholtz,2022-11-14 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1592140343096446977,"Read it. Study it. Eat, sleep, and breathe it.

Model evaluation is a crucial skill to master, and this is a great base of knowledge for it.

Link to the paper: https://t.co/UQVllT1UKD"
520,@marktenenholtz,2022-11-14 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1592140339854209025,"You should always work on improving your model evaluation skills.

Model evaluation is the worst taught skill in machine learning, and I believe the best way of improving is through practice.

But, this paper from @rasbt is by far my favorite single written resource: https://t.co/rQvnotJWJq"
521,@marktenenholtz,2022-11-13 16:59:52+00:00,https://twitter.com/marktenenholtz/status/1591838266923286528,@tunguz Inb4 “but why don’t you like LightGBM???”
522,@marktenenholtz,2022-11-13 16:20:28+00:00,https://twitter.com/marktenenholtz/status/1591828350649131009,@DanKornas A beautiful story 🥲
523,@marktenenholtz,2022-11-13 13:50:50+00:00,https://twitter.com/marktenenholtz/status/1591790696515649537,@MeetRandomQuant IMO transformers has the easiest code to read of them all
524,@marktenenholtz,2022-11-13 13:49:49+00:00,https://twitter.com/marktenenholtz/status/1591790439832911872,@yegyanathan We need more mid-level API libraries
525,@marktenenholtz,2022-11-13 13:49:14+00:00,https://twitter.com/marktenenholtz/status/1591790290800902145,@me_mayuree It’s the poster child for API design
526,@marktenenholtz,2022-11-13 13:48:51+00:00,https://twitter.com/marktenenholtz/status/1591790194520375296,@mrclbschff ❤️❤️❤️
527,@marktenenholtz,2022-11-13 13:48:07+00:00,https://twitter.com/marktenenholtz/status/1591790009719611392,"@PrasoonPratham That can be said for so many of us, which is incredible"
528,@marktenenholtz,2022-11-13 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1591777913841405952,What’s your favorite ML library and why?
529,@marktenenholtz,2022-11-12 14:12:59+00:00,https://twitter.com/marktenenholtz/status/1591433880330194944,@philipvollet It’s the next stage of the XGBoost takeover
530,@marktenenholtz,2022-11-12 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1591415529801072640,"BREAKING: Elon Musk replaced as CEO of Twitter by XGBoost.

All employees expected to boost at least one tree for their new CEO."
531,@marktenenholtz,2022-11-12 03:07:34+00:00,https://twitter.com/marktenenholtz/status/1591266425880350720,@Suhail Unverified is the new verified
532,@marktenenholtz,2022-11-11 14:00:26+00:00,https://twitter.com/marktenenholtz/status/1591068334543757313,@tunguz @PacktPublishing @kaggle Is that on using AutoML or building the AutoML system itself?
533,@marktenenholtz,2022-11-11 13:38:20+00:00,https://twitter.com/marktenenholtz/status/1591062775425818625,@tunguz https://t.co/hXNwu9Yyk1
534,@marktenenholtz,2022-11-11 13:24:20+00:00,https://twitter.com/marktenenholtz/status/1591059251963006976,"@TheDSGuru Haha yeah, you can either guess or you can know immediately"
535,@marktenenholtz,2022-11-11 13:23:35+00:00,https://twitter.com/marktenenholtz/status/1591059063982686209,@CanumaGdt Yes! Working closely with other data scientists and engineers is crucial
536,@marktenenholtz,2022-11-11 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1591053143726432261,"If I could only give a beginner ML modeler one tip, it would be to prioritize:

• Logging
• Visualizations
• Experiment tracking

…or really anything that makes your results more visible.

That will save you from so many problems."
537,@marktenenholtz,2022-11-11 05:34:57+00:00,https://twitter.com/marktenenholtz/status/1590941125883592704,@__mharrison__ I'm gonna wake up some day to you using decorators on dataframes
538,@marktenenholtz,2022-11-11 04:13:52+00:00,https://twitter.com/marktenenholtz/status/1590920720372363264,"@tszzl The ML community has proved, similarly, that in many fields it's better to be generalists with better tools than domain experts with worse tools. 

Maybe not with the same hit rate, though.

Maybe rationalism is the better tool? idk"
539,@marktenenholtz,2022-11-11 00:24:49+00:00,https://twitter.com/marktenenholtz/status/1590863080212529154,"@Aella_Girl I get a lot of criticism but at least it's criticism of my ideas/data science/ML and not of who I am as a person (mostly). 

Can't imagine how folks deal with that type of hate on a 24/7 basis."
540,@marktenenholtz,2022-11-10 18:21:05+00:00,https://twitter.com/marktenenholtz/status/1590771542673137665,@xamat Congrats Xavier!
541,@marktenenholtz,2022-11-10 15:29:30+00:00,https://twitter.com/marktenenholtz/status/1590728361592033280,@shreydan I only find that it helps on huge models like deberta-v2-xlarge
542,@marktenenholtz,2022-11-10 14:38:26+00:00,https://twitter.com/marktenenholtz/status/1590715508999196672,@JordiClive whoops. That should have an extra 0 lol
543,@marktenenholtz,2022-11-10 13:27:34+00:00,https://twitter.com/marktenenholtz/status/1590697674881503234,@rasbt I open it and say “why can’t I ctrl+F??” and then slam it in frustration
544,@marktenenholtz,2022-11-10 13:22:39+00:00,https://twitter.com/marktenenholtz/status/1590696440086507520,@EMostaque @ericjang11 @giffmana @ethanCaballero @sama @mobav0 And then it converts every molecule in the universe into dollar bills. Easy!
545,@marktenenholtz,2022-11-10 13:19:43+00:00,https://twitter.com/marktenenholtz/status/1590695703130800128,"@rasbt I had success with 1 in the days of LSTMs but not with transformers.

As far as 2, it seems like some Kagglers *maybe* have had success, but I can’t tell. It obviously works when you do it to achieve the RTD pretaining objective but that’s a slight bit different."
546,@marktenenholtz,2022-11-10 13:10:38+00:00,https://twitter.com/marktenenholtz/status/1590693414135177218,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
547,@marktenenholtz,2022-11-10 13:10:37+00:00,https://twitter.com/marktenenholtz/status/1590693411639595009,"TL;DR:

1. Immerse yourself in the data
2. Create a baseline
3. Set up a pipeline
4. Error analysis and QA
5. Start improving your model
6. Add regularization
7. Repeat 5+6

Follow me @marktenenholtz for more high-signal ML content!"
548,@marktenenholtz,2022-11-10 13:10:37+00:00,https://twitter.com/marktenenholtz/status/1590693409081020416,"7. Repeat 5+6!

At this point, you should continue to try to squeeze out more accuracy and then regularize when you begin to overfit.

Just make sure you don't melt your GPU!"
549,@marktenenholtz,2022-11-10 13:10:36+00:00,https://twitter.com/marktenenholtz/status/1590693406526689280,"There are also adversarial training methods like Fast Gradient Method (FGM) and Adversarial Weight Perturbation (AWP).

Basically, they perform adversarial attacks on the model's embeddings during training to improve robustness at the cost of extra training time."
550,@marktenenholtz,2022-11-10 13:10:35+00:00,https://twitter.com/marktenenholtz/status/1590693403997528064,"You can try:

• Token dropout
• Internal hidden dropout
• Internal attention dropout
• Dropout on your output layer

But, you should note that dropout on regression tasks usually hurts your model performance, so I would avoid it."
551,@marktenenholtz,2022-11-10 13:10:35+00:00,https://twitter.com/marktenenholtz/status/1590693401455824897,"6. Add regularization

There are a bunch of ways to regularize deep learning models, but my favorite (to start with) for transformers is dropout.

With transformers, there are many places to try dropout."
552,@marktenenholtz,2022-11-10 13:10:34+00:00,https://twitter.com/marktenenholtz/status/1590693399044030464,"So, improve your model until you start to overfit.

Once you start to overfit, it's time to regularize!"
553,@marktenenholtz,2022-11-10 13:10:34+00:00,https://twitter.com/marktenenholtz/status/1590693396485574656,"However! Sometimes, gains you make on a ""small"" or ""base"" variant will vanish if you scale up to a large (or bigger) variant.

I recommend you test the effect of scaling variants before testing other methods of scaling.

Some experts don't even try base variants for this reason."
554,@marktenenholtz,2022-11-10 13:10:33+00:00,https://twitter.com/marktenenholtz/status/1590693393893535744,"As a sidebar:

The performance of a transformer is heavily dependent on its capacity. It can be very helpful to add specialized heads, like an LSTM on the last hidden state."
555,@marktenenholtz,2022-11-10 13:10:32+00:00,https://twitter.com/marktenenholtz/status/1590693391376842752,"As far as text preprocessing, an example is when chaotic capitalization can confuse your model.

In these cases lowercasing your text before tokenization can help."
556,@marktenenholtz,2022-11-10 13:10:32+00:00,https://twitter.com/marktenenholtz/status/1590693388835115008,"For example, adding new tokens can be useful when working in a specialized domain.

Sometimes there are words like medical acronyms that your model won't otherwise understand."
557,@marktenenholtz,2022-11-10 13:10:31+00:00,https://twitter.com/marktenenholtz/status/1590693386104676352,"5. Start improving your model

The first methods are try are usually:

• Longer sequences
• Text preprocessing
• Testing the effect of scaling up
• MLM pretraining on your corpus
• Adding new tokens to your tokenizer

When are these useful?"
558,@marktenenholtz,2022-11-10 13:10:30+00:00,https://twitter.com/marktenenholtz/status/1590693383537729536,"4. Error analysis and QA

Now that you've run your pipeline, the easiest way to do QA is to make sure you're producing sensible outputs.

This is where error analysis comes in.

I usually start with sorting by the largest errors and seeing if I find any obvious issues."
559,@marktenenholtz,2022-11-10 13:10:30+00:00,https://twitter.com/marktenenholtz/status/1590693381004357632,"3. Set up a pipeline

I generally start off with something really simple and reliable:

• Fixed random seed
• DeBERTa V3 base or small
• AdamW optimizer, no schedule
• Experiment tracking and logging
• Short sequences (i.e. 256-512 tokens)

Make sure this runs, bug-free!"
560,@marktenenholtz,2022-11-10 13:10:29+00:00,https://twitter.com/marktenenholtz/status/1590693378471010304,"2. Create a baseline

Simple models like logistic regression or LightGBM on top of TF-IDF or bag-of-words features are usually a solid baseline.

If a transformer can't significantly outperform one of those on your task, it's probably not worth pursuing."
561,@marktenenholtz,2022-11-10 13:10:29+00:00,https://twitter.com/marktenenholtz/status/1590693375719534592,"1. Immerse yourself in the data

Most of the data preprocessing is handled for you when you use a pretrained tokenizer.

However, you have to understand how your tokenizer interacts with your dataset.

Sometimes important words get split up, some tokens are unknown, etc."
562,@marktenenholtz,2022-11-10 13:10:28+00:00,https://twitter.com/marktenenholtz/status/1590693373056122880,"Over the last year, I've spent 100's of hours training transformers for NLP.

I went back over my most successful projects and competitions and distilled them into a solid, repeatable process that anyone can follow.

7 steps to train transformers:"
563,@marktenenholtz,2022-11-10 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1590690815705198593,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
564,@marktenenholtz,2022-11-10 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1590690813184335873,"TL;DR:

1. Immerse yourself in the data
2. Create a baseline
3. Set up a pipeline
4. Error analysis and QA
5. Start improving your model
6. Add regularization
7. Repeat 5+6

Follow me @marktenenholtz for more high-signal ML content!"
565,@marktenenholtz,2022-11-09 18:06:12+00:00,https://twitter.com/marktenenholtz/status/1590405409172774912,Will the real Mark Tenenholtz please stand up?
566,@marktenenholtz,2022-11-09 18:05:21+00:00,https://twitter.com/marktenenholtz/status/1590405196160839681,@svpino looks like it’s contagious and I caught it
567,@marktenenholtz,2022-11-09 14:41:13+00:00,https://twitter.com/marktenenholtz/status/1590353823285268480,"@tunguz Like I said, easy :)"
568,@marktenenholtz,2022-11-09 13:42:19+00:00,https://twitter.com/marktenenholtz/status/1590338999637639169,@tng_konrad Alternatively: XGBoost
569,@marktenenholtz,2022-11-09 13:28:15+00:00,https://twitter.com/marktenenholtz/status/1590335462761467905,"@CanumaGdt You’re right, and that’s why it’s just a baseline. But you’ll find a more complex model losing to it more than you expect!"
570,@marktenenholtz,2022-11-09 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1590328364451041280,"In NLP, it's really easy to consistently beat a traditional bag-of-words approach with a transformer.

In computer vision, it's really easy to consistently beat an SVM with a CNN.

In time-series forecasting, it's $*&amp;%ing hard to consistently beat a seasonal rolling average."
571,@marktenenholtz,2022-11-08 16:55:31+00:00,https://twitter.com/marktenenholtz/status/1590025232181760001,@natelfortner 🙏🙏
572,@marktenenholtz,2022-11-08 15:29:45+00:00,https://twitter.com/marktenenholtz/status/1590003648985079809,@DSaience Oh Sairam... this is just one lesson :)
573,@marktenenholtz,2022-11-08 15:20:17+00:00,https://twitter.com/marktenenholtz/status/1590001267417714688,@prashantricoche 🙏
574,@marktenenholtz,2022-11-08 15:20:02+00:00,https://twitter.com/marktenenholtz/status/1590001204800913409,@mohammad2012191 You can count on it!
575,@marktenenholtz,2022-11-08 15:19:40+00:00,https://twitter.com/marktenenholtz/status/1590001110252945408,@JCassens Absolutely. Would appreciate if you linked back to the original post.
576,@marktenenholtz,2022-11-08 14:14:33+00:00,https://twitter.com/marktenenholtz/status/1589984724092846080,@_mxnyy_ The Croston method is a good place to start
577,@marktenenholtz,2022-11-08 13:07:34+00:00,https://twitter.com/marktenenholtz/status/1589967869424848896,@svpino So many great options for getting started in the field. And they’re often taught by huge names.
578,@marktenenholtz,2022-11-08 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1589966035842523138,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
579,@marktenenholtz,2022-11-08 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1589966033325944832,"TL;DR:

1. Seasonality
2. Trends
3. Reactiveness
4. Holidays
5. Bias
6. Accuracy by volume
7. Drift

Follow me @marktenenholtz for more high-signal ML content!"
580,@marktenenholtz,2022-11-08 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1589966030779994113,"7. Drift

Be sure to keep updated metrics on how your model performs in production!

If you're finding that it starts to drift too quickly, you might consider increasing the length of your roll-forward evaluation windows."
581,@marktenenholtz,2022-11-08 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1589966028091580417,"6. Accuracy by volume

If you have multiple time series, make sure to separate your evaluation.

Keep the series that are high-volume and regular in one group, and highly intermittent, low-volume series in another."
582,@marktenenholtz,2022-11-08 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1589966025386180608,"5. Bias

Check to see if your model is consistently over-/under-forecasting.

Modern ML models don't usually struggle with this, but if they do, you may just need more data.

A common issue that can cause this is poor data quality, though, so be sure to check that."
583,@marktenenholtz,2022-11-08 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1589966022747992065,"4. Holidays

Don't just check the effect of holidays on the day of the holiday.

Make sure that you're checking the period around them.

If your model struggles just before and after holidays, try adding features that tell your model that's it's close to a holiday."
584,@marktenenholtz,2022-11-08 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1589966020151648257,"Other options:

Mean squared error loss rewards a model that can react to sudden changes.

Mean absolute error rewards models that don't overreact.

If the changes are unexpected and ignorable, use MAE.

If it's important to be reactive, consider switching to MSE."
585,@marktenenholtz,2022-11-08 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1589966017559597057,"3. Reactiveness

This is the ability of your model to quickly react to changes not caused by a trend or cycle.

If your model is slow to react to sudden changes, try adding short-term rolling/lag features.

If it overreacts, try adding longer-term rolling/lag features."
586,@marktenenholtz,2022-11-08 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1589966014917144576,"If your model is having issues with trends, try adding difference features, i.e. the difference between 7 days ago and 28 days ago, or even the differences between rolling features."
587,@marktenenholtz,2022-11-08 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1589966012312477697,"2. Trends

Make sure your model can follow general rising/falling trends!

Even better -- make sure your model doesn't perform better/worse while either sort of trend is happening.

EVEN better -- check all of the other characteristics during rising/falling trends."
588,@marktenenholtz,2022-11-08 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1589966009724608512,"1. Seasonality

Many time series behave differently based on time, whether it's weekly, monthly, or any recurring cycle.

Break your error out by each of these cycles.

If you struggle to capture seasonality, try adding more time-based features."
589,@marktenenholtz,2022-11-08 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1589966007019241473,"Error analysis, 101: Time-series data

Time-series models are full of implementation traps.

Target leakage, data drift, etc.

But! You can usually solve these issues through solid error analysis.

So, here are 7 of the methods I use on all of my forecasting projects:"
590,@marktenenholtz,2022-11-07 18:41:25+00:00,https://twitter.com/marktenenholtz/status/1589689497129996289,@chrisalbon That was a fun thread to read 😂 glad you got it back!
591,@marktenenholtz,2022-11-07 16:49:33+00:00,https://twitter.com/marktenenholtz/status/1589661343623180288,@wizofafrica It's a joke. I love them all. Except the ones that aren't PyTorch.
592,@marktenenholtz,2022-11-07 15:47:18+00:00,https://twitter.com/marktenenholtz/status/1589645676232572929,@nischay_twt That’s you figuring out reasons to hate them all 😂😂
593,@marktenenholtz,2022-11-07 14:29:27+00:00,https://twitter.com/marktenenholtz/status/1589626085272002560,@unofficialazhan Catch me in a non-sarcastic moment and I’ll agree with you. I ❤️ JAX and torch :)
594,@marktenenholtz,2022-11-07 13:41:49+00:00,https://twitter.com/marktenenholtz/status/1589614097498509314,@tunguz @mims @WSJ Arguably the companies that he was formed at were software companies (PayPal + the companies he founded that were acquired by PayPal)
595,@marktenenholtz,2022-11-07 13:32:51+00:00,https://twitter.com/marktenenholtz/status/1589611842531254273,@thejustinwelsh Fantastic thread! Your ability to follow the signal with extreme consistency has always been fun to watch.
596,@marktenenholtz,2022-11-07 13:24:56+00:00,https://twitter.com/marktenenholtz/status/1589609848861515778,@PrasoonPratham WSL2 just got so good… I can’t remember the last time I opened my Ubuntu partition
597,@marktenenholtz,2022-11-07 13:21:49+00:00,https://twitter.com/marktenenholtz/status/1589609065097093121,@tunguz @mims @WSJ Feels like an article that’s been written about all of Elon’s companies at one point or another :)
598,@marktenenholtz,2022-11-07 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1589603613956923392,A senior ML engineer is one that passionately hates more than one deep learning framework.
599,@marktenenholtz,2022-11-07 02:40:35+00:00,https://twitter.com/marktenenholtz/status/1589447693486592001,"@EdLatimore The difference is that, in coding, you’re speaking a language to a computer that it’s meant to speak.

In fighting, you’re trying to flatten someone who doesn’t want to be flattened."
600,@marktenenholtz,2022-11-07 02:35:31+00:00,https://twitter.com/marktenenholtz/status/1589446418758270976,@jakevdp I’ve always found it’s less R than you think
601,@marktenenholtz,2022-11-06 23:52:58+00:00,https://twitter.com/marktenenholtz/status/1589405513632317442,@GaryMarcus Have you not had that happen to you? It’s a menace on this site.
602,@marktenenholtz,2022-11-06 21:59:46+00:00,https://twitter.com/marktenenholtz/status/1589377022564130816,@thejustinwelsh @dickiebush One of the best parts of moving to Texas is the abundance of Topo Chico
603,@marktenenholtz,2022-11-06 21:40:04+00:00,https://twitter.com/marktenenholtz/status/1589372066377371649,@Suhail This one sums it up for me
604,@marktenenholtz,2022-11-06 17:20:19+00:00,https://twitter.com/marktenenholtz/status/1589306699772416000,@tunguz …but the difficulty is what makes it fun 😞
605,@marktenenholtz,2022-11-06 17:15:51+00:00,https://twitter.com/marktenenholtz/status/1589305572863533057,@simonw And don’t chase engagement. Passion is what shines through
606,@marktenenholtz,2022-11-06 17:15:29+00:00,https://twitter.com/marktenenholtz/status/1589305480509550593,"@simonw Couldn’t agree more. As a corollary, don’t be afraid to learn in public (i.e. writing about the things you’re *actively* learning!)"
607,@marktenenholtz,2022-11-06 17:13:07+00:00,https://twitter.com/marktenenholtz/status/1589304887145218049,@pwlot You have to truly feel your pain if you’re ever going to accept it
608,@marktenenholtz,2022-11-06 14:42:44+00:00,https://twitter.com/marktenenholtz/status/1589267040535445505,@bernhardsson @modal_labs Add @cometml and @neptune_ai! Both great ML experiment trackers!
609,@marktenenholtz,2022-11-06 14:25:53+00:00,https://twitter.com/marktenenholtz/status/1589262801352421376,@Suhail You should package this up and sell it as a new algorithm
610,@marktenenholtz,2022-11-06 14:23:26+00:00,https://twitter.com/marktenenholtz/status/1589262183028109313,https://t.co/qwO2HBkhMm
611,@marktenenholtz,2022-11-06 13:39:23+00:00,https://twitter.com/marktenenholtz/status/1589251100028600325,@svpino Congratulations on clocking time towards your 50%
612,@marktenenholtz,2022-11-06 13:34:07+00:00,https://twitter.com/marktenenholtz/status/1589249774615920640,"@SeguraAndres7 I like it too, because it gives me something to complain about"
613,@marktenenholtz,2022-11-06 13:33:33+00:00,https://twitter.com/marktenenholtz/status/1589249631065493504,@TimothyKassis That’s where I complain about Jira
614,@marktenenholtz,2022-11-06 13:32:29+00:00,https://twitter.com/marktenenholtz/status/1589249362928218114,"@tunguz But if I don’t get angry, how will others know that I hold the correct opinion?"
615,@marktenenholtz,2022-11-06 13:31:08+00:00,https://twitter.com/marktenenholtz/status/1589249021109207040,@svpino This will be great! Can’t wait to check it out!
616,@marktenenholtz,2022-11-06 13:30:17+00:00,https://twitter.com/marktenenholtz/status/1589248810726805504,@tunguz For the uninitiated: https://t.co/5zVrZiyPZ5
617,@marktenenholtz,2022-11-06 13:22:52+00:00,https://twitter.com/marktenenholtz/status/1589246941040611328,@tunguz If only they could define real Jira…
618,@marktenenholtz,2022-11-06 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1589241197213536256,"Programming for fun: 10% writing code. 90% figuring out why it doesn’t work

Programming at work: 10% writing code. 40% figuring out why it doesn’t work. 50% complaining about Jira."
619,@marktenenholtz,2022-11-05 17:18:40+00:00,https://twitter.com/marktenenholtz/status/1588943897400442880,"@tunguz [100%, 100%]"
620,@marktenenholtz,2022-11-05 17:00:09+00:00,https://twitter.com/marktenenholtz/status/1588939236840943616,Prediction intervals are nice but 99% of the stakeholders that beg you for them never use them.
621,@marktenenholtz,2022-11-05 13:13:56+00:00,https://twitter.com/marktenenholtz/status/1588882306408009731,"@davidasboth “We use prediction intervals to increase the value of our forecast”

“But does anyone use anything but the point forecast?”

*crickets*"
622,@marktenenholtz,2022-11-05 13:07:48+00:00,https://twitter.com/marktenenholtz/status/1588880764300857347,@tunguz @Caltech @rmichaelalvarez Me enthusiastically diving into political polarization research https://t.co/62gGoyWNRG
623,@marktenenholtz,2022-11-05 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1588863723698933760,"To build the best forecasting models, spend less time tuning hyperparameters and architectures and more time:

• Evaluating your models
• Turning metrics into $$$
• Building stakeholder trust
• Understanding the business

The best forecast is the one that gets used."
624,@marktenenholtz,2022-11-05 02:18:53+00:00,https://twitter.com/marktenenholtz/status/1588717456377647104,"@TheZachMueller @charles_irl Yeah, see I don’t have to do this at all. I can install and uninstall at will. I’ll take a look at my bashrc tomorrow and see if I did anything fancy.

@charles_irl do you use conda environments?"
625,@marktenenholtz,2022-11-05 00:42:38+00:00,https://twitter.com/marktenenholtz/status/1588693237031907330,"@charles_irl Interesting, on WSL I have no issue with both pip and conda installing torch"
626,@marktenenholtz,2022-11-04 22:54:15+00:00,https://twitter.com/marktenenholtz/status/1588665960055341056,@charles_irl I just did the same thing a while back with a 3080 Ti on WSL2. Had a bunch of anxiety that I’d have to go to hell and back to get it working 😂
627,@marktenenholtz,2022-11-04 18:12:46+00:00,https://twitter.com/marktenenholtz/status/1588595123382013952,"@levelsio @dvassallo @dagorenouf @bohlenlabs I’ve watched many in the ML space burn out. For me, I get so much out of sharpening my ideas and expressing my passion"
628,@marktenenholtz,2022-11-04 15:07:46+00:00,https://twitter.com/marktenenholtz/status/1588548567446487040,@LeopolisDream Always work backwards from the problem!
629,@marktenenholtz,2022-11-04 15:07:34+00:00,https://twitter.com/marktenenholtz/status/1588548515017674753,@jim_dowling That’s the next step for sure!
630,@marktenenholtz,2022-11-04 14:41:44+00:00,https://twitter.com/marktenenholtz/status/1588542012957298689,@Austen Definitely @Splittestingcom
631,@marktenenholtz,2022-11-04 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1588501329273446400,"If you're a data scientist that wants to grow as a builder, here's how.

Pick a project that makes you:

1. Scrape data
2. Clean data
3. Organize data
4. Explore data
5. Try many models
6. Deploy models
7. Monitor models

If you want to be a builder, master those skills."
632,@marktenenholtz,2022-11-03 12:59:19+00:00,https://twitter.com/marktenenholtz/status/1588153854012563460,@cor3bit Or a whole series… but yeah kinda!
633,@marktenenholtz,2022-11-03 12:00:29+00:00,https://twitter.com/marktenenholtz/status/1588139046999425026,"The true purpose of forecasting is not just to predict the future, but rather to tell you what actions to take in the present.

All of your forecasting tools should be built with that in mind."
634,@marktenenholtz,2022-11-02 22:15:46+00:00,https://twitter.com/marktenenholtz/status/1587931498644402177,@tunguz import lightgbm as xgb
635,@marktenenholtz,2022-11-02 12:00:22+00:00,https://twitter.com/marktenenholtz/status/1587776630411317248,"The wrong way to build ML models:

• Scale up quickly
• Create huge ensembles
• Tune hyperparameters often

The right way:

• Meticulous error analysis
• Reliable, off-the-shelf models
• Robust model evaluation + value analysis

Focus on the easy gains + proving the value."
636,@marktenenholtz,2022-11-01 23:58:05+00:00,https://twitter.com/marktenenholtz/status/1587594859128520706,@ergestx Multiple datasets from multiple industries
637,@marktenenholtz,2022-11-01 23:56:27+00:00,https://twitter.com/marktenenholtz/status/1587594449122721793,@data_kristen Maybe you should re-read that tweet :) it's very useful
638,@marktenenholtz,2022-11-01 23:55:57+00:00,https://twitter.com/marktenenholtz/status/1587594324426067976,@jmrlsz 🫡
639,@marktenenholtz,2022-11-01 14:30:38+00:00,https://twitter.com/marktenenholtz/status/1587452055375024128,"@tng_konrad @thejustinwelsh “Hey, we’re recruiting for a job that we have no reason to assume that you’d be interested in, but it’s vaguely in your industry. Got 15 minutes to chat?”"
640,@marktenenholtz,2022-11-01 13:22:37+00:00,https://twitter.com/marktenenholtz/status/1587434940131676161,"@thejustinwelsh The most common DM I get:

“Hey”

If you expect someone to reply to that…"
641,@marktenenholtz,2022-11-01 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1587414172236533762,"If you're explaining a data science concept and you feel the need to use variable names in your ""simple"" explanation, it's not simple enough."
642,@marktenenholtz,2022-11-01 01:47:30+00:00,https://twitter.com/marktenenholtz/status/1587260010266071040,@Attack @bentossell @promptogame 👀
643,@marktenenholtz,2022-10-31 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1587051793430691841,"For this course, I'm going to follow the philosophy of building in public.

Follow me @marktenenholtz so you can follow it as it progresses!

Keep an eye out for the waitlist and the eventual sign-up process."
644,@marktenenholtz,2022-10-31 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1587051790880542721,"These are just a few things I'll go over, though!

On top of that, I'll touch on:

1. Ensembling
2. Interpretability
3. Low-data scenarios
4. Handling intermittency
5. Turning metrics into $$$
6. Drift detection + retraining
7. Building demo apps for yourself + stakeholders"
645,@marktenenholtz,2022-10-31 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1587051788296810496,"4. Handling additional features

On top of just the raw time-series values, you'll often have holidays, events, and other external features.

Combine a knowledge of:

1. The FE techniques at your disposal
2. A little domain knowledge

...and you'll never be confused again."
646,@marktenenholtz,2022-10-31 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1587051785759256576,"3. Model evaluation is *very hard*

Not only is cross-validation tough, but there's a million metrics and diagnostic plots you can use.

But, once you understand the common problems with forecasting models and business use-cases, it makes more sense!"
647,@marktenenholtz,2022-10-31 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1587051783146196993,"2. The options are overwhelming!

Time-series is one of the remaining domains where statistical modeling is still really useful.

So, should you use one of a 100 statistical models? What about an ML model? Deep learning?

Plus, how do I create features for all of these models???"
648,@marktenenholtz,2022-10-31 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1587051780528930817,"1. Feature engineering is hard... and slow

It's really hard to compute lag/rolling features while avoiding target leakage.

On top of that, the ""correct way"" of calculating a lagged rolling mean in Pandas is SLOW!

Fortunately, there are some great OS libraries that can help!"
649,@marktenenholtz,2022-10-31 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1587051777911767040,"I'm working on a cohort course for time-series forecasting! 🎉

There's a lot of good info on forecasting out there, but most of it is hard to dig into and (many times) out of date. It's time it got a facelift.

Here are some pain points I'll solve:"
650,@marktenenholtz,2022-10-31 02:05:49+00:00,https://twitter.com/marktenenholtz/status/1586902228866039809,@Aella_Girl Asks them why
651,@marktenenholtz,2022-10-31 02:05:22+00:00,https://twitter.com/marktenenholtz/status/1586902114659405825,@tszzl But we brothers remember the tanch function
652,@marktenenholtz,2022-10-30 13:07:37+00:00,https://twitter.com/marktenenholtz/status/1586706389665185797,@bangbangdown The first way
653,@marktenenholtz,2022-10-30 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1586689396815691776,"The best way to learn just about anything is:

1. Toy around with it
2. See what it can (and can't do)
3. Dive deeper, one layer at a time.

Despite that, most courses work by:

1. Starting from the bottom
2. Combining seemingly unrelated ideas
3. Miraculously ending up at thetop"
654,@marktenenholtz,2022-10-29 17:10:27+00:00,https://twitter.com/marktenenholtz/status/1586405114390216705,@hacklavya @ID_AA_Carmack I use it all the time
655,@marktenenholtz,2022-10-29 15:33:04+00:00,https://twitter.com/marktenenholtz/status/1586380604286926850,"@ID_AA_Carmack Were you using WSL? I generally don't have any issues when I use it, but I wouldn't dare install it on plain ol' Windows."
656,@marktenenholtz,2022-10-29 14:40:28+00:00,https://twitter.com/marktenenholtz/status/1586367368053374976,"@rasbt @FrankRHutter @predict_addict @tunguz @ph_singer One that comes to mind is “Don’t Overfit! II” but I think it’s synthetic data, so not really a great choice."
657,@marktenenholtz,2022-10-29 13:32:50+00:00,https://twitter.com/marktenenholtz/status/1586350345596596225,"@marccodess Ha, that’s a bit above my paygrade. But I think we need something that is way more sample-efficient and something that incorporates knowledge of the world."
658,@marktenenholtz,2022-10-29 13:31:08+00:00,https://twitter.com/marktenenholtz/status/1586349920025735169,@tunguz AGI will be my retirement plan 😂
659,@marktenenholtz,2022-10-29 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1586327014444699648,"Our currently ML models probably won't get us to AGI without some serious changes.

But that's not going to stop me from getting as much value as possible out of them in the mean time."
660,@marktenenholtz,2022-10-28 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1585964616277893120,"Time-series forecasting data is (conceptually) one of the most similar tasks to NLP.

Compared to NLP tasks, forecasting tasks probably have:

• 10x more applications
• 10x less time spent on innovation

We need another ""ImageNet moment,"" and this time for forecasting."
661,@marktenenholtz,2022-10-28 03:28:03+00:00,https://twitter.com/marktenenholtz/status/1585835761714315265,"@Suhail @_akhaliq Fréchet inception distance. It measures the quality of generated images. 

Basically compares the distributed of the generated images to a set of real images. https://t.co/Qjp6mEf5TA"
662,@marktenenholtz,2022-10-27 17:00:15+00:00,https://twitter.com/marktenenholtz/status/1585677770427838464,"We should normalize career paths built specifically for those with strong ML backgrounds to act as communicators to the rest of the business.

Not just VPs of data science or business analysts.

Someone who can clearly communicate and build trust in even the most complex models."
663,@marktenenholtz,2022-10-27 14:01:36+00:00,https://twitter.com/marktenenholtz/status/1585632809242460162,@tunguz Especially the one you order from AliExpress
664,@marktenenholtz,2022-10-27 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1585602258007302144,"Very few data scientists think they’re going to try to build their own AutoML tool.

…until they work on a time-series forecasting problem 😂"
665,@marktenenholtz,2022-10-26 17:11:45+00:00,https://twitter.com/marktenenholtz/status/1585318277974495234,"@machsci Oh, and occasionally target scaling can help. But yeah, not necessary for features"
666,@marktenenholtz,2022-10-26 17:00:07+00:00,https://twitter.com/marktenenholtz/status/1585315349049712640,"Hiring a data scientist with experience in your industry gives you someone that can incrementally improve your current performance.

Hiring a DS with experience outside your industry gives you someone with the potential to dramatically overhaul (and improve) your current process."
667,@marktenenholtz,2022-10-26 16:43:08+00:00,https://twitter.com/marktenenholtz/status/1585311075599544321,"@machsci It's only necessary to do it if you're doing linear boosting. Otherwise, you can include it as an extra feature to encode relativity, i.e. min-max scale to indicate what % off of the max value in a column a certain feature is.

Outside of that it's pretty much useless."
668,@marktenenholtz,2022-10-26 15:40:45+00:00,https://twitter.com/marktenenholtz/status/1585295377149804544,@AmandaMGoetz That’s incredible Amanda! Congratulations!
669,@marktenenholtz,2022-10-26 12:59:05+00:00,https://twitter.com/marktenenholtz/status/1585254691667337216,@machsci Converting errors into $$$ is tough!
670,@marktenenholtz,2022-10-26 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1585239857776930816,Model evaluation is often the largest form of creative expression in data science.
671,@marktenenholtz,2022-10-25 17:57:45+00:00,https://twitter.com/marktenenholtz/status/1584967465842671617,@xtinacomputes bring 👏 back 👏 vine 👏
672,@marktenenholtz,2022-10-25 16:27:27+00:00,https://twitter.com/marktenenholtz/status/1584944740822581249,"@businessbarista ""We expect full economy capture in year 5"""
673,@marktenenholtz,2022-10-25 16:19:04+00:00,https://twitter.com/marktenenholtz/status/1584942631993618432,"@levelsio I remember reading something a while ago about ""consensus"" decisions being less accurate for this reason.

Anyone in ML can tell you that the best ensemble is made from models that are as unrelated as possible!"
674,@marktenenholtz,2022-10-25 16:11:04+00:00,https://twitter.com/marktenenholtz/status/1584940617117102080,@amasad That's why Whisper was created!
675,@marktenenholtz,2022-10-25 16:09:14+00:00,https://twitter.com/marktenenholtz/status/1584940153490898944,"@amasad Any ML practitioner worth their salt will tell you that trying more new ideas is the best way to improve your models.

AI coding assistants are huge enablers of that!"
676,@marktenenholtz,2022-10-25 16:06:24+00:00,https://twitter.com/marktenenholtz/status/1584939440966729729,"@mervenoyann @abhi1thakur The ML space is definitely way more toxic than I expected when I started tweeting.

For the record, I think Yannic's joke and Francois' joke are hilarious 😂. Laughing at yourself is an important part of life."
677,@marktenenholtz,2022-10-25 15:58:08+00:00,https://twitter.com/marktenenholtz/status/1584937362995712002,"@danofer The real hype is a[i], not AI"
678,@marktenenholtz,2022-10-25 15:55:41+00:00,https://twitter.com/marktenenholtz/status/1584936744826601472,@abhi1thakur Even funnier considering this 😂 I guess jokes are funny again https://t.co/ufCFKSMDC0
679,@marktenenholtz,2022-10-25 15:48:30+00:00,https://twitter.com/marktenenholtz/status/1584934937731309569,"@svpino @GaryMarcus I also like Typefully. I wish it had Auto DM capabilities, but it's great for simple scheduling"
680,@marktenenholtz,2022-10-25 15:29:34+00:00,https://twitter.com/marktenenholtz/status/1584930171626213377,"@TheZachMueller @jeremyphoward Feels so much more idiomatic, right?

Here’s the high-level error description, and then the note tells you specifically what you did to trip it."
681,@marktenenholtz,2022-10-25 15:20:07+00:00,https://twitter.com/marktenenholtz/status/1584927794785460224,"If you enjoyed this, follow me @marktenenholtz for more high-signal Python and ML content!"
682,@marktenenholtz,2022-10-25 15:20:06+00:00,https://twitter.com/marktenenholtz/status/1584927791757172737,"The speed upgrade alone is exciting enough.

But it's just the first stop on a longer roadmap to make Python faster (and maybe JIT compiled?).

There's a master plan out there for Python 3.12 and beyond that has me extremely bullish on Python going forward https://t.co/xpP8T1vfrJ"
683,@marktenenholtz,2022-10-25 15:20:06+00:00,https://twitter.com/marktenenholtz/status/1584927788733112320,"5. A bunch of type hint stuff

There's now support for a Self type and marking TypedDict items as required or not.

But, the most exciting one for me are ""variadic generics"". Check this one out if you use libraries like NumPy: https://t.co/6z4uVg8U2n

In english... https://t.co/nU0i992y7E"
684,@marktenenholtz,2022-10-25 15:20:04+00:00,https://twitter.com/marktenenholtz/status/1584927782621937664,"4. Added notes on exceptions

Sometimes, when you're catching an error, it's helpful to leave an extra note to the user as to exactly *why* the error was raised. Many libraries (like Keras) spend a lot of time trying to enhance this.

Well, the .add_note() method now allows this! https://t.co/w3lluxEW1d"
685,@marktenenholtz,2022-10-25 15:20:03+00:00,https://twitter.com/marktenenholtz/status/1584927776255012865,"3. Grouped exceptions

We now have the introduction of the ""except*"" clase, which allows you to have more flexible error handling.

You can create a group of exceptions that may partial match with multiple of these except* clauses. https://t.co/vt1QgyZFwd"
686,@marktenenholtz,2022-10-25 15:20:01+00:00,https://twitter.com/marktenenholtz/status/1584927770399846403,"2. Better error messages

Anyone who has used a language like Rust knows that Python left something to be desired when it came to error messages.

Well, now Python has error messages that highlight the specific location of the problem! https://t.co/ed6Tmfr9vl"
687,@marktenenholtz,2022-10-25 15:19:58+00:00,https://twitter.com/marktenenholtz/status/1584927757896597504,"1. Faster CPython!

Python 3.11 is anywhere between 10-60% faster than Python 3.10, and on average it's 25% faster.

This includes faster startup times and more efficient use of/communication with C.

Here are the individual operations that are notably faster: https://t.co/2z187DxXHg"
688,@marktenenholtz,2022-10-25 15:19:56+00:00,https://twitter.com/marktenenholtz/status/1584927749168250881,"Python 3.11 is out! 🎉

This is one of the most exciting releases in a while, including significant speed upgrades and better error messages.

Here's what's new:"
689,@marktenenholtz,2022-10-25 13:55:41+00:00,https://twitter.com/marktenenholtz/status/1584906545732755456,@DThompsonDev *furiously digging through GitHub issues*
690,@marktenenholtz,2022-10-25 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1584877464802557953,"Modern ML prod pipelines are less about ""one great model"" than you think.

The bumpers and fallbacks that surround the model are just as important.

Try to make a perfect model all you want, but there's always a user/dataset that's dumber/more malicious/both than you planned for."
691,@marktenenholtz,2022-10-24 14:57:29+00:00,https://twitter.com/marktenenholtz/status/1584559710203949056,@math3mantic_ Metrics seems to be a common theme!
692,@marktenenholtz,2022-10-24 12:58:41+00:00,https://twitter.com/marktenenholtz/status/1584529815490932736,@TheZachMueller @fastdotai Congrats Zach! This is so exciting!
693,@marktenenholtz,2022-10-24 12:40:30+00:00,https://twitter.com/marktenenholtz/status/1584525239823630337,"@cmerk72 In your case, what about the non-stationary world gives your metrics issues?"
694,@marktenenholtz,2022-10-24 12:32:28+00:00,https://twitter.com/marktenenholtz/status/1584523217397776384,@rodbarest The last point sounds like a good problem to have!
695,@marktenenholtz,2022-10-24 12:30:42+00:00,https://twitter.com/marktenenholtz/status/1584522773598449665,@atarikkarakas Yeah it helps at massive scale but usually there are faster solutions that are more accurate
696,@marktenenholtz,2022-10-24 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1584515091432493056,What are the biggest challenges you face when building time-series forecasting models?
697,@marktenenholtz,2022-10-24 03:10:12+00:00,https://twitter.com/marktenenholtz/status/1584381716239482882,"@mhajabri @fastdotai @jeremyphoward @radekosmulski @iScienceLuvr For clarity, I’m not involved in the production of the courses. Just a big advocate for them."
698,@marktenenholtz,2022-10-23 18:47:43+00:00,https://twitter.com/marktenenholtz/status/1584255261581066240,@leonsolon @a_erdem4 @tunguz Maybe I should try to make LightGBM the household name to annoy him
699,@marktenenholtz,2022-10-23 17:57:08+00:00,https://twitter.com/marktenenholtz/status/1584242534561513473,@a_erdem4 I use LightGBM more but I feel like XGBoost is like a brand name
700,@marktenenholtz,2022-10-23 17:08:18+00:00,https://twitter.com/marktenenholtz/status/1584230244801597448,@a_erdem4 Same thing. I just sick of typing “XGBoost/LightGBM/Catboost” every time I mention those models.
701,@marktenenholtz,2022-10-23 16:34:44+00:00,https://twitter.com/marktenenholtz/status/1584221798085730306,"XGBoost is many things, but above all else it’s the best low-code ML tool out there"
702,@marktenenholtz,2022-10-23 14:20:01+00:00,https://twitter.com/marktenenholtz/status/1584187893966594049,@CHRAnonymous And PyTorch itself was like a different library
703,@marktenenholtz,2022-10-23 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1584152673380540417,"We talk all the time about how far large, pre-trained models have come and not enough about how far the tooling has come.

Do this: pick your favorite ML library and look at its docs from 4 years ago. It’s incredible."
704,@marktenenholtz,2022-10-22 17:14:56+00:00,https://twitter.com/marktenenholtz/status/1583869527799959552,@DSaience https://t.co/86iTq4cavL
705,@marktenenholtz,2022-10-22 15:27:20+00:00,https://twitter.com/marktenenholtz/status/1583842445783052288,@_brohrer_ This indeed
706,@marktenenholtz,2022-10-22 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1583790281500991488,"At least 60% of creating great time-series forecasting models is strong data wrangling skills.

Target leakage, bugs in features, etc. are far too common. Time-series data is really hard to get right."
707,@marktenenholtz,2022-10-21 20:17:45+00:00,https://twitter.com/marktenenholtz/status/1583553146898763776,@TheDSGuru I just paused and stared for a sec
708,@marktenenholtz,2022-10-21 20:16:05+00:00,https://twitter.com/marktenenholtz/status/1583552726977241093,@karpathy @ID_AA_Carmack But I could be very wrong
709,@marktenenholtz,2022-10-21 20:15:24+00:00,https://twitter.com/marktenenholtz/status/1583552553077592064,"@karpathy @ID_AA_Carmack I believe it’s because the Mersenne Twister algo (that torch uses) is a linear-feedback shift register algo, meaning it’s based on the result of bit-shifting a sequence of 1’s and 0’s"
710,@marktenenholtz,2022-10-21 19:56:20+00:00,https://twitter.com/marktenenholtz/status/1583547757528109057,@ID_AA_Carmack and the best one is the one they're using
711,@marktenenholtz,2022-10-21 19:56:03+00:00,https://twitter.com/marktenenholtz/status/1583547682437550080,@ID_AA_Carmack The worst seed is always the one that you're using
712,@marktenenholtz,2022-10-21 19:55:29+00:00,https://twitter.com/marktenenholtz/status/1583547541597356033,@AlexCoventry4 execute cell and go to the next one in Jupyter
713,@marktenenholtz,2022-10-21 19:53:42+00:00,https://twitter.com/marktenenholtz/status/1583547094027370496,@DSaience These test benchmarks don't beat themselves...
714,@marktenenholtz,2022-10-21 19:50:40+00:00,https://twitter.com/marktenenholtz/status/1583546330525204480,I know I've written too much code today because I just hit Shift+Enter to search something on Google
715,@marktenenholtz,2022-10-21 17:43:36+00:00,https://twitter.com/marktenenholtz/status/1583514350677295105,@gunesevitan You and me both
716,@marktenenholtz,2022-10-21 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1583427906021126145,"This just makes the already-powerful scikit-learn transformer pipelines that much more powerful.

It's currently released in their nightly build. You can find more about it here: https://t.co/WqYPneQGbY"
717,@marktenenholtz,2022-10-21 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1583427902703403008,"If you're like me, there's one change to scikit-learn you've been praying for, for years.

Scikit-learn transformers will now have a ""set_output()"" method to convert your output to a Pandas DataFrames... and my prayers have been answered.

Check it out! https://t.co/RUte6VIpDw"
718,@marktenenholtz,2022-10-21 03:04:14+00:00,https://twitter.com/marktenenholtz/status/1583293052348551168,@Austen You mean to tell me that a hustler hustled his way into some fake hustles?
719,@marktenenholtz,2022-10-20 13:09:10+00:00,https://twitter.com/marktenenholtz/status/1583082902446669824,"Just to emphasize: you can now get 30-40 hours of free, multi-GPU compute per week.

Follow me @marktenenholtz and I’ll show you more about what you can do with it!"
720,@marktenenholtz,2022-10-20 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1583065543040376832,"TL;DR

1. CPU kernel RAM upgrades
2. Multi-GPU offerings
3. Tensor cores (finally)
4. More compute for your quota

In short: 🤯"
721,@marktenenholtz,2022-10-20 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1583065540515360774,"3. 2xT4's == 1 P100 quota

The first thing I thought when I saw the T4 offering:

That's nice, but certainly that'll eat more into my GPU quota, right?

Nope. 1 hour spent using 2xT4's takes the same amount of your quota as a P100."
722,@marktenenholtz,2022-10-20 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1583065537734508545,"I went into more detail on the differences between data center GPU's here, if you're curious: https://t.co/nkOLi0RLVC"
723,@marktenenholtz,2022-10-20 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1583065535201230848,"2. Replace your P100 with 2xT4's

Most modern deep learning workloads use mixed precision training, and now you can finally get a GPU with tensor cores.

Multi-GPU are becoming very common and you can now learn how to use them without spending a dime."
724,@marktenenholtz,2022-10-20 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1583065532646846464,"1. CPU kernels now have 30 GB RAM

Bigger datasets become more accessible every day, and 16 GB could feel very restraining at times.

Heavy feature engineering, training models on large datasets, etc.

Now you get nearly double the RAM on a kernel w/o a usage quota."
725,@marktenenholtz,2022-10-20 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1583065530092527617,"For a while now, Kaggle has offered:

• 4-core CPU instances w/ 16 GB RAM
• 2-core CPU, P100 GPU w/ 13GB RAM

These are pretty standard resources as far as any free ML compute platform goes.

However, yesterday, they upgraded both of those."
726,@marktenenholtz,2022-10-20 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1583065527504601089,"Kaggle just significantly upgraded their available free compute.

Here's what happened:"
727,@marktenenholtz,2022-10-19 12:48:21+00:00,https://twitter.com/marktenenholtz/status/1582715272422510592,@rodbarest Any Manhattan distance will do
728,@marktenenholtz,2022-10-19 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1582703153534799872,"There's no ML problem I've come across that couldn't have been better solved by first going for a 20-minute walk.

There's no better ML-learning-productivity-hack than going to the gym for 20+ minutes every morning."
729,@marktenenholtz,2022-10-18 17:15:23+00:00,https://twitter.com/marktenenholtz/status/1582420085750132739,@danishdatty @__mharrison__ Yeah nobody except the thousands of people that love his book
730,@marktenenholtz,2022-10-18 17:11:50+00:00,https://twitter.com/marktenenholtz/status/1582419196000231425,"@thejustinwelsh You can either work backwards from the desired outcome or work forwards from a solution.

The first one guarantees a successful product, second one guarantees a failure to launch."
731,@marktenenholtz,2022-10-18 14:12:50+00:00,https://twitter.com/marktenenholtz/status/1582374146524450817,@psyfico It’s good to know. It often works very well in an ensemble with XGBoost. But XGBoost is still the priority.
732,@marktenenholtz,2022-10-18 14:12:10+00:00,https://twitter.com/marktenenholtz/status/1582373978299326465,@DSaience T-shaped learning is the way! Was it Andrew Ng that coined that or did he just popularize it?
733,@marktenenholtz,2022-10-18 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1582340756554977280,"There will certainly be some things you don't enjoy learning that are unavoidable. No doubt.

But at least you'll have some motivation to learn them."
734,@marktenenholtz,2022-10-18 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1582340753941938176,"The advice I wish I had when I started ML:

Deciding what not to learn is just as important as deciding what to learn.

ML as a field is as wide as an ocean and just as deep. You won't master everything.

Find what you actually enjoy learning and let that guide you."
735,@marktenenholtz,2022-10-17 12:19:46+00:00,https://twitter.com/marktenenholtz/status/1581983304957689856,"@mehdi_tweets @huggingface @wightmanr I’ve seen some folks say this, but I find it to be hard to apply to grouped time series with covariates"
736,@marktenenholtz,2022-10-17 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1581978346992603136,"The tooling trifecta for ML:

1. Tabular: XGBoost/LightGBM
2. Text: @huggingface Transformers
3. Vision: @wightmanr's timm

I can solve 99% of the problems I come across using those libraries."
737,@marktenenholtz,2022-10-17 02:58:12+00:00,https://twitter.com/marktenenholtz/status/1581841982141317120,@untitled01ipynb Hahaha… wait a minute
738,@marktenenholtz,2022-10-16 23:19:29+00:00,https://twitter.com/marktenenholtz/status/1581786939111665666,"Posted earlier about beating a benchmark set by @jeremyphoward on IMDb. 

It’s actually not the case because it didn’t include pretraining time. Whoops! Code is still here anyways:"
739,@marktenenholtz,2022-10-16 23:14:23+00:00,https://twitter.com/marktenenholtz/status/1581785656400891904,"@jeremyphoward Ah sorry, did not realize. I’ll update my claim"
740,@marktenenholtz,2022-10-16 21:32:44+00:00,https://twitter.com/marktenenholtz/status/1581760074565177344,@tunguz What if I manipulate the models that manipulate the digital objects?
741,@marktenenholtz,2022-10-16 20:54:50+00:00,https://twitter.com/marktenenholtz/status/1581750540177879040,"@TheZachMueller @weights_biases @fastdotai It’s fun! If you create a PR with new results, I’ll add them to the leaderboard. The current benchmark shouldn’t be that hard to beat :)"
742,@marktenenholtz,2022-10-16 17:06:35+00:00,https://twitter.com/marktenenholtz/status/1581693098190856192,"I did very little tweaking, but if you want to play around with it yourself, I added optional @weights_biases integration. Have fun!

Code here: https://t.co/OWek8t6bSp"
743,@marktenenholtz,2022-10-16 16:57:14+00:00,https://twitter.com/marktenenholtz/status/1581690744511033344,"@jeremyphoward @rasbt @tunguz Does this count? It's a deberta-v3-small model. Trained in 7 minutes, inference in 3.5 minutes on my undervolted 3080 Ti, uses ~8-9 GB VRAM (maybe less if your GPU isn't also your display GPU).

Pushed the code here: https://t.co/OWek8t6bSp https://t.co/CcGG1MuBbe"
744,@marktenenholtz,2022-10-16 15:39:57+00:00,https://twitter.com/marktenenholtz/status/1581671296089223171,"@tdinh_me As a dedicated user for almost a year now, it’s been incredible to watch your growth! Congrats!!!"
745,@marktenenholtz,2022-10-16 13:21:04+00:00,https://twitter.com/marktenenholtz/status/1581636342500405248,@moruobai Textbooks are fair game!
746,@marktenenholtz,2022-10-16 13:20:15+00:00,https://twitter.com/marktenenholtz/status/1581636140188127232,@FeedCompu @rasbt My favorite is his model evaluation paper
747,@marktenenholtz,2022-10-16 13:17:11+00:00,https://twitter.com/marktenenholtz/status/1581635366574227459,@realcid1 Love the Kaggle Book!
748,@marktenenholtz,2022-10-16 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1581615967217455104,What's your favorite intermediate-level ML/data science course/book?
749,@marktenenholtz,2022-10-15 22:20:53+00:00,https://twitter.com/marktenenholtz/status/1581409805570572289,@tunguz Kentucky bluegrass &gt;&gt;&gt; whatever Texas’ grass is
750,@marktenenholtz,2022-10-15 20:58:38+00:00,https://twitter.com/marktenenholtz/status/1581389107284316160,"@jeremyphoward @rasbt @tunguz Sorry, I moreso meant the amount of compute you used. I didn't see that anywhere in the paper"
751,@marktenenholtz,2022-10-15 19:25:36+00:00,https://twitter.com/marktenenholtz/status/1581365694591303681,@jeremyphoward @rasbt @tunguz Have you published those results anywhere? Could be fun to take a crack at it
752,@marktenenholtz,2022-10-15 18:45:17+00:00,https://twitter.com/marktenenholtz/status/1581355548112744448,"@rasbt @tunguz The reason I say that is because they’re the best example of LSTM performance (I’m confident they won’t beat the transformer, but I’m curious in the gap)"
753,@marktenenholtz,2022-10-15 18:40:52+00:00,https://twitter.com/marktenenholtz/status/1581354434633732096,"@rasbt @tunguz We should do this on a smaller dataset to show the advantage that pre-trained models really have.

But we also should benchmark ULMFiT and ELMO"
754,@marktenenholtz,2022-10-15 18:36:57+00:00,https://twitter.com/marktenenholtz/status/1581353449219710976,@rasbt @tunguz Well done! This is how real ML benchmarking works
755,@marktenenholtz,2022-10-15 17:55:18+00:00,https://twitter.com/marktenenholtz/status/1581342968417751040,@LeopolisDream More parameters -&gt; more eyes on your product
756,@marktenenholtz,2022-10-15 17:19:57+00:00,https://twitter.com/marktenenholtz/status/1581334072735199233,@Aastedet @Xvayzet3 I don’t think it’s semantics. Analytics doesn’t necessarily require ML
757,@marktenenholtz,2022-10-15 17:19:27+00:00,https://twitter.com/marktenenholtz/status/1581333949120679936,"@Xvayzet3 ML is creating a model that makes predictions.

Analytics is turning data (not just predictions) into useful business decisions."
758,@marktenenholtz,2022-10-15 17:00:12+00:00,https://twitter.com/marktenenholtz/status/1581329101696032770,"Machine learning raises funding rounds.

Analytics pays the bills."
759,@marktenenholtz,2022-10-15 16:41:38+00:00,https://twitter.com/marktenenholtz/status/1581324429191426055,"@EcoLaurenY Yeah it’s pretty untenable for most unless you want to go the ML Engineer route.

I’d say every data scientist should try to get at least a little exposure just so they can have an appreciation for the design decisions that need to be made."
760,@marktenenholtz,2022-10-15 16:15:42+00:00,https://twitter.com/marktenenholtz/status/1581317903357423618,"@machsci Yeah a huge problem is that it’s so arduous that it’s hard to show beyond a very specific example. 

Contrast that with building models, you can show off 50 CNNs in one tutorial"
761,@marktenenholtz,2022-10-15 16:14:15+00:00,https://twitter.com/marktenenholtz/status/1581317538670989312,@olihawkins The lack of sunshine that these tools get undoubtably hurts their ability to grow. It’s sad tbh
762,@marktenenholtz,2022-10-15 15:55:37+00:00,https://twitter.com/marktenenholtz/status/1581312849309143040,@rasbt Now that’s the good stuff
763,@marktenenholtz,2022-10-15 15:00:05+00:00,https://twitter.com/marktenenholtz/status/1581298872898330624,"Search for feature selection tutorials and you’ll find a million cloned blog posts.

Search for high quality serverless ML deployment tutorials and you’ll find crickets."
764,@marktenenholtz,2022-10-15 14:33:46+00:00,https://twitter.com/marktenenholtz/status/1581292250574647296,@EMCP_ The key word here is “underrated”
765,@marktenenholtz,2022-10-15 14:32:53+00:00,https://twitter.com/marktenenholtz/status/1581292027811356678,"@LukasValatka The difference between great skills and average skills is an approach being viable or not viable.

That’s incredibly valuable to a business."
766,@marktenenholtz,2022-10-15 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1581253578722660352,"Robust data pipelines are more pipelines are more important than the model itself.

We all know that.

But, great model building skill opens up more ML opportunities than you can imagine.

Great model building is the most underrated skill in the industry."
767,@marktenenholtz,2022-10-14 20:15:53+00:00,https://twitter.com/marktenenholtz/status/1581015962244943873,@tunguz @rasbt I assumed this would be a small dataset based on the conversation. Let's see where this goes...
768,@marktenenholtz,2022-10-14 19:27:30+00:00,https://twitter.com/marktenenholtz/status/1581003785907818499,@tunguz @rasbt Do I have to train it in a Kaggle env too?
769,@marktenenholtz,2022-10-14 19:25:22+00:00,https://twitter.com/marktenenholtz/status/1581003248034406401,@tunguz @rasbt Sounds like fun
770,@marktenenholtz,2022-10-14 14:31:47+00:00,https://twitter.com/marktenenholtz/status/1580929362865508353,@MeganRisdal That’s a beautiful concept. Celebrating how far you’ve come or remembering something great is always worth it.
771,@marktenenholtz,2022-10-14 14:25:43+00:00,https://twitter.com/marktenenholtz/status/1580927839662411778,"Part of this is that the field is incredibly elitist.

Show any sort of public confusion and folks often write you off as an idiot"
772,@marktenenholtz,2022-10-14 13:32:53+00:00,https://twitter.com/marktenenholtz/status/1580914541873225728,"@njgroene We all struggle learning most new stuff at this field to some degree, and we all struggled a lot at first.

Don’t be afraid to own it."
773,@marktenenholtz,2022-10-14 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1580891178219671554,"Stop writing generic explainer blog posts about ML basics.

Instead, write how *you* learned topics.

• What tripped you up?
• What’s your unique insight?
• How did you develop an intuition for it?

That’s how you help yourself and others learn, at the same time."
774,@marktenenholtz,2022-10-13 17:13:28+00:00,https://twitter.com/marktenenholtz/status/1580607666673815553,@jonst0kes “Guy who podcasts from an Airstream (that’s not a food truck) in Texas”
775,@marktenenholtz,2022-10-13 14:53:19+00:00,https://twitter.com/marktenenholtz/status/1580572396133953536,@ph_singer Do you usually find yourself doing small amounts of EDA in an iterative fashion? Or do you just do a little at first and then use problem-specific intuition from that point on?
776,@marktenenholtz,2022-10-13 14:48:36+00:00,https://twitter.com/marktenenholtz/status/1580571209259155458,"@ph_singer I probably should have made it more clear, but I usually focus on doing this in the error analysis phase. 

Sorting by top errors often helps me improve my augmentation strategy and find data issues."
777,@marktenenholtz,2022-10-13 14:11:59+00:00,https://twitter.com/marktenenholtz/status/1580561992804233217,@mohammad2012191 That's the plan!
778,@marktenenholtz,2022-10-13 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1580528839993659393,"TL;DR:

1. Immerse yourself in the data
2. Human baseline
3. Set up a pipeline
4. Overfit on a single batch
5. Add capacity
6. Reduce overfitting

Follow me @marktenenholtz for more high-signal ML content!

Reply to this with the tips/tricks you have!"
779,@marktenenholtz,2022-10-13 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1580528837464494080,"6. Repeat steps 5 + 6

Your goal now should be to keep trying to overfit and then reduce that overfitting.

You can stop when you run out of time or when you're no longer seeing improvements in validation accuracy when you regularize."
780,@marktenenholtz,2022-10-13 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1580528834880757760,"There are some tricks you can use, but they're more situational.

Here are some of them:

• Crop out as much background as possible
• Decrease batch size
• Add a learning rate scheduler (I always do this)

These are helpful, but usually not as much as the previous ones."
781,@marktenenholtz,2022-10-13 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1580528832267681793,"6. Reduce overfitting

Adding capacity usually causes you to overfit. Now, it's time to figure out how to regularize your model.

I usually follow these 4 steps, in order:

1. Get more data
2. Augmentation
3. Regularization (i.e. dropout, weight decay)
4. Smaller architecture"
782,@marktenenholtz,2022-10-13 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1580528829616963584,"5. Add capacity

In this stage, try to improve your loss as much as possible.

Try and pull different levers to accomplish this, such as:

• Increases to model size
• Increased image size
• Bigger output head

Make sure you only try one new thing at a time!"
783,@marktenenholtz,2022-10-13 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1580528827037458433,"4. Overfit on a single batch

Just as a QA step, see if you can overfit your model on a single image or a single batch of images.

This can uncover so many tricky bugs in your pipeline."
784,@marktenenholtz,2022-10-13 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1580528824399192064,"This is also where you should set up some QA steps.

Here are some I use (mostly borrowed from @karpathy):

• Set inputs to all zeros and compare loss to normal run
• Visualize some samples right before they enter your model
• Ensure training loss is decreasing"
785,@marktenenholtz,2022-10-13 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1580528821731663872,"3. Set up your pipeline

I start off with an extremely minimal pipeline. Here's my checklist:

• Fixed random seed
• No augmentation
• Small pretrained model (resnet18, efficientnet-b0)
• AdamW optimizer with no scheduler
• Implement logging
• Sanity check your metric"
786,@marktenenholtz,2022-10-13 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1580528818925670401,"2. Create a human baseline

While you're scrolling through images, try to get a gauge of your own accuracy.

Kaggle is nice in the sense that you get a leaderboard as a benchmark, but if you don't have this, a human benchmark is much better than a naive one, like the target mean."
787,@marktenenholtz,2022-10-13 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1580528816245514241,"You can use Jupyter widgets and @Gradio apps to make this faster, also.

While you're scrolling, ask yourself questions like:

• Does spatial position matter?
• Are there any data issues (i.e. duplicates)
• How noisy is the target?
• Is the target ever occluded?"
788,@marktenenholtz,2022-10-13 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1580528813636603904,"1. Immerse yourself in the data

The great part about image data is that it's as visual as it gets.

You should scroll through as many images as possible and try to find patterns.

The best models come from those who have spent hours on this, not minutes."
789,@marktenenholtz,2022-10-13 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1580528811011035136,"I've spent 100's of hours training computer vision models.

I've revisited my code and notes from those projects and distilled them into a repeatable process that anyone can follow.

Here are the 7 steps I take every time I train a vision model:"
790,@marktenenholtz,2022-10-13 04:15:26+00:00,https://twitter.com/marktenenholtz/status/1580411868744908800,"@bernhardsson I’ve found that if you do this for long enough, you find some really cool ideas when you combine your thoughts from different design docs"
791,@marktenenholtz,2022-10-12 15:07:56+00:00,https://twitter.com/marktenenholtz/status/1580213685389119488,@levelsio @huggingface DM me. I'm a Python guy but there's a chance I can help.
792,@marktenenholtz,2022-10-12 14:26:31+00:00,https://twitter.com/marktenenholtz/status/1580203265119354881,@LukasValatka Good model evaluation is the difference between having the trust of your stakeholders and not having a data science function
793,@marktenenholtz,2022-10-12 13:03:37+00:00,https://twitter.com/marktenenholtz/status/1580182400369569792,@tunguz @huggingface Wow! That’s an awesome haul! I’m jealous 😂
794,@marktenenholtz,2022-10-12 12:54:06+00:00,https://twitter.com/marktenenholtz/status/1580180006101479425,@thejustinwelsh Consistency is incredible. Showing up every day for 10 months has completely changed my life in more ways than I could’ve imagined.
795,@marktenenholtz,2022-10-12 12:46:30+00:00,https://twitter.com/marktenenholtz/status/1580178091732742144,@DanKornas Nothing makes me feel better than being able to convert an evaluation metric into $$$
796,@marktenenholtz,2022-10-12 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1580166419752267777,"The worst taught skill in ML is model evaluation.

Teaching model evaluation isn't just teaching simple K-Fold cross-validation.

It involves teaching how ML models fit into larger systems, and the business-guided goal it needs to accomplish.

Product + goal -&gt; proper evaluation"
797,@marktenenholtz,2022-10-11 15:13:21+00:00,https://twitter.com/marktenenholtz/status/1579852661167632385,"@__mharrison__ PyTorch + the RAPIDS libraries (cuDF, cuPy, cuML, etc)."
798,@marktenenholtz,2022-10-11 12:00:31+00:00,https://twitter.com/marktenenholtz/status/1579804132319166464,"Every ML project should keep the following documentation:

• Changelog
• Tech debt log
• Potential risks
• Experiment logs
• Future work ideas
• List of assumptions
• ETL pipeline description

Even on solo projects, it’s hard to remember everything. Write it down."
799,@marktenenholtz,2022-10-10 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1579441639168167936,"Machine learning roles that claim they are:

• 80% model building
• 20% data manipulation

Are actually:

• 45% communication
• 45% data manipulation
• 10% model building

Nobody is going to give you a dataset on a silver platter."
800,@marktenenholtz,2022-10-10 02:09:43+00:00,https://twitter.com/marktenenholtz/status/1579293066120335360,@Aella_Girl Some folks really didn’t like this one
801,@marktenenholtz,2022-10-09 13:29:14+00:00,https://twitter.com/marktenenholtz/status/1579101684093288448,@untitled01ipynb https://t.co/R4EN194qCH
802,@marktenenholtz,2022-10-09 13:23:52+00:00,https://twitter.com/marktenenholtz/status/1579100332856000512,@tunguz @AdamSinger Yes. We are.
803,@marktenenholtz,2022-10-09 13:22:26+00:00,https://twitter.com/marktenenholtz/status/1579099971789680641,@untitled01ipynb This is a much more pro-level meme
804,@marktenenholtz,2022-10-09 13:21:38+00:00,https://twitter.com/marktenenholtz/status/1579099771746209792,@svpino Prepare yourself for the firestorm!
805,@marktenenholtz,2022-10-09 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1579079249129967616,The real ones don't overfit https://t.co/Q9i052g48j
806,@marktenenholtz,2022-10-08 21:59:53+00:00,https://twitter.com/marktenenholtz/status/1578867804849459202,@tszzl Sounds like I just wasted 1.2 mW on that take. Sad.
807,@marktenenholtz,2022-10-08 21:48:24+00:00,https://twitter.com/marktenenholtz/status/1578864914642632705,@DSaience The same folks that love to attack medical CV researchers for overfitting (which they do) are BEGGING you to overfit on ImageNet 😂
808,@marktenenholtz,2022-10-08 21:46:51+00:00,https://twitter.com/marktenenholtz/status/1578864525394137088,@data_telling_d Yeah I care more about applicability than the tiniest bit more accuracy
809,@marktenenholtz,2022-10-08 18:50:35+00:00,https://twitter.com/marktenenholtz/status/1578820167131230208,"@joanfihu You don’t have access to the test set in Kaggle competitions. They’re designed to prevent overfitting.

The same can’t be said about academic benchmark dataset. Those datasets encourage overfitting."
810,@marktenenholtz,2022-10-08 17:07:08+00:00,https://twitter.com/marktenenholtz/status/1578794131518025729,@thedavescience Which new method do I get to claim SOTA on? Lol
811,@marktenenholtz,2022-10-08 17:00:04+00:00,https://twitter.com/marktenenholtz/status/1578792352411312128,"We need to redefine state-of-the-art (SOTA).

In my eyes, SOTA is any new method that changes the way practitioners approach a problem.

Not some completely impractical ensemble that's overfit on ImageNet."
812,@marktenenholtz,2022-10-08 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1578716860928864256,"The ML research world is switching its focus to generative image models just as the NLP world is hitting a great spot:

• Mature training tooling
• So many inference optimization methods
• Finetuning only takes a couple 1000 data points

ML engineers: NLP is ready for action."
813,@marktenenholtz,2022-10-07 17:00:05+00:00,https://twitter.com/marktenenholtz/status/1578429971109801984,"Babe, are you okay? You haven’t shared a paper about a new diffusion model today."
814,@marktenenholtz,2022-10-07 13:02:23+00:00,https://twitter.com/marktenenholtz/status/1578370151795527680,@leastsquared_ They’re referring to models that folks try (and fail) to put in production
815,@marktenenholtz,2022-10-07 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1578354494639050753,"90% of ML models don't make it to production.

...or do they?

This is one of the most misleading numbers quoted in the industry.

In reality, it just proves that applied ML is highly experimental. Data scientists try a lot of different approaches before deploying a model."
816,@marktenenholtz,2022-10-07 02:23:48+00:00,https://twitter.com/marktenenholtz/status/1578209446328680453,@tunguz @ylecun Boe Jan?
817,@marktenenholtz,2022-10-06 19:50:59+00:00,https://twitter.com/marktenenholtz/status/1578110592765730816,@tunguz @elonmusk It’ll be an entirely vision-based system
818,@marktenenholtz,2022-10-06 16:34:00+00:00,https://twitter.com/marktenenholtz/status/1578061020064522253,@xtinacomputes Take is too hot for me
819,@marktenenholtz,2022-10-06 15:59:43+00:00,https://twitter.com/marktenenholtz/status/1578052389965283329,@rasbt @tunguz @Datanerdkim Time is a flat circle... ML is all a matrix multiplication 😂
820,@marktenenholtz,2022-10-06 15:05:36+00:00,https://twitter.com/marktenenholtz/status/1578038771139780612,"@rasbt PI is still a good option, but LOFO is more accurate if you can afford the training time.

PI gives your model a different set of inputs than it was trained with, which is a bit more unrealistic than LOFO, and IMO provides a worse estimate."
821,@marktenenholtz,2022-10-06 13:21:16+00:00,https://twitter.com/marktenenholtz/status/1578012514717237255,"@sidradcliffe I love Sebastian and I’ve noticed the same thing, but I think Zachary is 100% the expert here since he is well-published in the field"
822,@marktenenholtz,2022-10-06 13:11:37+00:00,https://twitter.com/marktenenholtz/status/1578010088802963456,@sidradcliffe See this:
823,@marktenenholtz,2022-10-06 13:10:09+00:00,https://twitter.com/marktenenholtz/status/1578009716772540417,"@sidradcliffe LOFO isn’t a local interpretation mechanism.

Also, local methods like SHAP have fallen under serious doubt recently."
824,@marktenenholtz,2022-10-06 12:00:21+00:00,https://twitter.com/marktenenholtz/status/1577992151308247040,Here's a great implementation of LOFO (the one I use) by @a_erdem4 https://t.co/wpkh2OWAIn
825,@marktenenholtz,2022-10-06 12:00:20+00:00,https://twitter.com/marktenenholtz/status/1577992148716163077,"TL;DR:

1. Correlated features
2. Different model, different insights
3. Model agnostic
4. Negative feature importances
5. Strong generalization

Follow me @marktenenholtz for more high-signal ML content!"
826,@marktenenholtz,2022-10-06 12:00:19+00:00,https://twitter.com/marktenenholtz/status/1577992146186997760,"5. Strong generalization

Since LOFO is calculated over your cross-validation splits, it provides strong estimates of true importance.

Plus, you get a variance for each importance score to see if it ever drops into the negatives."
827,@marktenenholtz,2022-10-06 12:00:19+00:00,https://twitter.com/marktenenholtz/status/1577992143540404234,"4. Negative feature importances

If you overload your models with features, it will hurt performance.

LOFO actually assigns negative importance to those features.

(That's a feature, not a bug)"
828,@marktenenholtz,2022-10-06 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1577992140948312064,"3. Model agnostic

Even if I'm not using LightGBM, I can use LOFO with any model.

XGBoost, CatBoost, NNs, etc.

One really helpful technique: compare the feature importances for different models to see how robust they are."
829,@marktenenholtz,2022-10-06 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1577992138381357056,"2. Different model, different insights

If you're not going to use a linear model in production, why are you using it to generate insights for your production model?

The less disconnected your insights, the better you'll understand your actual production model."
830,@marktenenholtz,2022-10-06 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1577992135709577216,"1. Correlated features

Linear models struggle to provide useful insights when dealing with correlated features.

Out-of-the-box GBDT feature importances can also fall prey to this.

LOFO removes that worry."
831,@marktenenholtz,2022-10-06 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1577992133079756801,"Most data scientists use linear/logistic regression to figure out which features are important in a dataset.

I almost never do this.

Instead, I generally use leave-one-out feature importance (LOFO) + LightGBM.

Here's why:"
832,@marktenenholtz,2022-10-05 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1577629698745458688,"You should always make data-driven decisions while remembering that your data will never cover 100% of true reality.

The difference between 0% and 80% coverage is a solid data operation.

The difference between 90% and 99.9% is often incredibly intrusive surveillance."
833,@marktenenholtz,2022-10-05 00:04:33+00:00,https://twitter.com/marktenenholtz/status/1577449628034662400,"@jim_dowling Generally I'd say anything where you:

• Build towards a planned goal
• Manipulate/clean/etc the data
• Build a model
• Create a service to extract value from it"
834,@marktenenholtz,2022-10-04 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1577267322867154945,You can check it out here: https://t.co/TVgZjEOaHO
835,@marktenenholtz,2022-10-04 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1577267319385956355,"Project-based learning is the best way to build your ML skills.

You can scrape your own data, but that can be tricky (and expensive).

Google Research has released 117 datasets spanning text to images to time series data.

Want to try a new method? Use one of these as a basis! https://t.co/gAQLs48f1C"
836,@marktenenholtz,2022-10-03 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1576904933273522176,"Tools that bridge the gap between:

• ML and deployment infrastructure
• ML and training hardware
• ML and data

...are great.

But, we need more tools that bridge the gap between:

• ML and non-tech stakeholders
• ML and paying customers
• ML and decision-making"
837,@marktenenholtz,2022-10-03 02:57:11+00:00,https://twitter.com/marktenenholtz/status/1576768298322386946,@roydanroy Classic twitter 😂
838,@marktenenholtz,2022-10-03 00:05:37+00:00,https://twitter.com/marktenenholtz/status/1576725121863032833,@roydanroy Ah I see. I moreso meant seeking out a reason to e.g. use a huge NLP model and forcing a problem onto that solution.
839,@marktenenholtz,2022-10-02 12:36:17+00:00,https://twitter.com/marktenenholtz/status/1576551644992401408,@roydanroy When would you say working forwards from a solution is more useful?
840,@marktenenholtz,2022-10-02 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1576542533223469058,"Advice that changed my life:

Always work backwards from the problem. Not forwards from a solution.

Try to force a solution onto a problem and you might succeed 1% of the time.

Working backwards from the problem means you nearly always craft an ideal solution."
841,@marktenenholtz,2022-10-01 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1576180149283790848,"To add to this:

Modern ML models (deep learning especially) are very robust to bugs in your pipeline.

Corrupt data, bad augmentations, etc.

These errors are infuriating to debug (if you even know that they're happening).

Better code is the answer."
842,@marktenenholtz,2022-10-01 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1576180146695917568,"ML research papers with easy-to-use, open source code are 10x more likely to have a real impact.

ML models in industry with a robust, reliable codebase are 10x more likely to succeed in production.

Want your ML work to succeed?

Learn to write better code."
843,@marktenenholtz,2022-10-01 00:55:09+00:00,https://twitter.com/marktenenholtz/status/1576012809389084673,@svpino I’ll be joining the fold soon enough!
844,@marktenenholtz,2022-09-30 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1575817772180721665,"Question: ""Will X feature or Y feature help my model?""

Answer: Try both and see.

Q: ""Should I standardize or normalize?""

A: Try both and see.

Q: ""Are ViTs better than CNNs?""

A: Try both and see.

Beginners want absolutes.

Experts let model evaluation speak for itself."
845,@marktenenholtz,2022-09-29 20:17:51+00:00,https://twitter.com/marktenenholtz/status/1575580637964050433,"@giffmana @wightmanr @ducha_aiki Every time I try to write some extra fancy streaming dataloader or optimized disk loading beyond the standard stuff, I realize it'd probably be cheaper to run fewer experiments on a bigger machine 😂"
846,@marktenenholtz,2022-09-29 14:10:25+00:00,https://twitter.com/marktenenholtz/status/1575488169596035076,"@Muay_Khaoboy Sometimes poorly written code makes it nearly impossible for even the engineers to deploy it. 

Making your model easy to use is almost as important as how accurate it is."
847,@marktenenholtz,2022-09-29 14:09:02+00:00,https://twitter.com/marktenenholtz/status/1575487821225558018,@DSaience We definitely need a new definition of state-of-the-art
848,@marktenenholtz,2022-09-29 14:08:21+00:00,https://twitter.com/marktenenholtz/status/1575487650370179076,"@thedavescience Great point.

A lot of that stuff fits in with ""your model is worthless if nobody uses it"" (READMEs, documentation, etc), but compressing what you learn into writing is the greatest way to retain it."
849,@marktenenholtz,2022-09-29 13:59:07+00:00,https://twitter.com/marktenenholtz/status/1575485327938297858,@currypurin No problem! That one took me hours to figure out 😂
850,@marktenenholtz,2022-09-29 13:47:32+00:00,https://twitter.com/marktenenholtz/status/1575482410590179335,"@currypurin One thing to keep in mind: I'm pretty sure that running out of RAM gives you ""Notebook Exceeded Allowed Compute"", but running out of GPU RAM gives you ""Notebook Threw Exception,"" which confused me at first.

So, I always first make sure to reduce batch sizes."
851,@marktenenholtz,2022-09-29 12:10:25+00:00,https://twitter.com/marktenenholtz/status/1575457971823312898,@LeopolisDream Thanks Alex! Glad you liked it
852,@marktenenholtz,2022-09-29 12:00:21+00:00,https://twitter.com/marktenenholtz/status/1575455437159178240,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
853,@marktenenholtz,2022-09-29 12:00:20+00:00,https://twitter.com/marktenenholtz/status/1575455434579709952,"TL;DR:

1. Don't frontload the theory
2. Value comes first
3. Unused models are worthless
4. Model evaluation &gt;&gt;&gt; model building
5. Learn to reinvent yourself

I WISH someone told me this when I started learning ML.

Follow me @marktenenholtz for more high-signal ML content!"
854,@marktenenholtz,2022-09-29 12:00:20+00:00,https://twitter.com/marktenenholtz/status/1575455431987634177,"Learn how to re-learn the field

Your college courses are out-of-date because the field moves so quickly.

Want to stay relevant in industry?

Commit yourself to continual learning.

Seek out new models, MLOps tools, libraries, etc., and always be one step ahead."
855,@marktenenholtz,2022-09-29 12:00:19+00:00,https://twitter.com/marktenenholtz/status/1575455429408104448,"Model evaluation &gt;&gt;&gt; model building

If you can’t evaluate your model:

• You can’t improve it
• You can’t tell anyone how good it is
• (and most importantly) You can’t use it!

Using a poorly evaluated model is like traveling to Antarctica in your summer clothes. Bad move."
856,@marktenenholtz,2022-09-29 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1575455426841223168,"That’s right. Learn sales.

It usually takes a data scientist 2-3 years before they start to empathize with stakeholder needs.

Until you can do that, you’re forcing solutions on to problems, and that always ends in a disaster.

Once you learn to do it, your value skyrockets."
857,@marktenenholtz,2022-09-29 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1575455424332976128,"Your model is worthless if nobody uses it

If you can't communicate the value of your model, nobody will use it.

If nobody uses your model, then you just lit a big pile of money on fire.

Stakeholder management is crucial. Learning a bit of sales goes a long way."
858,@marktenenholtz,2022-09-29 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1575455421833191424,"ML is only as useful as it creates value

Nobody cares about your fancy model for a problem that could be solved by an if-statement.

(It's actually a bad thing)

Data scientists are hired for their problem-solving creativity first, and the ability to use ML to do it second."
859,@marktenenholtz,2022-09-29 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1575455419312394241,"Learn the theory as needed, not all at once

It's a BAD idea to try to calculate the derivative of a convolution before you've applied them to a few problems.

Data science is an inherently applied field, so learn it that way.

Learn enough theory to get started, and then start!"
860,@marktenenholtz,2022-09-29 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1575455416758153216,"A note:

ML is a very new field.

Just as we're still learning/inventing new ML methods, we're still learning how to teach it, too.

We need these programs to be strong if we want to cultivate strong ML builders that can solve problems when they hit the real (messy) world."
861,@marktenenholtz,2022-09-29 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1575455414069518337,"College ML usually have major holes.

Static courses are rarely up-to-date in a field moving this fast, plus they usually don’t teach the practical skills industry expects you to have.

Here are 5 changes I’d make to every college ML program:"
862,@marktenenholtz,2022-09-29 01:07:23+00:00,https://twitter.com/marktenenholtz/status/1575291114302701568,"@bernhardsson Python is living proof that network effects are the real deal.

Make a language that people want to use and are very productive while using, and great things happen!"
863,@marktenenholtz,2022-09-28 20:59:27+00:00,https://twitter.com/marktenenholtz/status/1575228717567451136,"@radekosmulski Wow, thank you so much Radek!

Quite an endorsement coming from someone like yourself!"
864,@marktenenholtz,2022-09-28 15:49:37+00:00,https://twitter.com/marktenenholtz/status/1575150745732669440,@rasbt And we wonder why so much research that comes out nowadays is completely useless
865,@marktenenholtz,2022-09-28 15:24:59+00:00,https://twitter.com/marktenenholtz/status/1575144547339878402,@jonst0kes I can’t remember the last time I got something less than a V100 on Pro+
866,@marktenenholtz,2022-09-28 14:36:20+00:00,https://twitter.com/marktenenholtz/status/1575132305433174016,@thedavescience It’s like a red pill haha
867,@marktenenholtz,2022-09-28 13:45:21+00:00,https://twitter.com/marktenenholtz/status/1575119473861967872,@tunguz okay but like I'm not kidding
868,@marktenenholtz,2022-09-28 13:44:58+00:00,https://twitter.com/marktenenholtz/status/1575119378638569472,@tunguz Whisper was created to allow GPT-4 to scale by expanding the amount of available training data
869,@marktenenholtz,2022-09-28 13:43:26+00:00,https://twitter.com/marktenenholtz/status/1575118992116695050,@antgoldbloom Here's a great notebook for BERTopic (if you haven't already seen it) https://t.co/2P0Owgdhnw
870,@marktenenholtz,2022-09-28 13:41:50+00:00,https://twitter.com/marktenenholtz/status/1575118589736140802,"@Tim_Dettmers I keep flipping back to full-precision Adam just to verify I'm not missing anything with 8-bit, and performance is basically always equal.

8-bit optimizers are probably the greatest trick out there right now."
871,@marktenenholtz,2022-09-28 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1575093001482739712,"XGBoost has proved many things, but one that flies under the radar:

It's not just how accurate the model is. It's also how easy/fast it is to use.

My training code for deep learning can be &gt;500 lines, easily.

My XGBoost training code (w/o feature engineering) is usually &lt;100."
872,@marktenenholtz,2022-09-28 01:06:59+00:00,https://twitter.com/marktenenholtz/status/1574928623617789952,"@svpino @karpathy One addition I'd propose:

When people say XGBoost, they also mean LightGBM and CatBoost.

When people say ResNet, they also mean EfficientNet.

...and so on."
873,@marktenenholtz,2022-09-27 23:47:27+00:00,https://twitter.com/marktenenholtz/status/1574908610605420544,@karpathy Wish I could attach this to all my tweets
874,@marktenenholtz,2022-09-27 17:41:40+00:00,https://twitter.com/marktenenholtz/status/1574816554864283648,"@abhi1thakur I think folks commonly hope it’s way easier than it actually is.

Just because the guide is simply put and straightforward to follow, it doesn’t mean it’s simple and straightforward to accomplish!"
875,@marktenenholtz,2022-09-27 17:34:10+00:00,https://twitter.com/marktenenholtz/status/1574814669973471233,"@schlange90 Great point, was hoping someone would say this.

You don’t need to be able to build it yourself. You just need to understand it enough to know what assumptions you can/cannot make."
876,@marktenenholtz,2022-09-27 16:41:51+00:00,https://twitter.com/marktenenholtz/status/1574801503264690179,"@machsci If you haven’t contacted 10 people to figure out why an anomaly is present in a column, are you really doing ML?"
877,@marktenenholtz,2022-09-27 12:00:24+00:00,https://twitter.com/marktenenholtz/status/1574730672987459584,"The better your data scientists know every step of the ELT pipeline, the better your models will be.

That's one of the strongest correlations you'll find in the whole field."
878,@marktenenholtz,2022-09-26 14:26:37+00:00,https://twitter.com/marktenenholtz/status/1574405084985040902,@svpino Most valuable type of ML out there!
879,@marktenenholtz,2022-09-26 13:06:54+00:00,https://twitter.com/marktenenholtz/status/1574385019870740480,@TheRmbomo Why waste 20 minutes when you could waste 2 hours?
880,@marktenenholtz,2022-09-26 13:06:24+00:00,https://twitter.com/marktenenholtz/status/1574384896906256385,@tunguz The inventor of treadmill desks must have created them just to annoy me
881,@marktenenholtz,2022-09-26 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1574368205799243777,"You can spend 2 hours staring at your screen, wondering why your code won’t work.

Or you can go for a 20 minute walk and probably figure it out right after."
882,@marktenenholtz,2022-09-25 15:52:07+00:00,https://twitter.com/marktenenholtz/status/1574064212657283072,@400trix That’s some inception shit right there
883,@marktenenholtz,2022-09-25 12:12:57+00:00,https://twitter.com/marktenenholtz/status/1574009056632643585,@JimPearse1 People that understand how to approach problems and craft data/ML solutions that actually generate business value
884,@marktenenholtz,2022-09-25 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1574005813319913473,"How many great ML builders do we lose because of our backwards interviewing methods?

I don't care if you know the math behind an SVM. I can teach you that.

I care if you can be a part of building something great."
885,@marktenenholtz,2022-09-24 13:56:59+00:00,https://twitter.com/marktenenholtz/status/1573672848627060737,@EMCP_ Haha yeah I know how you feel… that’s why I never use straight up AWS/GCP/Azure instances
886,@marktenenholtz,2022-09-24 13:37:21+00:00,https://twitter.com/marktenenholtz/status/1573667909905387520,@aidan_mclau Vast is a great one!
887,@marktenenholtz,2022-09-24 13:12:31+00:00,https://twitter.com/marktenenholtz/status/1573661659209637890,I just wish Colab wasn’t so notebook centric…
888,@marktenenholtz,2022-09-24 13:10:26+00:00,https://twitter.com/marktenenholtz/status/1573661134581911650,@untitled01ipynb @mrclbschff Yeah @tunguz does it all the time
889,@marktenenholtz,2022-09-24 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1573643445599539200,"Google Colab Pro/Pro+ is proof that pay-per-hour GPU costs can add way too much stress to the data science workflow.

What are your favorite cloud tools for model development?"
890,@marktenenholtz,2022-09-23 17:38:11+00:00,https://twitter.com/marktenenholtz/status/1573366130764558336,@aakashg0 Aakash! Congrats! Been great to follow your growth here.
891,@marktenenholtz,2022-09-23 13:25:59+00:00,https://twitter.com/marktenenholtz/status/1573302662438731779,@LukasPlatinsky Haha if it keeps getting optimized then we’ll see
892,@marktenenholtz,2022-09-23 13:25:33+00:00,https://twitter.com/marktenenholtz/status/1573302552967495680,"@janseben True, but less VRAM and compute than 2 3090s"
893,@marktenenholtz,2022-09-23 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1573281097487568896,"Hope you enjoyed! Follow me @marktenenholtz for more high-signal ML content!

Also, if you attended NVIDIA's GTC event, today is your last day to enter my giveaway to win a FREE 3080 Ti (courtesy of NVIDIA)!

Enter here: https://t.co/CCXGOPRrLh"
894,@marktenenholtz,2022-09-23 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1573281094799179777,"The 4090 is also seeing a bump in memory bandwidth and clock speed, which is also great news.

Finally, I'd also say it's a pretty solid deal that the MSRP only went up by $100. I think folks were scared there was going to be an even bigger price jump."
895,@marktenenholtz,2022-09-23 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1573281092085288960,"Power consumption: 450W (+100, 29% over 3090).

Yeah, well... that extra processing power has to come at some cost.

That's a huge power bump, so hopefully you've already got a big PSU installed (if you have your own system)."
896,@marktenenholtz,2022-09-23 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1573281089497407488,"Memory (VRAM): 24 GB (no change).

Not much of a surprise here.

My understanding is that upgrading VRAM is a pretty expensive thing to do, so considering the processing power boost, it's not a shock they opted to stick with 24 GB."
897,@marktenenholtz,2022-09-23 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1573281086947217409,"Tensor cores: 512 (+184, +56% over 3090).

This is huge for us deep learning enthusiasts.

FP16 operations already provide a very sizeable speedup, and increasing the number of tensor cores by 56% will just make that speedup even more absurd."
898,@marktenenholtz,2022-09-23 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1573281084460044288,"CUDA cores: 16384 (+5888, +56% over 3090).

That's a ridiculous upgrade.

CUDA cores are the cores that perform FP32 operations, so there's a ton more processing power available for generic tasks."
899,@marktenenholtz,2022-09-23 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1573281081956061184,"NVIDIA just announced the RTX 4090 and it looks insane!

Here are its top-line specs (compared to the 3090):"
900,@marktenenholtz,2022-09-22 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1572918653150003200,"Most data scientists think EDA is:

• Fancy plots
• Bright colors
• A few rows of raw data

This is so wrong.

Good EDA is anything, no matter how barebones, that helps to make REAL decisions.

You should look at tons of raw data and understand the intricacies of each feature."
901,@marktenenholtz,2022-09-22 01:46:50+00:00,https://twitter.com/marktenenholtz/status/1572764326804455426,"@radekosmulski Howdy, I build models and tweet about it (a lot)!"
902,@marktenenholtz,2022-09-22 00:22:19+00:00,https://twitter.com/marktenenholtz/status/1572743055752536064,@KLohrasbi And the easiest way to reduce waste is to tell folks they’ll significantly increase their profits while doing it.
903,@marktenenholtz,2022-09-21 12:40:56+00:00,https://twitter.com/marktenenholtz/status/1572566547352391680,"@LeopolisDream It’s true, but models like XGBoost give you so much more time to do it"
904,@marktenenholtz,2022-09-21 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1572556261648113664,"Tabular ML is very straightforward nowadays:

1. Explore your data
2. Solve the problem as if you’re doing it by hand
3. Create features that mimic those ideas
4. Fit XGBoost/LightGBM on those features

GBDT models are (currently) the ultimate low-code tool."
905,@marktenenholtz,2022-09-21 02:24:46+00:00,https://twitter.com/marktenenholtz/status/1572411485485244417,@xamat I guess not trusting the CDC has gone full circle 😂
906,@marktenenholtz,2022-09-21 02:11:09+00:00,https://twitter.com/marktenenholtz/status/1572408055496597505,@__mharrison__ Congrats Matt! Your books are a massive contribution to the world of data!
907,@marktenenholtz,2022-09-20 14:28:40+00:00,https://twitter.com/marktenenholtz/status/1572231272226820102,@MeganRisdal He just wanted you to miss him 😂
908,@marktenenholtz,2022-09-20 12:55:42+00:00,https://twitter.com/marktenenholtz/status/1572207875052666881,@thedavescience The first step is always communication!
909,@marktenenholtz,2022-09-20 12:55:13+00:00,https://twitter.com/marktenenholtz/status/1572207755628523521,@ilketopak That’s usually the first thing they need help with
910,@marktenenholtz,2022-09-20 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1572193877901348866,"Imagine how much less waste our economy would have if every small business had access to an accurate, reliable forecast..."
911,@marktenenholtz,2022-09-19 14:46:57+00:00,https://twitter.com/marktenenholtz/status/1571873484783128577,"@janseben They’re actually fantastic for ML. Usually the advantage that data center cards have is that they’re really well cooled and can run at max performance.

That being said, no RTX card can touch an A100."
912,@marktenenholtz,2022-09-19 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1571831515973324800,"All you've gotta do is register here: https://t.co/sWU9FJ7zmM

Then take a picture of yourself attending and submit it here to enter the giveaway: https://t.co/JS5oli3wVA"
913,@marktenenholtz,2022-09-19 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1571831510394867712,"And, if you want a GPU to do it with, you can have this exact one (that I took a bad picture of) for FREE courtesy of the fine folks at NVIDIA.

All you've gotta to is watch some amazing content put out by NVIDIA at their GTC conference starting TODAY! https://t.co/AYRHwMFnkz"
914,@marktenenholtz,2022-09-19 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1571831488827752450,"GPUs are NOT just for training deep learning models.

NVIDIA has built libraries for *drop-in* GPU acceleration of common ML tasks:

• cuPy for NumPy
• cuDF for Pandas
• cuML for Scikit-Learn
• cuGraph for NetworkX

Rapid iteration is key. Use your GPU for more than training."
915,@marktenenholtz,2022-09-18 23:49:36+00:00,https://twitter.com/marktenenholtz/status/1571647659362119680,@BenHanowell @NickDoesData Well said
916,@marktenenholtz,2022-09-18 21:14:43+00:00,https://twitter.com/marktenenholtz/status/1571608680604344322,"@ykilcher My first reaction was a laugh. Surprised (and not, at the same time) that not everyone had the same one"
917,@marktenenholtz,2022-09-18 13:27:56+00:00,https://twitter.com/marktenenholtz/status/1571491211961040896,"@NickDoesData I think the argument is that you have to massage the data to get it into AutoML tools anyways, which is what takes a lot of time.

From that point on, the custom modeling part really doesn't take that long. Talking on the scale of weeks (generously), not months."
918,@marktenenholtz,2022-09-18 12:50:33+00:00,https://twitter.com/marktenenholtz/status/1571481804590260226,"@LeopolisDream I would say that the results are all that matter.

The problem is just that once you pay the big bill associated with an AutoML tool, it’s what you’re expected to use even when you can do better on your own (even accounting for the time gain)."
919,@marktenenholtz,2022-09-18 12:49:09+00:00,https://twitter.com/marktenenholtz/status/1571481453292052480,"@NickDoesData The counter argument is that a skilled model builder can fairly straightforwardly outperform most AutoML tools on most problems, and it sorta encourages laziness on the feature engineering side."
920,@marktenenholtz,2022-09-18 12:47:22+00:00,https://twitter.com/marktenenholtz/status/1571481003574829057,@psyfico AutoML automates part of a data scientist’s job
921,@marktenenholtz,2022-09-18 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1571469097975820288,"Many data scientists don't understand the resistance to automating jobs with ML.

Until someone suggests that they use AutoML."
922,@marktenenholtz,2022-09-17 17:23:48+00:00,https://twitter.com/marktenenholtz/status/1571188181176926210,@tunguz I’m scared of Hormonal Diffusion
923,@marktenenholtz,2022-09-17 15:58:12+00:00,https://twitter.com/marktenenholtz/status/1571166642100379650,@matloff @rasbt Wow that’s why it’s called that? I genuinely had no idea. I always figured it had to be for SOME reason 😂
924,@marktenenholtz,2022-09-17 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1571106715818659844,"A model is NOT just a LightGBM/XGBoost/NN/etc.

A model is the algorithm *plus*:

• The parameters
• The data preprocessing
• The engineered features

Good deployment pipelines make these inextricable and/or easily reproducible."
925,@marktenenholtz,2022-09-16 19:57:22+00:00,https://twitter.com/marktenenholtz/status/1570864440177983488,@rasbt The most justified post so far :)
926,@marktenenholtz,2022-09-16 17:41:41+00:00,https://twitter.com/marktenenholtz/status/1570830294441426945,"@svpino I don’t disagree that they create a lot of value. The question, though, is do they generate more value than text?

Vision models have a huge disadvantage since our data capturing methods for vision have been around for a lot less time and text has an easier time scaling."
927,@marktenenholtz,2022-09-16 13:34:49+00:00,https://twitter.com/marktenenholtz/status/1570768169782116354,@tunguz Yeah I’m surprised by the answers I HAVEN’T seen honestly lol
928,@marktenenholtz,2022-09-16 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1570744352451985409,"In terms of economic value...

Tabular models have created $______
Vision models have created $______
Text models have created $______

How do you think they compare to each other?"
929,@marktenenholtz,2022-09-16 00:58:16+00:00,https://twitter.com/marktenenholtz/status/1570577777845489665,"@DanKornas That's a typo. Should have been ""features"""
930,@marktenenholtz,2022-09-15 19:20:38+00:00,https://twitter.com/marktenenholtz/status/1570492808179646466,@rahuldave @DSaience Well I've screamed at my models before so don't feel too bad 😂
931,@marktenenholtz,2022-09-15 19:12:29+00:00,https://twitter.com/marktenenholtz/status/1570490756326096897,@gusthema Oh yeah I mean there are tons. I just usually don’t use them 😂
932,@marktenenholtz,2022-09-15 18:40:46+00:00,https://twitter.com/marktenenholtz/status/1570482777572904962,@gusthema Usually just a matter of some sort of a linear optimizer to create a weighted average of their predictions
933,@marktenenholtz,2022-09-15 18:40:14+00:00,https://twitter.com/marktenenholtz/status/1570482640159117318,@arehmanAz09 @gusthema Just average their predictions or use a stacking model
934,@marktenenholtz,2022-09-15 18:39:50+00:00,https://twitter.com/marktenenholtz/status/1570482539382571009,@thedavescience It’s error analysis! I’ll talk about it some more at a later date
935,@marktenenholtz,2022-09-15 18:39:21+00:00,https://twitter.com/marktenenholtz/status/1570482417399484419,@DSaience It’s all about error analysis. Basically the EDA that occurs during the modeling process as opposed to the introduction
936,@marktenenholtz,2022-09-15 14:54:07+00:00,https://twitter.com/marktenenholtz/status/1570425738624368641,@LBacaj Actually it’s only been 10 months 😂
937,@marktenenholtz,2022-09-15 14:37:42+00:00,https://twitter.com/marktenenholtz/status/1570421603976224770,@LBacaj Best decision I’ve ever made
938,@marktenenholtz,2022-09-15 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1570381991572570112,"GPU acceleration is one of the easiest ways to make this faster. 

But you have to know how to do it.

Want to learn how AND win a free 3080 Ti? Sign up for the next GTC event (link in next tweet) and add your favorite sessions to your calendar.

(You'll love the RAPIDS session) https://t.co/ZmGOS8C7Cg"
939,@marktenenholtz,2022-09-15 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1570381982408019970,"How I approach (almost) all tabular ML problems:

1. EDA
2. Train a LightGBM on raw features
3. Create some obvious features
4. See what my model is missing
5. Create models to solve those errors
6. Repeat 4+5
7. (optional) ensemble with an NN

The faster you repeat, the better."
940,@marktenenholtz,2022-09-14 12:26:16+00:00,https://twitter.com/marktenenholtz/status/1570026141750104065,"@ergestx Definitely algorithms, but also hyperparameters, feature engineering techniques, etc."
941,@marktenenholtz,2022-09-14 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1570019563559071744,"Building the best ML models is less about picking what to try, and more about picking what NOT to try."
942,@marktenenholtz,2022-09-14 01:21:41+00:00,https://twitter.com/marktenenholtz/status/1569858893244776448,"@alt227Joydeep Anywhere from 1 to 10,000"
943,@marktenenholtz,2022-09-14 01:21:26+00:00,https://twitter.com/marktenenholtz/status/1569858831873622023,@gusthema Glad you liked it Gus!
944,@marktenenholtz,2022-09-13 13:42:15+00:00,https://twitter.com/marktenenholtz/status/1569682876966522890,@machsci We need better representation learning IMO. That’s what has allowed for scale in modalities like text and images.
945,@marktenenholtz,2022-09-13 12:11:25+00:00,https://twitter.com/marktenenholtz/status/1569660017460105217,"@thedavescience Yep, and tuning it is really only a matter of playing with a few of the full set of hyperparameters"
946,@marktenenholtz,2022-09-13 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1569657186347024384,"TL;DR:

1. GBDTs rule
2. More widespread than automated
3. Value &gt;&gt;&gt; cost
4. The illusion of understanding

If you liked this, follow me @marktenenholtz for more high-signal ML content!"
947,@marktenenholtz,2022-09-13 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1569657183541014529,"On top of all of that, local explainability tools like LIME and SHAP are very poorly understood, and (usually) don't give you the type of explainability that you think they do.

The illusion of understanding is more deceiving than accepting the black box-ness of a transformer."
948,@marktenenholtz,2022-09-13 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1569657180747624452,"If you use XGBoost a lot, you'd be amazed how few people use it poorly.

• Inapplicable feature engineering
• Tuning the wrong parameters
• Overthinking NaN handling
• Scaling data

We also don't fully understand why NNs don't work well."
949,@marktenenholtz,2022-09-13 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1569657178004537344,"Our understanding is an illusion

This might be a bit of a surprise to read.

But, after 100's of hours of teaching others and 1000's of hours of building my own models, I believe we vastly overestimate how well we understand tabular data."
950,@marktenenholtz,2022-09-13 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1569657172526796800,"There's plenty of AutoML tools, but even those ask you to formulate your own training data and help out with the feature engineering.

Those tools definitely can save data scientists time, but they serve a completely different role than, say, GPT-3."
951,@marktenenholtz,2022-09-13 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1569657169649500162,"The most widespread, the least commoditized

No-code, no-domain-knowledge inference APIs have certainly come for NLP, and now they're coming for computer vision.

But tabular data? Not really."
952,@marktenenholtz,2022-09-13 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1569657166885441538,"It's the GBDT show

The most efficient way to solve problems in tabular ML nowadays is just making XGBoost/LightGBM/CatBoost work.

NNs are also a good ensemble piece, but don't waste your time on models linear regression unless you're stacking or you have a really good reason."
953,@marktenenholtz,2022-09-13 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1569657164217843718,4 quick thoughts on the current state of tabular ML:
954,@marktenenholtz,2022-09-13 01:39:29+00:00,https://twitter.com/marktenenholtz/status/1569500985562918912,@thedavescience Maybe someday I’ll create one that’ll be on this list 👀
955,@marktenenholtz,2022-09-13 00:36:31+00:00,https://twitter.com/marktenenholtz/status/1569485140300267521,@radekosmulski The real magic are the systems that take the forecasts as input
956,@marktenenholtz,2022-09-12 12:13:58+00:00,https://twitter.com/marktenenholtz/status/1569298271063113729,"@LeopolisDream It’s less about amount of data and more about the capability to use it.

Data scientists are more expensive than a small retail store can afford."
957,@marktenenholtz,2022-09-12 12:12:31+00:00,https://twitter.com/marktenenholtz/status/1569297905202102272,"@thedavescience Sounds great and all, but if it’s not embedded in a decision making process you may as well have ML-as-a-waste-of-effort.

That’s what gives the companies with the most 💰 the advantage"
958,@marktenenholtz,2022-09-12 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1569294796593983490,"Forecasting has completely changed retail.

Mom &amp; pop shops are at an incredible logistical disadvantage.

We say ML is more democratized than ever, but there's still a massive disparity."
959,@marktenenholtz,2022-09-11 12:22:49+00:00,https://twitter.com/marktenenholtz/status/1568938108044861440,@levikul09 Build your own project that involves deploying a model and collecting your own data
960,@marktenenholtz,2022-09-11 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1568932382023221248,"Kaggle only helps you learn a portion of the full data science workflow.

And yet, it's the portion that we are worst at (as a field)."
961,@marktenenholtz,2022-09-10 14:27:56+00:00,https://twitter.com/marktenenholtz/status/1568607207574339584,@LeopolisDream Great answers. How do you feel about UMAP vs. t-SNE?
962,@marktenenholtz,2022-09-10 13:32:55+00:00,https://twitter.com/marktenenholtz/status/1568593364865454080,@carlolepelaars I figured this one would be the most popular. Gotta love it!
963,@marktenenholtz,2022-09-10 13:30:59+00:00,https://twitter.com/marktenenholtz/status/1568592874895187968,@Rob_Mulla This is why NLP problems are great 😂
964,@marktenenholtz,2022-09-10 12:00:28+00:00,https://twitter.com/marktenenholtz/status/1568570097593360385,What’s your favorite library for any step of the ML model training process?
965,@marktenenholtz,2022-09-09 21:23:25+00:00,https://twitter.com/marktenenholtz/status/1568349382117937152,"@__mharrison__ @rasbt @CalcCon Ah okay yes, then we agree!"
966,@marktenenholtz,2022-09-09 20:54:36+00:00,https://twitter.com/marktenenholtz/status/1568342129977466880,@__mharrison__ @rasbt @CalcCon Not even Gumroad?
967,@marktenenholtz,2022-09-09 12:25:06+00:00,https://twitter.com/marktenenholtz/status/1568213908988510213,@samoye95 I wouldn’t say it’s an enormous advantage. The amount of MLOps you can teach in most college programs is fairly limited.
968,@marktenenholtz,2022-09-09 12:05:56+00:00,https://twitter.com/marktenenholtz/status/1568209084591276033,@Albert_FPL Not that it isn’t applicable. Just that it’s out of date/oversimplified
969,@marktenenholtz,2022-09-09 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1568207629322964995,"Data scientists who learn ML in college definitely have an advantage.

But they only keep that advantage if they're willing to un-learn and re-learn almost everything they were taught."
970,@marktenenholtz,2022-09-08 14:12:36+00:00,https://twitter.com/marktenenholtz/status/1567878574568345603,@gib_hurst You’re not creating an individualized statistical model for every problem. You’re using a base architecture that works across a vast number of problems.
971,@marktenenholtz,2022-09-08 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1567845278908297217,"How to spend time building most ML models:

Old way:

• Arduous literature review
• Intense assumption checking
• Bespoke statistical representation

New way:

• Off-the-shelf model
• Product-lead modeling
• Data curation, evaluation, EDA

Spend your time wisely."
972,@marktenenholtz,2022-09-08 02:49:32+00:00,https://twitter.com/marktenenholtz/status/1567706675464867842,@thedavescience Great thread! Glad you pointed people to real competition datasets and not just Titanic-like datasets
973,@marktenenholtz,2022-09-07 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1567482869068763138,"If you build forecasting models, you are NOT in the model-building business.

You're in the decision-making business."
974,@marktenenholtz,2022-09-07 02:58:54+00:00,https://twitter.com/marktenenholtz/status/1567346643439075328,@rasbt Swap noise was invented specifically as an augmentation strategy for denoising autoencoders. Don’t think it’s widely applicable
975,@marktenenholtz,2022-09-06 17:36:51+00:00,https://twitter.com/marktenenholtz/status/1567205201051848707,@nischay_twt @tunguz In between is the 1.5th best time
976,@marktenenholtz,2022-09-06 15:34:56+00:00,https://twitter.com/marktenenholtz/status/1567174517142835207,"@LuisDuquePE Yep, and knowing how to properly evaluate your models can become more clear when you understand what your model will be used for"
977,@marktenenholtz,2022-09-06 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1567120444628828167,"Most data scientists spend their time:

• 90% on building models
• 10% on solution engineering

But, the value comes from:

• 10% from building models
• 90% from solution engineering

ML models are incredibly valuable, but remember: they are still software."
978,@marktenenholtz,2022-09-05 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1566758054644760576,"So many beginner-level ML questions just ""make sense"" once you've collected your own data.

• Model evaluation
• Target engineering
• Feature engineering
• Solution engineering
• Imputing missing values (or not)

100x less confusing when you have all the context."
979,@marktenenholtz,2022-09-04 18:49:27+00:00,https://twitter.com/marktenenholtz/status/1566498693091557376,"@rasbt Haha you joke, but Kaggle taught me how to solve problems at work way faster without any fanciness. I can get all of my fanciness out of the way in competitions 😂"
980,@marktenenholtz,2022-09-04 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1566395666661646338,"A senior data scientist knows that the best model is the simplest one that gets the job done.

A principal data scientist knows how to define what “gets the job done.”"
981,@marktenenholtz,2022-09-03 22:05:33+00:00,https://twitter.com/marktenenholtz/status/1566185658623877120,@giffmana @ykilcher “Don’t use pickles” is something I’ll abide by for Python but never for my diet
982,@marktenenholtz,2022-09-03 12:58:46+00:00,https://twitter.com/marktenenholtz/status/1566048053773172737,@One5Writer That’s the best way to learn!
983,@marktenenholtz,2022-09-03 12:35:28+00:00,https://twitter.com/marktenenholtz/status/1566042188986851328,"@svpino You can start with either one. It doesn’t matter.

You’ll switch to PyTorch eventually 😂"
984,@marktenenholtz,2022-09-03 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1566033279307771905,"The eternal cycle of learning ML:

1. Figure out what you don't know by taking courses and reading books.

2. Actually learn it by building your own projects.

3. Repeat 1+2

It really is that simple."
985,@marktenenholtz,2022-09-02 23:26:53+00:00,https://twitter.com/marktenenholtz/status/1565843738320158723,@radekosmulski @tunguz “Let me find a pattern that fits one person and ascribe it to a population”
986,@marktenenholtz,2022-09-02 23:26:24+00:00,https://twitter.com/marktenenholtz/status/1565843613707288577,"@tunguz If we relax it to &gt;= 6, then count me in"
987,@marktenenholtz,2022-09-02 17:41:06+00:00,https://twitter.com/marktenenholtz/status/1565756717790633989,@bernhardsson Hugging Face's transformers library does this well with a combo of classmethods and subclasses.
988,@marktenenholtz,2022-09-02 13:06:49+00:00,https://twitter.com/marktenenholtz/status/1565687692301672448,"@nishparadox You need to feed a model with high quality data, but you also need to trust that your model will do good things with that data"
989,@marktenenholtz,2022-09-02 12:18:01+00:00,https://twitter.com/marktenenholtz/status/1565675409072594944,@Saboo_Shubham_ The most reliable way to good performance!
990,@marktenenholtz,2022-09-02 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1565670892775956480,A model is only as good as its features.
991,@marktenenholtz,2022-09-01 18:31:28+00:00,https://twitter.com/marktenenholtz/status/1565407004612501504,@vad13irt Thanks Vadim! Keep it up on Kaggle
992,@marktenenholtz,2022-09-01 13:22:59+00:00,https://twitter.com/marktenenholtz/status/1565329373896605697,@ArmandOlivares3 Thanks for reading!
993,@marktenenholtz,2022-09-01 12:00:41+00:00,https://twitter.com/marktenenholtz/status/1565308663178285062,"Check out the event here to be eligible for the free 3080 Ti: https://t.co/sWU9FIPq8E

(Giveaway details will come closer to the event)

In the meantime, click the link to sign up for free and check out the other sessions!"
994,@marktenenholtz,2022-09-01 12:00:41+00:00,https://twitter.com/marktenenholtz/status/1565308660703961088,"To sum it up:

1. Evaluation = speed
2. GPUs everywhere
3. Limit your architectures
4. Kaggle solutions are a DB
5. Refactor more
6. Pipelines &gt; models

I hope you learned something!

Follow me @marktenenholtz to get more high-signal ML content!"
995,@marktenenholtz,2022-09-01 12:00:40+00:00,https://twitter.com/marktenenholtz/status/1565308658174402560,"Pipelines &gt; models

Most of the code that goes into training ML models is written either for getting the data to the model or getting the predictions out.

Want fast and reliable models? Spend more time improving your pipelines."
996,@marktenenholtz,2022-09-01 12:00:40+00:00,https://twitter.com/marktenenholtz/status/1565308655657840641,"Refactor, refactor, refactor

ML code can get out of hand really quickly.

With rapid iteration comes a rapid mess.

Don't be afraid to spend 30 minutes cleaning up your code instead of training models."
997,@marktenenholtz,2022-09-01 12:00:39+00:00,https://twitter.com/marktenenholtz/status/1565308653120339968,"Create your own framework

99% of my projects follow the exact same structure.

Down to the names of the files, what goes in them, and when they are created.

It's probably not the most efficient setup, but having such a deep knowledge of the structure makes me work much faster."
998,@marktenenholtz,2022-09-01 12:00:38+00:00,https://twitter.com/marktenenholtz/status/1565308650607955968,"Reuse old Kaggle solutions

Kaggle solutions are like a database for battle-tested solutions.

If you have a similar enough problem to an old competition, those solution writeups are an absolute goldmine.

My first stop on any problem for ""literature review"" is Kaggle."
999,@marktenenholtz,2022-09-01 12:00:38+00:00,https://twitter.com/marktenenholtz/status/1565308648095490048,"Pick 1 model and max it out

Your first goal should be to take one of those 3-5 models and improve it until you can't anymore.

On image tasks, I usually start by improving efficientnet-b0 until I can't anymore.

On tabular tasks, it's usually LightGBM or a random forest."
1000,@marktenenholtz,2022-09-01 12:00:37+00:00,https://twitter.com/marktenenholtz/status/1565308645448880129,"Skip to the good models

For any given problem, there are 3-5 model architectures you should focus (unless you have a really good reason).

The less time you're playing with models, the more time you're spending on high-leverage tasks (feature engineering, data exploration)."
1001,@marktenenholtz,2022-09-01 12:00:36+00:00,https://twitter.com/marktenenholtz/status/1565308642231848960,"In fact, NVIDIA has a session lined up for their next GTC event that goes deep into RAPIDS.

Even better: NVIDIA hooked me up with a 3080 Ti to send to one lucky winner who attends any of their FREE sessions (more on that below) https://t.co/o4AiqRILnk"
1002,@marktenenholtz,2022-09-01 12:00:34+00:00,https://twitter.com/marktenenholtz/status/1565308632341721090,"Use a GPU wherever you can

@RAPIDSai is an absolute blessing.

For 95% of my tabular problems, I can change 2 lines of code to make the whole thing run on my GPU. 

Everything from feature engineering to model training.

I've seen instances where this speeds pipelines up by 50x."
1003,@marktenenholtz,2022-09-01 12:00:34+00:00,https://twitter.com/marktenenholtz/status/1565308629758029825,"Model evaluation is the best time saver

Experiments done with a bad evaluation strategy are a waste of time.

Conversely, a good evaluation setup is the most powerful tool for iterating to a good solution.

This is arguably the highest-leverage time investment in an ML project."
1004,@marktenenholtz,2022-09-01 12:00:33+00:00,https://twitter.com/marktenenholtz/status/1565308627145019392,"I've spent 10,000+ hours building models over the last couple of years. 100's of them ended up in production.

Here are some of the best tips I learned along the way (that anyone can immediately apply):"
1005,@marktenenholtz,2022-08-31 13:35:59+00:00,https://twitter.com/marktenenholtz/status/1564970254756978688,@rasbt I always reference it when I’m writing up model evaluation training materials to make sure I’m not missing anything haha
1006,@marktenenholtz,2022-08-31 12:00:24+00:00,https://twitter.com/marktenenholtz/status/1564946202864467969,"I hope you find endless value from these like I did.

Follow me @marktenenholtz for more high-signal ML content!"
1007,@marktenenholtz,2022-08-31 12:00:23+00:00,https://twitter.com/marktenenholtz/status/1564946199840301061,"Training models is great, but you've gotta build something eventually.

@sandra_kublik and @Saboo_Shubham_ wrote a fantastic book on how to do just that with GPT-3: https://t.co/VZaoXVb4Jv"
1008,@marktenenholtz,2022-08-31 12:00:21+00:00,https://twitter.com/marktenenholtz/status/1564946191061618688,"Momentum, explained:
https://t.co/57K2shXi3k"
1009,@marktenenholtz,2022-08-31 12:00:21+00:00,https://twitter.com/marktenenholtz/status/1564946188310159360,"Deep Learning for Coders:

https://t.co/r8qGhGLvAa"
1010,@marktenenholtz,2022-08-31 12:00:20+00:00,https://twitter.com/marktenenholtz/status/1564946185391001600,"NYU's Deep Learning course:

https://t.co/4ju1S7HoVl"
1011,@marktenenholtz,2022-08-31 12:00:19+00:00,https://twitter.com/marktenenholtz/status/1564946182685614080,"Matrix Calculus for Deep Learning:

https://t.co/7L7ISqyQqU"
1012,@marktenenholtz,2022-08-31 12:00:19+00:00,https://twitter.com/marktenenholtz/status/1564946179887992832,"Essence of Linear Algebra from @3blue1brown:

https://t.co/rGpmE3kNXZ"
1013,@marktenenholtz,2022-08-31 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1564946177119834113,"If you want to apply the material you learned in the above course, use this library:
https://t.co/r6tXmACAhm"
1014,@marktenenholtz,2022-08-31 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1564946174385106945,Deep Learning for Computer Vision from University of Michigan: https://t.co/aEAfggK0UT
1015,@marktenenholtz,2022-08-31 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1564946171717570561,"Deep Learning with PyTorch: 
https://t.co/p2otzAh7xC"
1016,@marktenenholtz,2022-08-31 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1564946168945070081,"The best paper ever written on model evaluation from @rasbt:

https://t.co/eZ7NcXzZe6"
1017,@marktenenholtz,2022-08-31 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1564946164830482432,"This wonderful book on making the best use of Pandas, by @__mharrison__: https://t.co/hIaLgNsGnM"
1018,@marktenenholtz,2022-08-31 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1564946156555161601,"@huggingface's own book on NLP:
https://t.co/aPUYZupi3V"
1019,@marktenenholtz,2022-08-31 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1564946153950457856,"I've shared tons of free/inexpensive material for learning data science and ML (and not just the basics).

Combined, their value is easily greater than a $100k+ data science degree.

Here are some of my favorites:"
1020,@marktenenholtz,2022-08-30 14:40:14+00:00,https://twitter.com/marktenenholtz/status/1564624035551535106,@thedavescience I do. It's been absolutely instrumental in my development as a data scientist
1021,@marktenenholtz,2022-08-30 13:04:20+00:00,https://twitter.com/marktenenholtz/status/1564599901861908480,"At the very least, the framework of Kaggle competitions should inform how you run experiments"
1022,@marktenenholtz,2022-08-30 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1564583735185485824,Saying that learnings from Kaggle don't apply to real life is like saying that shoe companies have nothing to learn from Olympic sprinters.
1023,@marktenenholtz,2022-08-29 17:28:25+00:00,https://twitter.com/marktenenholtz/status/1564303974290923521,"@xtinacomputes imo this should be defined on the scale of ""% of texts with :)"" rather than ""# of :) per text"""
1024,@marktenenholtz,2022-08-29 13:52:07+00:00,https://twitter.com/marktenenholtz/status/1564249539845971968,@dLobatog @MetaAI That’s what creates a great talent pipeline
1025,@marktenenholtz,2022-08-29 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1564221362041802752,"Data scientists aren't hired to top tech companies because they've memorized every architecture and are a master of every tool.

They're hired because of their ability to learn it on the fly.

(...or at least that's how it should be)"
1026,@marktenenholtz,2022-08-28 14:38:56+00:00,https://twitter.com/marktenenholtz/status/1563898933738438658,@toiletkingcap Depends on the problem and who you ask
1027,@marktenenholtz,2022-08-28 14:13:21+00:00,https://twitter.com/marktenenholtz/status/1563892498048716806,"@JagersbergKnut @MLJARofficial I usually just use XGBoost/LightGBM + optuna directly. I always find I need lower level control than other libraries give me.

But sklearn’s CV splitting methods are invaluable!"
1028,@marktenenholtz,2022-08-28 12:20:47+00:00,https://twitter.com/marktenenholtz/status/1563864169111343108,"@PhilippBayer You should try multiple things and see what works, but I usually end up sticking to a linear optimizer to create an optimized weighted average"
1029,@marktenenholtz,2022-08-28 12:10:54+00:00,https://twitter.com/marktenenholtz/status/1563861681540849664,You can even argue that step one is unnecessary!
1030,@marktenenholtz,2022-08-28 12:10:37+00:00,https://twitter.com/marktenenholtz/status/1563861608446758919,@jrosell On both. You're probably going to test a lot of your features on both models.
1031,@marktenenholtz,2022-08-28 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1563858953838022656,"Highly accurate tabular ML modeling in 30 seconds:

1. Start building a random forest
2. Upgrade to XGBoost/LightGBM
3. Tune

Optional:
4. Create a deep learning model
5. Tune it
6. Add it to an ensemble

The most consistent you get with NNs in tabular modeling is in an ensemble."
1032,@marktenenholtz,2022-08-27 16:35:58+00:00,https://twitter.com/marktenenholtz/status/1563565998958989316,@HaffendenHector Good way to help with compressing your ideas!
1033,@marktenenholtz,2022-08-27 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1563496562650558471,"The best way to 10x your data science skills:

1. Learn the bare minimum to start
2. Start building something
3. Hit a roadblock
4. Learn what you need to progress
5. Repeat steps 2-4 until complete
6. Reflect on it
7. Compress it into simple ideas

Learn by building."
1034,@marktenenholtz,2022-08-27 01:54:20+00:00,https://twitter.com/marktenenholtz/status/1563344130138198016,@Austen I was too late but I would have guessed the rodeo
1035,@marktenenholtz,2022-08-26 17:22:53+00:00,https://twitter.com/marktenenholtz/status/1563215419049398273,"@MasslessChaps Well hey, I care! Thanks for sharing!"
1036,@marktenenholtz,2022-08-26 14:43:42+00:00,https://twitter.com/marktenenholtz/status/1563175359394496512,@Taylor_Street_ You can be special if you also use Feather
1037,@marktenenholtz,2022-08-26 14:43:08+00:00,https://twitter.com/marktenenholtz/status/1563175214435168258,@candeira @BendikAF Databricks now has me at Delta!
1038,@marktenenholtz,2022-08-26 14:42:54+00:00,https://twitter.com/marktenenholtz/status/1563175158998654976,@BendikAF Believe it or not!
1039,@marktenenholtz,2022-08-26 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1563134189456699394,I started storing my datasets on disk in Parquet format and never looked back.
1040,@marktenenholtz,2022-08-25 14:06:25+00:00,https://twitter.com/marktenenholtz/status/1562803586706579457,@HaffendenHector I’m praying!
1041,@marktenenholtz,2022-08-25 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1562771846126137348,That's ~$400 BELOW MSRP
1042,@marktenenholtz,2022-08-25 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1562771842896580612,"6 months ago: RTX 3090s cost $2250+, and good luck even finding one.

Today, with 2-day Prime shipping: https://t.co/BHfnwezfNH"
1043,@marktenenholtz,2022-08-24 18:54:51+00:00,https://twitter.com/marktenenholtz/status/1562513786971750401,"@BgProtesti They won prize money from Kaggle which was paid for by the NFL, had their work featured on national television, and a couple of the participants in the next competition the NFL hosted were hired by them."
1044,@marktenenholtz,2022-08-24 18:26:23+00:00,https://twitter.com/marktenenholtz/status/1562506622739501057,"@BgProtesti Yeah, usually the top 3-5 participants get prize money"
1045,@marktenenholtz,2022-08-24 13:50:01+00:00,https://twitter.com/marktenenholtz/status/1562437073667117059,"@machsci The paid competitions are the truly competitive ones, and there are usually only ~10 out at a time. 

Many competitions aren't straightforward either, and you'll struggle to even get a submission through in one day, let alone a competitive one.

But best of luck :D"
1046,@marktenenholtz,2022-08-24 12:41:49+00:00,https://twitter.com/marktenenholtz/status/1562419911212908544,@Evan_Coop I'd say you might be discounting just how much skill Kaggle competitors build throughout the course of competing
1047,@marktenenholtz,2022-08-24 12:40:19+00:00,https://twitter.com/marktenenholtz/status/1562419532219793408,@aliancagoias Hope you enjoy! Let me know which ones you're most interested in!
1048,@marktenenholtz,2022-08-24 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1562409438761594885,"Register here for free and check out some of the other data science-specific topics:
https://t.co/sWU9FJ7zmM https://t.co/ZD8SaffvRf"
1049,@marktenenholtz,2022-08-24 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1562409430138109953,"This isn't the only example of an extremely valuable winning solution, though.

NVIDIA's GTC conference (starting September 20) will have a talk about exactly this topic.

I'll even be giving away a 3080 Ti for FREE to one lucky winner... JUST for attending! (details forthcoming)"
1050,@marktenenholtz,2022-08-24 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1562409427365269505,"Credit to @ph_singer and @dott1718 for creating the model that is used by the NFL to this day!

You can read their solution here: https://t.co/lzqkCeYgHs"
1051,@marktenenholtz,2022-08-24 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1562409424001478658,"This allows them to significantly increase their production value and engage fans that want more depth.

They can generate graphics like this, and tell fans in real-time, during the broadcast, just how impressive (or depressing 😂) any particular play was. https://t.co/cGBHo3ASrD"
1052,@marktenenholtz,2022-08-24 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1562409412467490818,"Why does the NFL care about this?

Well, if you know how many yards the ""average"" rusher would have gained on a play, you can quantify the skill of rushers based on how many yards above (or below) expectation they gain throughout the season."
1053,@marktenenholtz,2022-08-24 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1562409409765986306,"The data you were given was the tracking data + trajectories for every player at the moment the ball was handed off to the rusher.

Simply a slice of the data that they already use."
1054,@marktenenholtz,2022-08-24 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1562409406868115458,"The goal of this competition was to predict how many yards a rusher will get on a play.

If you don't know what that means, it's a play like the ones in this video (until 1:30), where the ball is handed off by the quarterback (rather than throwing it). 

https://t.co/S17j0ydLbp"
1055,@marktenenholtz,2022-08-24 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1562409404179582977,"The competition was the NFL Big Data Bowl, hosted in 2019.

The NFL collects tracking data on all of its players. Internally, their datasets have coordinates for every player collected at a high frequency."
1056,@marktenenholtz,2022-08-24 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1562409401386160129,"Kaggle has a reputation for being ""fun, but impractical for real-world problems.""

But, contrary to popular belief:

Many of the winning solutions end up deployed into production by the organizer.

Here's my favorite story of a winning solution deployed by a $150B+ organization:"
1057,@marktenenholtz,2022-08-24 09:13:55+00:00,https://twitter.com/marktenenholtz/status/1562367591275970560,@dk21 Congrats!
1058,@marktenenholtz,2022-08-24 00:55:45+00:00,https://twitter.com/marktenenholtz/status/1562242223437238272,@SaasSavant @shepard_vic @SaaSWiz @saastux Lmk if you have any questions about the models🔥
1059,@marktenenholtz,2022-08-24 00:13:56+00:00,https://twitter.com/marktenenholtz/status/1562231697638309889,@radekosmulski First platform I saw do this was @jarvislabsai. I always wondered why this wasn't a feature everyone had.
1060,@marktenenholtz,2022-08-23 18:58:46+00:00,https://twitter.com/marktenenholtz/status/1562152386000551938,"My mistake: the company that created this (Stability AI) isn’t exactly a group of independent researchers, but I believe they support a bunch of independent research groups like Eleuther and LAION."
1061,@marktenenholtz,2022-08-23 13:46:48+00:00,https://twitter.com/marktenenholtz/status/1562073876103823360,"@glennko They released it for commercial use, so not sure there’s too much of a difference"
1062,@marktenenholtz,2022-08-23 12:13:19+00:00,https://twitter.com/marktenenholtz/status/1562050349938114562,@UzairMughal110 No problem. It is incredible.
1063,@marktenenholtz,2022-08-23 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1562047035036930048,"(Note: I'm not saying there's anything wrong with releasing DALLE-2 behind a paywall -- the inference API is a real service.)

Link: https://t.co/GL0F4JITTt"
1064,@marktenenholtz,2022-08-23 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1562047032172326912,"DALLE-2 was paywall-released recently by an extremely well-funded company.

Just yesterday, a group of independent researchers released their own model (Stable Diffusion) that you can use in a few lines of code for FREE.

The speed of the ML research community is insane 🤯 https://t.co/7NUoL6Xngx"
1065,@marktenenholtz,2022-08-22 17:39:58+00:00,https://twitter.com/marktenenholtz/status/1561770164261163009,@nischay_twt Oh yeah 100%. I’ve only ever seen good stuff on it on Kaggle as well.
1066,@marktenenholtz,2022-08-22 16:52:59+00:00,https://twitter.com/marktenenholtz/status/1561758340677525506,@nischay_twt You mean like random forests and XGBoost?
1067,@marktenenholtz,2022-08-22 14:56:21+00:00,https://twitter.com/marktenenholtz/status/1561728990959779849,@curiovana That’s my number 1
1068,@marktenenholtz,2022-08-22 14:56:05+00:00,https://twitter.com/marktenenholtz/status/1561728921506299904,@math_dandy I agree with this so much
1069,@marktenenholtz,2022-08-22 13:34:01+00:00,https://twitter.com/marktenenholtz/status/1561708269554569217,@tunguz I need a meta model to predict the best seed
1070,@marktenenholtz,2022-08-22 13:03:41+00:00,https://twitter.com/marktenenholtz/status/1561700637620011015,@NainaChaturved8 Extremely common theme. Kaggle helped me a ton with this
1071,@marktenenholtz,2022-08-22 13:02:58+00:00,https://twitter.com/marktenenholtz/status/1561700455369134085,@PoloniumBiscuit Surprised I see this one a lot even though it’s rarely used nowadays
1072,@marktenenholtz,2022-08-22 12:08:18+00:00,https://twitter.com/marktenenholtz/status/1561686699280850944,@V_J_S_1 The problem there is that almost nobody knows it well enough 😂
1073,@marktenenholtz,2022-08-22 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1561684632508067840,What's one machine learning topic that you wish was taught better in courses/books?
1074,@marktenenholtz,2022-08-22 11:20:54+00:00,https://twitter.com/marktenenholtz/status/1561674770604752899,"@rasbt Usually I prefer to just use knowledge distillation from a diverse ensemble into a single model for prod.

The problem I see with model soup is that it seems to require the ensemble to be made up of variations of one architecture, which usually isn’t nearly as effective."
1075,@marktenenholtz,2022-08-21 12:00:38+00:00,https://twitter.com/marktenenholtz/status/1561322383826247680,"You can follow someone else's roadmap to data science.

Or you can build your own projects and learn 100x more."
1076,@marktenenholtz,2022-08-20 23:45:44+00:00,https://twitter.com/marktenenholtz/status/1561137439984459777,"@goolig Major display of confidence in those situations, which is definitely linked to experience"
1077,@marktenenholtz,2022-08-20 21:38:53+00:00,https://twitter.com/marktenenholtz/status/1561105516818055179,@BornFitness @SahilBloom @hubermanlab You’ve helped me to remember that answers are framed far less in “what’s the correct answer” than they are in “what’s the good stuff and which one is right for me”
1078,@marktenenholtz,2022-08-20 21:36:50+00:00,https://twitter.com/marktenenholtz/status/1561105001258201090,@SaaSWiz European soccer stars know all too well
1079,@marktenenholtz,2022-08-20 21:21:35+00:00,https://twitter.com/marktenenholtz/status/1561101163302453249,@karlhigley I think your replies to this sum it up nicely and I think we agree 100%.
1080,@marktenenholtz,2022-08-20 19:46:54+00:00,https://twitter.com/marktenenholtz/status/1561077333527863298,"@karlhigley I’m with you, but I think the real question is how much needs to be glossed over.

You can explain anything simply if you leave out all the hard details"
1081,@marktenenholtz,2022-08-20 14:22:57+00:00,https://twitter.com/marktenenholtz/status/1560995810904612864,@Taylor_Street_ I’ve been building my skills for far too long 😈
1082,@marktenenholtz,2022-08-20 12:34:54+00:00,https://twitter.com/marktenenholtz/status/1560968619076493314,@_Alex_Adamov It does! Well said.
1083,@marktenenholtz,2022-08-20 12:34:05+00:00,https://twitter.com/marktenenholtz/status/1560968412024668161,@carsondahlberg Ohhhh man 😂
1084,@marktenenholtz,2022-08-20 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1560959858756554753,"A junior data scientist is proud of a highly-optimized, complex solution.

A senior data scientist is proud of the simplest solution that gets the job done effectively."
1085,@marktenenholtz,2022-08-19 16:17:17+00:00,https://twitter.com/marktenenholtz/status/1560662192709521409,@visarga Not how I'd do it but I think I like your method better
1086,@marktenenholtz,2022-08-19 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1560597485831106561,The first method that 10x'd my data science skills was working on compressing even the most complex methods into simple terms.
1087,@marktenenholtz,2022-08-19 02:17:25+00:00,https://twitter.com/marktenenholtz/status/1560450835229851651,@tibo_maker Amazing milestone 😂
1088,@marktenenholtz,2022-08-19 02:16:31+00:00,https://twitter.com/marktenenholtz/status/1560450608271867906,@alepiad Looks wonderful! Explains your cryptic tweets about hacking around in web dev 😂
1089,@marktenenholtz,2022-08-18 14:52:49+00:00,https://twitter.com/marktenenholtz/status/1560278550434816002,@ethantenison Thanks for consistently reading!
1090,@marktenenholtz,2022-08-18 14:39:23+00:00,https://twitter.com/marktenenholtz/status/1560275167594434561,"@PsycheNerd I don’t, but they have great intro material on their site"
1091,@marktenenholtz,2022-08-18 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1560235117435445248,"I could probably add 100 more.

Follow me @marktenenholtz for more high-signal ML content. At some point, I'll write a newsletter with more of these ideas: https://t.co/34sBfoLpMx"
1092,@marktenenholtz,2022-08-18 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1560235114868588544,"19. Don't be a tool zealot. Be skeptical of new things without being dismissive.

20. The most important part of learning is being able to simply explain what you just learned."
1093,@marktenenholtz,2022-08-18 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1560235112331022337,"17. 95%+ of blog posts written on intro to ML materials have some major flaws in their content. And that's okay, as long as you're aware of that when you're reading/writing them.

18. Industry and a lot of academia are probably 15+ years behind what is commonly known on Kaggle."
1094,@marktenenholtz,2022-08-18 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1560235109709537280,"15. A data scientist that is a master of stakeholder management is irreplaceable.

16. The only absolute truth in ML is that model evaluation is the most important step."
1095,@marktenenholtz,2022-08-18 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1560235107201404929,"13. Academic discourse gets very toxic, and there are a lot of academic types in data science. Don't let their gatekeeping discourage you, and don't stoop to their level.

14. Writing about data science is one of the best ways to learn it."
1096,@marktenenholtz,2022-08-18 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1560235104642818048,"11. If a modern ML method has a mathematical justification, it was probably created post-hoc.

12. It takes a lot longer to learn how to build than it takes to learn the math. You should learn the math along the way while you learn to build."
1097,@marktenenholtz,2022-08-18 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1560235102075887616,"8. The best metrics are ones that directly translate to $$$ (but this isn't always possible).

9. Kaggle is 100x better at determining which models work than academic benchmark datasets.

10. The biggest waste of time possible is running experiments with a poor evaluation setup."
1098,@marktenenholtz,2022-08-18 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1560235099534217216,"5. A bad model that is ""more interpretable"" will tell you step-by-step how to make bad predictions.

6. A great model that nobody uses is equivalent to lighting money on fire.

7. The measure of a data scientist's knowledge is how simple they make complex methods sound."
1099,@marktenenholtz,2022-08-18 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1560235097009168384,"3. There's no such thing as a ""best algorithm"" or ""best preprocessing method"", etc. You just have to go try it.

4. Linear/logistic regression is not the only simple model. A simple model is any that is a fast and easy-to-implement building block."
1100,@marktenenholtz,2022-08-18 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1560235094337433600,"20 simple ideas I would tell anyone learning data science:

1. Building intuition for how techniques work is the most powerful tool you have.

2. A great individual model is better to have than an ensemble of decent models.

(keep reading)"
1101,@marktenenholtz,2022-08-18 00:39:06+00:00,https://twitter.com/marktenenholtz/status/1560063704422653953,@JohnVial I think that’s a matter of preference. Once you get good enough at applying XGBoost I personally believe you can skip straight to it
1102,@marktenenholtz,2022-08-17 17:41:16+00:00,https://twitter.com/marktenenholtz/status/1559958554681004032,"@conorosullyDS If it's really easy to use with interpretability tools, then I classify it as easily interpretable."
1103,@marktenenholtz,2022-08-17 17:40:14+00:00,https://twitter.com/marktenenholtz/status/1559958294470459393,"I guess it needs to be said: XGBoost, LightGBM, and CatBoost are very, very similar models and can be used interchangeably here."
1104,@marktenenholtz,2022-08-17 14:46:28+00:00,https://twitter.com/marktenenholtz/status/1559914564363468800,@hugo_le_moine_ Also very underrated. I usually find pretty good results with just defaults
1105,@marktenenholtz,2022-08-17 12:26:38+00:00,https://twitter.com/marktenenholtz/status/1559879373754449920,@MolasAlex Works nicely with common interpretability tools + feature importances
1106,@marktenenholtz,2022-08-17 12:26:00+00:00,https://twitter.com/marktenenholtz/status/1559879215599722497,@OGsiji Just don’t overfit!
1107,@marktenenholtz,2022-08-17 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1559872709667872768,"XGBoost is not all you need.

It’s not:

• Always the fastest
• Always the most accurate
• Always the most interpretable

But, it’s usually:

• The fastest to great accuracy
• The best accuracy/speed tradeoff 
• The most accurate interpretable model

It’s worth sticking to."
1108,@marktenenholtz,2022-08-16 14:39:37+00:00,https://twitter.com/marktenenholtz/status/1559550450705342467,"@andrehaykaljr Machine learning is the same way. If I could tell you how good a strategy would be, we wouldn’t need the model!"
1109,@marktenenholtz,2022-08-16 12:51:03+00:00,https://twitter.com/marktenenholtz/status/1559523130007224321,"@dylanbert If you have grouped data and you randomly split it, your folds will be leaky and your validation is worthless.

Accounting for these problems is 90% of model evaluation."
1110,@marktenenholtz,2022-08-16 12:50:00+00:00,https://twitter.com/marktenenholtz/status/1559522865799651330,@nischay_twt That makes me nauseous
1111,@marktenenholtz,2022-08-16 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1559510318589493248,It's equal parts amazing and scary how many students come out of data science programs thinking the extent of model evaluation is simple random cross-validation.
1112,@marktenenholtz,2022-08-15 15:08:02+00:00,https://twitter.com/marktenenholtz/status/1559195215474327552,@geofurb We're all in this tweet
1113,@marktenenholtz,2022-08-15 12:56:20+00:00,https://twitter.com/marktenenholtz/status/1559162070683320320,@tunguz iterative-stratification https://t.co/AEWrhWIe4h
1114,@marktenenholtz,2022-08-15 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1559147915481780224,"Every data scientist should know Parkinson’s Law:

It claims work expands to fill the time you have available to complete it.

You can try a million different models, preprocessing techniques, etc.

But at some point, you need to actually wrap it up and deliver some value"
1115,@marktenenholtz,2022-08-14 23:43:27+00:00,https://twitter.com/marktenenholtz/status/1558962538238328832,@LBacaj Acting like income is the only thing you can bring to the table is such a toxic perspective
1116,@marktenenholtz,2022-08-14 23:42:06+00:00,https://twitter.com/marktenenholtz/status/1558962194796236800,@JurgenDrv Story of my life
1117,@marktenenholtz,2022-08-14 23:41:37+00:00,https://twitter.com/marktenenholtz/status/1558962077242359810,@ErrataRob Law is very misrepresented but at least they get some of it right
1118,@marktenenholtz,2022-08-14 20:55:12+00:00,https://twitter.com/marktenenholtz/status/1558920196542402561,@catalinbanu I can die happy when I see XGBoost in a Hollywood film
1119,@marktenenholtz,2022-08-14 20:54:02+00:00,https://twitter.com/marktenenholtz/status/1558919900764192768,@guysnovelutumba Thank you!
1120,@marktenenholtz,2022-08-14 17:00:07+00:00,https://twitter.com/marktenenholtz/status/1558861034793750528,I spend 30 minutes implementing my model and 3 hours finding the tiny bug in my pipeline.
1121,@marktenenholtz,2022-08-14 12:08:16+00:00,https://twitter.com/marktenenholtz/status/1558787589364224001,"@RishiSrinivas14 Right, only data scientists wear hoodies"
1122,@marktenenholtz,2022-08-14 12:05:33+00:00,https://twitter.com/marktenenholtz/status/1558786904396627971,@thinker_mann I'M IN
1123,@marktenenholtz,2022-08-14 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1558785547992084481,That or cybersecurity
1124,@marktenenholtz,2022-08-14 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1558785545525809152,Machine learning is probably the most misrepresented field in movies and TV shows.
1125,@marktenenholtz,2022-08-13 14:01:13+00:00,https://twitter.com/marktenenholtz/status/1558453625608871938,@bhutanisanyam1 @kaggle @CroDoc @__hsuya @nischay_twt @shubh_KS08 We’ll find a way :)
1126,@marktenenholtz,2022-08-13 12:52:54+00:00,https://twitter.com/marktenenholtz/status/1558436433186197505,@bhutanisanyam1 @kaggle @CroDoc @__hsuya @nischay_twt @shubh_KS08 Congrats!
1127,@marktenenholtz,2022-08-13 12:32:33+00:00,https://twitter.com/marktenenholtz/status/1558431312264011778,"@Eyquem_ That’s maybe 20% of the work. Actually collecting the data, solution engineering, model evaluation are all huge pieces"
1128,@marktenenholtz,2022-08-13 12:29:51+00:00,https://twitter.com/marktenenholtz/status/1558430633076133893,"@eaglyl The problem isn’t that there are basic tutorials. The problem is that there’s very, very little beyond that."
1129,@marktenenholtz,2022-08-13 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1558423131626168322,"ML tutorials on blogs:

• Curated dataset
• Basic features
• No evaluation effort
• Methods are outdated

Real life:

• Messy/nonexistent dataset
• Highly creative feature engineering
• Thorough evaluation
• Constant search for new methods

We need to evolve our tutorials"
1130,@marktenenholtz,2022-08-12 21:03:33+00:00,https://twitter.com/marktenenholtz/status/1558197520571957249,@rasbt You 100% deserve it. Congrats!
1131,@marktenenholtz,2022-08-12 19:01:14+00:00,https://twitter.com/marktenenholtz/status/1558166740646903809,@machsci https://t.co/4L1HdPOvAD
1132,@marktenenholtz,2022-08-12 15:17:33+00:00,https://twitter.com/marktenenholtz/status/1558110446493769733,"@Ak_Python If it makes sense for your data, then sure"
1133,@marktenenholtz,2022-08-12 14:39:14+00:00,https://twitter.com/marktenenholtz/status/1558100804837543938,"@abhi1thakur I do, but I also consider it to be its own type of tabular data. That's probably a pretty annoying answer 😂

I (usually) consider tabular data to be anything that is best represented in a SQL table, and time series data at least fits that."
1134,@marktenenholtz,2022-08-12 12:58:57+00:00,https://twitter.com/marktenenholtz/status/1558075565164331009,"@GaelVaroquaux Ideally, yes I 100% agree"
1135,@marktenenholtz,2022-08-12 12:09:56+00:00,https://twitter.com/marktenenholtz/status/1558063229821222912,@tunguz XGBoost take the wheel
1136,@marktenenholtz,2022-08-12 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1558060756569972736,"I often get asked:

“What’s the best way to fill NaN’s?”

Short answer: there isn’t one.

Better answer:

Use your domain knowledge for all of the features you can (usually for values not missing at random).

For the rest, cross-validate a bunch of different strategies."
1137,@marktenenholtz,2022-08-12 11:53:35+00:00,https://twitter.com/marktenenholtz/status/1558059115045236736,@iScienceLuvr @StableDiffusion @ylecun @geoffreyhinton @AndrewYNg Looks like a Jeremy Howard + Malcolm Gladwell hybrid
1138,@marktenenholtz,2022-08-11 20:26:47+00:00,https://twitter.com/marktenenholtz/status/1557825878993813504,@tunguz This poll is correct because it confirms my priors
1139,@marktenenholtz,2022-08-11 19:41:43+00:00,https://twitter.com/marktenenholtz/status/1557814537176465409,"@hugo_le_moine_ I see what you mean but that’s not strictly what I meant.

For example, a forecasting algorithm replaces a store manager’s “finger in the wind” forecast."
1140,@marktenenholtz,2022-08-11 19:40:17+00:00,https://twitter.com/marktenenholtz/status/1557814178886328321,@BrittaRude Thanks Britta!
1141,@marktenenholtz,2022-08-11 19:29:41+00:00,https://twitter.com/marktenenholtz/status/1557811511434911746,"@ChrSzegedy @tunguz “Perfect professional headshot, step by step”"
1142,@marktenenholtz,2022-08-11 15:36:11+00:00,https://twitter.com/marktenenholtz/status/1557752748992503809,@jsulopzs Depends on what their role is I’d say
1143,@marktenenholtz,2022-08-11 13:27:36+00:00,https://twitter.com/marktenenholtz/status/1557720388083355654,"@Taylor_Street_ That’s true, but automation can often involve something like, say, replacing a store manager’s “finger in the wind” forecast with an automated ML model"
1144,@marktenenholtz,2022-08-11 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1557698358411608064,"Data scientists often forget:

Your job isn't always to outperform humans.

Often, it's just to automate a boring task to free a human up to do something more productive.

Challenge yourself to think more about utility than pure accuracy."
1145,@marktenenholtz,2022-08-10 19:21:52+00:00,https://twitter.com/marktenenholtz/status/1557447156612595714,@PredaGabi I’ve been outed
1146,@marktenenholtz,2022-08-10 16:00:51+00:00,https://twitter.com/marktenenholtz/status/1557396568503894016,@datacascadia Yeah! There are corresponding unordered functions if you don’t care about the order
1147,@marktenenholtz,2022-08-10 15:27:14+00:00,https://twitter.com/marktenenholtz/status/1557388106483089411,@rrherr I love this!
1148,@marktenenholtz,2022-08-10 12:54:33+00:00,https://twitter.com/marktenenholtz/status/1557349685286899712,@BenGaffinet Straightforward is the only way I like it!
1149,@marktenenholtz,2022-08-10 12:37:29+00:00,https://twitter.com/marktenenholtz/status/1557345387974893570,@hugo_le_moine_ @modin_project Modin is great but I don’t always want to add another dependency
1150,@marktenenholtz,2022-08-10 12:36:42+00:00,https://twitter.com/marktenenholtz/status/1557345191685660673,@sacdallago What if you don’t have a GPU?
1151,@marktenenholtz,2022-08-10 12:15:46+00:00,https://twitter.com/marktenenholtz/status/1557339922977144832,"Line 20 should be:

res = pd.concat(res)"
1152,@marktenenholtz,2022-08-10 12:14:53+00:00,https://twitter.com/marktenenholtz/status/1557339700385484800,@code_star Oh yeah my code must have not saved. Good catch
1153,@marktenenholtz,2022-08-10 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1557336004721160192,"Common Pandas problem:

You have a big dataframe and a function that can't be easily vectorized.

So, you want to run it in parallel. Surprisingly, most answers on StackOverflow just point you to a different library.

So here's a little recipe I use: https://t.co/ZFsPHLp6ms"
1154,@marktenenholtz,2022-08-09 14:04:09+00:00,https://twitter.com/marktenenholtz/status/1557004811257741315,@arankomatsuzaki Released too early in the sense that it’s overlooked now?
1155,@marktenenholtz,2022-08-09 13:13:15+00:00,https://twitter.com/marktenenholtz/status/1556992000838275072,@dav_ell I have some other plans
1156,@marktenenholtz,2022-08-09 12:28:56+00:00,https://twitter.com/marktenenholtz/status/1556980850759180291,@grad2vec ❤️
1157,@marktenenholtz,2022-08-09 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1556973591266148353,"My mission is to teach every data scientist what I learned over 1000+ hours of building models and putting them into production.

The state-of-the-art moves forward very quickly, and I want everyone to be as close as possible."
1158,@marktenenholtz,2022-08-08 17:13:25+00:00,https://twitter.com/marktenenholtz/status/1556690054235447297,"@undrash @nachimak28 Some folks feel it reinforces poor coding practices/messy code, and some think it makes the transition into prod more difficult"
1159,@marktenenholtz,2022-08-08 16:52:05+00:00,https://twitter.com/marktenenholtz/status/1556684687111774211,@historysheeter These tweets radicalized me on explainability https://t.co/NaTYXX8cUW
1160,@marktenenholtz,2022-08-08 15:58:47+00:00,https://twitter.com/marktenenholtz/status/1556671271827636226,@StatsGary That will get violent quickly
1161,@marktenenholtz,2022-08-08 15:01:57+00:00,https://twitter.com/marktenenholtz/status/1556656969058066437,@tunguz @rasbt Yeah I've run into this before. One of the worst silent bugs I've run into
1162,@marktenenholtz,2022-08-08 14:09:50+00:00,https://twitter.com/marktenenholtz/status/1556643851972055040,"@BhavisheyT @tng_konrad This is a tweet on how to start fights, not on how to end them!"
1163,@marktenenholtz,2022-08-08 13:21:13+00:00,https://twitter.com/marktenenholtz/status/1556631618936266755,@tng_konrad That’s an easy win right there
1164,@marktenenholtz,2022-08-08 13:01:34+00:00,https://twitter.com/marktenenholtz/status/1556626672069361664,@mf_benedetto It’s definitely easier now
1165,@marktenenholtz,2022-08-08 13:01:00+00:00,https://twitter.com/marktenenholtz/status/1556626533472690179,@HaffendenHector Oh I’ve never seen a fight between this one but I can totally see it
1166,@marktenenholtz,2022-08-08 12:40:40+00:00,https://twitter.com/marktenenholtz/status/1556621414626729984,@nezubn Get over here @tunguz
1167,@marktenenholtz,2022-08-08 12:07:48+00:00,https://twitter.com/marktenenholtz/status/1556613141546602496,@Nandini_HS Ha that’s more like between data scientists and statisticians but I’d say it works for both
1168,@marktenenholtz,2022-08-08 12:07:21+00:00,https://twitter.com/marktenenholtz/status/1556613029193818112,@DzeRichard Another true classic
1169,@marktenenholtz,2022-08-08 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1556611211994681344,"What's the easiest way to start a fight between data scientists?

My answer: ""What's the definition of overfitting?"""
1170,@marktenenholtz,2022-08-08 00:14:09+00:00,https://twitter.com/marktenenholtz/status/1556433546369269761,"@ai_for_humans @svpino I would say it's the exception to the norm that you have enough data to be extremely confident in just having one validation set, so I was assuming cross-validation"
1171,@marktenenholtz,2022-08-07 23:04:55+00:00,https://twitter.com/marktenenholtz/status/1556416123733741568,@JesperDramsch Yeah that’s how all the callbacks in PyTorch/TF are used by folks
1172,@marktenenholtz,2022-08-07 22:50:06+00:00,https://twitter.com/marktenenholtz/status/1556412397044305921,@JesperDramsch Said another way here: https://t.co/SNz7VjtNpF
1173,@marktenenholtz,2022-08-07 22:49:44+00:00,https://twitter.com/marktenenholtz/status/1556412304442511361,"@bailey_stimac Every one of your validation sets is slightly different, no matter how well you split your data.

Your goal is to find the set of hyperparameters (training steps included) that works best over ALL validation sets, not just one or two."
1174,@marktenenholtz,2022-08-07 22:48:16+00:00,https://twitter.com/marktenenholtz/status/1556411933649260544,"@JesperDramsch The number of steps that you train for is one of the most important hyperparameters for your model.

You can always tweak it, but if you train on some folds longer than others, then you're overfitting a model onto a subset of your total validation data."
1175,@marktenenholtz,2022-08-07 22:33:43+00:00,https://twitter.com/marktenenholtz/status/1556408271770107905,@dariogargas I don't understand the argument. Just train for fewer epochs.
1176,@marktenenholtz,2022-08-07 22:00:11+00:00,https://twitter.com/marktenenholtz/status/1556399835477118976,@Rob_Mulla @Gillesvdwiele Readability is not bad to optimize on
1177,@marktenenholtz,2022-08-07 17:35:22+00:00,https://twitter.com/marktenenholtz/status/1556333192327700480,"@Gillesvdwiele @Rob_Mulla .loc is actually faster than .query(). I hate not getting code completion, so I always use .loc and never use .query() 😬"
1178,@marktenenholtz,2022-08-07 16:16:07+00:00,https://twitter.com/marktenenholtz/status/1556313248231727104,@JulienMouchnino Would you find it acceptable to use a different value of dropout for each of your 5 folds in 5-fold cross validation?
1179,@marktenenholtz,2022-08-07 16:14:33+00:00,https://twitter.com/marktenenholtz/status/1556312853795282944,@OxlerCeo Try both and see which works best
1180,@marktenenholtz,2022-08-07 16:14:06+00:00,https://twitter.com/marktenenholtz/status/1556312740242808834,@mrclbschff @Tugrul_Guner 💯
1181,@marktenenholtz,2022-08-07 16:13:52+00:00,https://twitter.com/marktenenholtz/status/1556312681908412417,@Tugrul_Guner That’s what early stopping is
1182,@marktenenholtz,2022-08-07 13:47:18+00:00,https://twitter.com/marktenenholtz/status/1556275795663921153,"@LeopolisDream Yep, augmentation, different learning rate scheduler, weight decay, etc. etc. It gets easier with practice."
1183,@marktenenholtz,2022-08-07 13:46:25+00:00,https://twitter.com/marktenenholtz/status/1556275571985993730,@svpino @Emxyz1 I gave it up entirely and haven't looked back :)
1184,@marktenenholtz,2022-08-07 13:04:11+00:00,https://twitter.com/marktenenholtz/status/1556264943409930241,@Emxyz1 That’s early stopping
1185,@marktenenholtz,2022-08-07 13:03:13+00:00,https://twitter.com/marktenenholtz/status/1556264700735881217,@svpino Yep. The correct answer is to modify your hyper parameters so that your last validation epoch is the best.
1186,@marktenenholtz,2022-08-07 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1556248806554906624,"One of the worst machine learning habits you can teach beginners:

Using early stopping by default.

It's one of the most common forms of overfitting that I see.

Ask yourself - would you modify any other hyperparameter between folds?"
1187,@marktenenholtz,2022-08-06 19:19:08+00:00,https://twitter.com/marktenenholtz/status/1555996915656957952,@tunguz Pseudo labeling
1188,@marktenenholtz,2022-08-06 14:14:06+00:00,https://twitter.com/marktenenholtz/status/1555920153380110337,@TheChosenMach @evoluminate Comes with the territory 🤷‍♂️
1189,@marktenenholtz,2022-08-06 13:37:33+00:00,https://twitter.com/marktenenholtz/status/1555910953639845888,"@hugo_le_moine_ Not only is the speed terrible when you try to reinvent the wheel, but you’re SO much more likely to introduce bugs"
1190,@marktenenholtz,2022-08-06 13:36:35+00:00,https://twitter.com/marktenenholtz/status/1555910712203026432,"@evoluminate “Popular ML frameworks have you running 95% of your code in a fast, compiled language.”"
1191,@marktenenholtz,2022-08-06 12:46:48+00:00,https://twitter.com/marktenenholtz/status/1555898181606580226,"@wewegomb Depends on the application, but you’re right for 95% of common cases"
1192,@marktenenholtz,2022-08-06 12:46:11+00:00,https://twitter.com/marktenenholtz/status/1555898027239522305,@LeopolisDream It definitely benefits a lot from scale
1193,@marktenenholtz,2022-08-06 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1555886422564818944,"There are some reasons you may want to switch, though.

I'll put a thread together about that for next week."
1194,@marktenenholtz,2022-08-06 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1555886419914067968,"Python is a slow language.

But not for ML.

Popular ML frameworks have you running 95% of your code in a fast, compiled language.

Combine that with Python's dev speed and you've got a potent tool.

Explore if you want. But if you like Python, don't feel compelled to switch."
1195,@marktenenholtz,2022-08-05 21:13:28+00:00,https://twitter.com/marktenenholtz/status/1555663301605998595,@nischay_twt @hey_kishore @tunguz Looks like I need to tweet more
1196,@marktenenholtz,2022-08-05 20:07:38+00:00,https://twitter.com/marktenenholtz/status/1555646732783976449,"@ShaanVP Most of the leaders in AI don't believe this will happen any time soon. There's nothing built into these models that allows them to have any coherence with the real world, which is a major roadblock.

There's definitely wide application for them in the meantime, though."
1197,@marktenenholtz,2022-08-05 16:24:28+00:00,https://twitter.com/marktenenholtz/status/1555590573838532613,@tunguz It’s crazy how much fun it is to train models. The fact that I’m addicted to Kaggle is a true marvel
1198,@marktenenholtz,2022-08-05 13:47:02+00:00,https://twitter.com/marktenenholtz/status/1555550952161218560,@rodbarest That’s why you ask strong coders to build strong internal tools and strong modelers to make strong models
1199,@marktenenholtz,2022-08-05 13:46:14+00:00,https://twitter.com/marktenenholtz/status/1555550752604577792,@alt227Joydeep Yeah but I wouldn’t call that a data science function
1200,@marktenenholtz,2022-08-05 13:35:57+00:00,https://twitter.com/marktenenholtz/status/1555548163381018625,@GabeSchenz Devops is also very important
1201,@marktenenholtz,2022-08-05 12:12:39+00:00,https://twitter.com/marktenenholtz/status/1555527201403052035,"@NirantK This is why you ask them to build tooling and not data engineering pipelines.

That’s why you have data engineers."
1202,@marktenenholtz,2022-08-05 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1555524037144563712,"Want to 10x your data science function?

Focus less on:

• Hiring more data scientists
• Hiring more data engineers

And instead?

Hire ML-savvy software engineers.

Data teams that work and communicate with well-structured, easy-to-use APIs are easily 10x better."
1203,@marktenenholtz,2022-08-04 16:35:32+00:00,https://twitter.com/marktenenholtz/status/1555230967639457793,@haltakov It's like the opposite of cloud storage 😂
1204,@marktenenholtz,2022-08-04 14:37:45+00:00,https://twitter.com/marktenenholtz/status/1555201328305115137,@carsondahlberg Will do!
1205,@marktenenholtz,2022-08-04 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1555161657378099200,"I'll be tweeting my thoughts once I get into the book, so keep an eye out!"
1206,@marktenenholtz,2022-08-04 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1555161654454673409,"Thrilled to receive a copy of ""GPT-3: Building Innovative NLP Products Using Large Language Models.""

I'm extremely passionate about building great data products, and this book is a strong step towards getting us there with GPT-3!

Thanks to @Saboo_Shubham_ for hooking me up! https://t.co/5scQmMbWEB"
1207,@marktenenholtz,2022-08-03 19:01:44+00:00,https://twitter.com/marktenenholtz/status/1554905372695085057,@azuremis https://t.co/YoQWOfwXEw
1208,@marktenenholtz,2022-08-03 18:24:15+00:00,https://twitter.com/marktenenholtz/status/1554895940128976897,"The best researchers embrace real-world problems.

The best applied ML practitioners embrace research."
1209,@marktenenholtz,2022-08-03 15:21:41+00:00,https://twitter.com/marktenenholtz/status/1554849998289920003,"Sometimes it’s easy to forget just how empirical of a process ML is.

Model evaluation is a big reminder."
1210,@marktenenholtz,2022-08-03 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1554799265431425024,"Model evaluation should not be taught:

• On a whiteboard...
• in a classroom...
• without training a model

It should be taught:

• Hands-on...
• ...seeing how the model works in prod
• ...while learning data eng. skills

Learning about what goes in and out is the key."
1211,@marktenenholtz,2022-08-02 14:17:08+00:00,https://twitter.com/marktenenholtz/status/1554471363796209666,@gib_hurst They probably won't :)
1212,@marktenenholtz,2022-08-02 13:29:12+00:00,https://twitter.com/marktenenholtz/status/1554459301384785920,"@GaelVaroquaux Thank you for sharing that!

I’ll add it to my list alongside this one: https://t.co/8EE4mLmaGH"
1213,@marktenenholtz,2022-08-02 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1554436872524292096,"3 machine learning topics I wish were taught better:

1. Model evaluation
2. Avoiding target leakage
3. Preparing models for production

...because they teach you to:

1. Measure your model's value
2. Validate those measures
3. Actually extract value from the model"
1214,@marktenenholtz,2022-08-02 00:32:49+00:00,https://twitter.com/marktenenholtz/status/1554263918553464834,@charitymansson ♥️
1215,@marktenenholtz,2022-08-02 00:10:35+00:00,https://twitter.com/marktenenholtz/status/1554258323624660998,@jappleby Replies to my tweets are 90% positive and QTs of my tweets are 90% toxic
1216,@marktenenholtz,2022-08-01 17:41:43+00:00,https://twitter.com/marktenenholtz/status/1554160462677348356,"@tunguz I've got a 🔥 model evaluation module coming eventually that I'll probably release in the first installment of my ML masterclass series.

Basically details everything I've learned from Kaggle on model eval and examples of how to apply the techniques to much hairier problems."
1217,@marktenenholtz,2022-08-01 14:04:09+00:00,https://twitter.com/marktenenholtz/status/1554105708911726592,"@tunguz I use @RAPIDSai in a lot of my projects now. Like I've said before, cuML literally brought SVMs back from the dead."
1218,@marktenenholtz,2022-08-01 13:16:56+00:00,https://twitter.com/marktenenholtz/status/1554093826301743104,"@EMCP_ We don’t think in overly structured formats, so why would AGI? Great point"
1219,@marktenenholtz,2022-08-01 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1554074489738080257,"Good data teams force rigorous structure onto their data.

Great data teams embrace unstructured data."
1220,@marktenenholtz,2022-07-31 23:02:41+00:00,https://twitter.com/marktenenholtz/status/1553878849208913920,@carlolepelaars @kaggle Amen to that
1221,@marktenenholtz,2022-07-31 21:23:19+00:00,https://twitter.com/marktenenholtz/status/1553853840792948736,"@_brohrer_ There’s a strong positive correlation between the amount my content breaks down gatekeeping and the ire I get from academic/purist types.

It’s sad — academics and applied ML practitioners have so much to learn from each other."
1222,@marktenenholtz,2022-07-31 15:14:09+00:00,https://twitter.com/marktenenholtz/status/1553760938507083777,@simonw I've come to appreciate Spark mostly because of how much I appreciate SQL
1223,@marktenenholtz,2022-07-31 13:48:36+00:00,https://twitter.com/marktenenholtz/status/1553739407953371137,@bertil_hatt Stakeholder management is a perpetually underrated skill
1224,@marktenenholtz,2022-07-31 13:05:18+00:00,https://twitter.com/marktenenholtz/status/1553728510195277829,@alanbuxton Absolute life savor for me. And I check many more than 100!
1225,@marktenenholtz,2022-07-31 12:26:43+00:00,https://twitter.com/marktenenholtz/status/1553718801849622529,@HaffendenHector That one got me too
1226,@marktenenholtz,2022-07-31 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1553712089096540160,What's one tip that seriously boosted your data science skills?
1227,@marktenenholtz,2022-07-31 00:57:24+00:00,https://twitter.com/marktenenholtz/status/1553545327805964290,@gusthema We’ll see :)
1228,@marktenenholtz,2022-07-30 16:19:26+00:00,https://twitter.com/marktenenholtz/status/1553414976441913349,@Rob_Mulla My secret: my face is an adversarial attack
1229,@marktenenholtz,2022-07-30 16:09:02+00:00,https://twitter.com/marktenenholtz/status/1553412359405838336,@Rob_Mulla How'd you get that picture of me?
1230,@marktenenholtz,2022-07-30 16:02:06+00:00,https://twitter.com/marktenenholtz/status/1553410616739696646,"@__mharrison__ You test by running it until it runs to completion, right?"
1231,@marktenenholtz,2022-07-30 15:13:58+00:00,https://twitter.com/marktenenholtz/status/1553398501115133952,@hugo_le_moine_ I do the same as you but it’s good to focus on specific parts of the process sometimes
1232,@marktenenholtz,2022-07-30 14:37:31+00:00,https://twitter.com/marktenenholtz/status/1553389330999164928,@MichaelInYEG @tunguz The most active work is exploration. Validation is more up front work to get it right and training is more passive work with babysitting.
1233,@marktenenholtz,2022-07-30 14:35:41+00:00,https://twitter.com/marktenenholtz/status/1553388868250091520,"@gusthema My boy gets a bit too curious, but if you weren’t careful his brother would run off with a chicken in his teeth"
1234,@marktenenholtz,2022-07-30 14:32:48+00:00,https://twitter.com/marktenenholtz/status/1553388141087756288,@tunguz Just give me SSH access and you don’t even have to make me dinner
1235,@marktenenholtz,2022-07-30 14:31:57+00:00,https://twitter.com/marktenenholtz/status/1553387929183162369,"@MichaelInYEG @tunguz I think that’s the wrong way to think about it. It’s more about repeating the cycle many times, productively."
1236,@marktenenholtz,2022-07-30 14:06:57+00:00,https://twitter.com/marktenenholtz/status/1553381638196920321,"@tunguz Explore, train, validate, repeat"
1237,@marktenenholtz,2022-07-30 13:11:32+00:00,https://twitter.com/marktenenholtz/status/1553367691314008064,"@LeopolisDream You don’t have to. Starting with XGB is perfectly fine, but I usually only recommend people do it when they are actually pretty decent at applying XGB"
1238,@marktenenholtz,2022-07-30 13:10:58+00:00,https://twitter.com/marktenenholtz/status/1553367547994587138,@hugo_le_moine_ Focusing on the model here
1239,@marktenenholtz,2022-07-30 12:11:46+00:00,https://twitter.com/marktenenholtz/status/1553352652012093440,@ashutoshbh93 Focusing on the model here
1240,@marktenenholtz,2022-07-30 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1553349701931454465,"Highly accurate tabular ML modeling in 30 seconds:

1. Start with a random forest
2. Upgrade to XGBoost/LightGBM
3. Tune it

Optional:
4. Create a deep learning model
5. Tune it

6. Pick one or the other or ensemble them

A well-built NN might beat XGBoost, but it's more work."
1241,@marktenenholtz,2022-07-30 01:52:31+00:00,https://twitter.com/marktenenholtz/status/1553196813150199809,@Even_Oldridge NVIDIA knows where the 💰 is!
1242,@marktenenholtz,2022-07-29 17:40:20+00:00,https://twitter.com/marktenenholtz/status/1553072950986870785,"@KyeGomezB If noobs generate trillions of $$$ for the economy then count me aboard the noob train, bucko"
1243,@marktenenholtz,2022-07-29 14:19:31+00:00,https://twitter.com/marktenenholtz/status/1553022411469213697,@svpino Deterministic systems (typical SaaS) =/= Non-deterministic systems (ML products)
1244,@marktenenholtz,2022-07-29 13:58:57+00:00,https://twitter.com/marktenenholtz/status/1553017237761564672,@dylanbert Go do old Kaggle competitions. Companies often put problems up there because they’re problems they face
1245,@marktenenholtz,2022-07-29 13:58:04+00:00,https://twitter.com/marktenenholtz/status/1553017013911670785,"@JulienMouchnino Sales forecasting, churn prediction, targeted marketing, labor forecasting, delinquency prediction, etc"
1246,@marktenenholtz,2022-07-29 13:55:57+00:00,https://twitter.com/marktenenholtz/status/1553016483407855617,@rodbarest Feature engineering is crazy tough to get right
1247,@marktenenholtz,2022-07-29 13:55:27+00:00,https://twitter.com/marktenenholtz/status/1553016356945231874,@PathikGhugare Just go filter completed competitions by data type = tabular data
1248,@marktenenholtz,2022-07-29 12:13:57+00:00,https://twitter.com/marktenenholtz/status/1552990810735517697,@EyquemBI @tunguz Oh I would say he has even stronger views on this than I do :)
1249,@marktenenholtz,2022-07-29 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1552987350304575488,"Tabular ML is the most ripe opportunity at 90%+ of companies.

And yet:

• Most companies don’t use XGBoost
• Research is focused on NLP/CV
• Many data scientists think it’s boring

My advice: zig when everyone else is zagging."
1250,@marktenenholtz,2022-07-28 15:03:08+00:00,https://twitter.com/marktenenholtz/status/1552671000998952962,"@alexdconf I've heard standardized and normalized before. Sometimes it helps, sometimes it hurts 🤷‍♂️"
1251,@marktenenholtz,2022-07-28 15:02:15+00:00,https://twitter.com/marktenenholtz/status/1552670778428211200,"@PawarBI @smasis And sometimes, the cost of a big miss is WAY bigger than the difference between a couple percentage points of better accuracy"
1252,@marktenenholtz,2022-07-28 14:00:37+00:00,https://twitter.com/marktenenholtz/status/1552655266298023942,@LukasPlatinsky A similar adage goes: “give 100 data scientists the same dataset and you’ll get 100 different answers”
1253,@marktenenholtz,2022-07-28 13:59:50+00:00,https://twitter.com/marktenenholtz/status/1552655069371174916,@curiovana Yeah the field seems very academic on the outside and very empirical on the inside
1254,@marktenenholtz,2022-07-28 12:34:58+00:00,https://twitter.com/marktenenholtz/status/1552633715435753474,"@Ak_Python You don’t always need the math to get started, but you definitely need it at some point"
1255,@marktenenholtz,2022-07-28 12:34:07+00:00,https://twitter.com/marktenenholtz/status/1552633500842577923,@RealAmeerHameed That’s one of the more pernicious ones. Stops so many from entering the field
1256,@marktenenholtz,2022-07-28 12:33:30+00:00,https://twitter.com/marktenenholtz/status/1552633344545984513,@dresq28 Welcome to the world of buzzwords!
1257,@marktenenholtz,2022-07-28 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1552624928859439104,For me: I thought training models would be much harder than deploying them.
1258,@marktenenholtz,2022-07-28 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1552624926397370368,What’s the biggest misconception you had about ML before you got serious about learning?
1259,@marktenenholtz,2022-07-27 19:26:35+00:00,https://twitter.com/marktenenholtz/status/1552374911062212608,"@chrisalbon I felt like I was being gaslit in high school when my teachers told me that Wikipedia was a garbage source…

BUT I could use the citations from Wikipedia to find GOOD sources."
1260,@marktenenholtz,2022-07-27 17:20:17+00:00,https://twitter.com/marktenenholtz/status/1552343127607988229,@lndian_Bronson Much appreciated. Mine look like I’m comparing BASIC to Fortran.
1261,@marktenenholtz,2022-07-27 16:48:42+00:00,https://twitter.com/marktenenholtz/status/1552335180806905864,@MrRobertClayton Talk about empowering
1262,@marktenenholtz,2022-07-27 14:26:31+00:00,https://twitter.com/marktenenholtz/status/1552299397890936832,@Taylor_Street_ Data science is more democratized than ever!
1263,@marktenenholtz,2022-07-27 13:27:44+00:00,https://twitter.com/marktenenholtz/status/1552284604371189761,"@alepiad “No need to toot your own horn”

Humility is vital, but if you don’t fight for yourself and the people you manage, you’ll get rolled over at all but the best companies."
1264,@marktenenholtz,2022-07-27 12:28:13+00:00,https://twitter.com/marktenenholtz/status/1552269626268065793,@pddykn @typefully Sorry about that
1265,@marktenenholtz,2022-07-27 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1552262540574347266,"Stop for a moment and appreciate:

You’re able to harness billions of rows of data into something meaningful.

That’s truly incredible."
1266,@marktenenholtz,2022-07-26 17:00:14+00:00,https://twitter.com/marktenenholtz/status/1551975696276361216,@GTK_ARJUN_AGS It serves an entirely different purpose. It would be silly to port existing enormous C libraries if there was a far easier way of maintaining and improving them.
1267,@marktenenholtz,2022-07-26 14:50:32+00:00,https://twitter.com/marktenenholtz/status/1551943055435079680,"@gusthema Oh my apologies, I read your tweet wrong.

My point was that Rust built a whole “batteries-included” ecosystem around their language (package manager, super helpful compiler, etc) and it has rapidly accelerated adoption."
1268,@marktenenholtz,2022-07-26 14:43:43+00:00,https://twitter.com/marktenenholtz/status/1551941340056911874,@gusthema Agreed. Rust is the poster child.
1269,@marktenenholtz,2022-07-26 14:43:16+00:00,https://twitter.com/marktenenholtz/status/1551941223836950528,@IBlackmailLands Python that just-in-time complies like Julia does
1270,@marktenenholtz,2022-07-26 14:28:42+00:00,https://twitter.com/marktenenholtz/status/1551937561635045377,@ovjocm https://t.co/vekQwYZnqK
1271,@marktenenholtz,2022-07-26 14:28:28+00:00,https://twitter.com/marktenenholtz/status/1551937499706150914,@dav_ell https://t.co/vekQwYZnqK
1272,@marktenenholtz,2022-07-26 13:35:59+00:00,https://twitter.com/marktenenholtz/status/1551924292056748041,@MeetRandomQuant 8th tweet
1273,@marktenenholtz,2022-07-26 12:31:12+00:00,https://twitter.com/marktenenholtz/status/1551907990512582657,@leassis91 Yeah not sure why that happened. Check them out here https://t.co/5ouQe6atIP
1274,@marktenenholtz,2022-07-26 12:24:22+00:00,https://twitter.com/marktenenholtz/status/1551906270042537985,@LukasValatka 8th tweet
1275,@marktenenholtz,2022-07-26 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1551900207415078912,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
1276,@marktenenholtz,2022-07-26 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1551900204911058944,"TL;DR:

1. Python is misunderstood
2. But, ML code is still totally reliant on C
3. There's an ocean of C code holding ML up
4. Carbon could make it much more maintainable

Follow me @marktenenholtz for more high-signal ML content!"
1277,@marktenenholtz,2022-07-26 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1551900202360905728,"Here's what I feel pretty confident in saying:

User-friendly, batteries-included, low-level languages are going to be immensely impactful in ML in the coming years, and you should keep your eyes out."
1278,@marktenenholtz,2022-07-26 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1551900199857008641,"I don't know if Carbon is going to be ditched in a year or if it's going to revolutionize the way C codebases are maintained.

Honestly, the point of this thread is less about Carbon and more about the impact of modern languages."
1279,@marktenenholtz,2022-07-26 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1551900197327761413,"New libraries that are backed by fast, low-level implementations let us Python users try out entirely new methods.

Wouldn't it be great if library developers could extend the ocean of ML-focused C to create new libraries for Python users WITHOUT needing to use C?"
1280,@marktenenholtz,2022-07-26 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1551900194748321792,"This was definitely the case with deep learning (when coupled with hardware improvements, at least).

I mean, shoot, I even saw AutoARIMA get some big performance boosts recently.

How much could we improve GBDT models if we didn't have to write them from scratch in C?"
1281,@marktenenholtz,2022-07-26 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1551900189761290244,(Side note: will CUDA C integrate at all with Carbon? That'd be pretty cool.)
1282,@marktenenholtz,2022-07-26 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1551900187307544577,"Given that mission, you have 2 options:

1. Try to migrate everything off of C
2. Extend existing C code with something better

1 is a valiant mission, but 2 is more pragmatic in the short term."
1283,@marktenenholtz,2022-07-26 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1551900184828715008,"As far as reducing tech debt goes, I really appreciate the pragmatic mission Google has here.

It's basically this:

• There's a lotta C code out there
• That code is great
• But it's full of tech debt
• Tech debt is inherent to C
• More code in C leads to more tech debt"
1284,@marktenenholtz,2022-07-26 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1551900182224052225,"Going by GitHub stats, here's the % of code in C/C++ for some of your favorite libraries:

• NumPy: 36.4%
• PyTorch: 54.0%
• TensorFlow: 62.7%
• SciPy: 20.4% (19.1% Fortran)

I included the Fortran % for SciPy just to get across how reliant we are on fast library code."
1285,@marktenenholtz,2022-07-26 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1551900179745296384,"Languages like Rust and Go are fantastic for performance, and we should continue to use them for all kinds of applications.

But it's undeniable that we have a vested interest in maintaining the enormous C codebases that make up our favorite ML libraries."
1286,@marktenenholtz,2022-07-26 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1551900177224544256,"Of course, it's only true if we can keep getting developers to write kick-ass C libraries to do all the heavy lifting.

Which, of course, brings me to Carbon."
1287,@marktenenholtz,2022-07-26 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1551900174645047297,"But as it stands now:

When you run Python ML code, you're probably looking at maybe a 10% performance overhead?

This number is heavily fudged, but I think it's pretty conservative."
1288,@marktenenholtz,2022-07-26 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1551900172187099137,"We can have conversations about whether Python is the future of ML until the cows come home.

Personally, I think it probably isn't (unless a fully JIT'd Python clone shows up, which, well... I'm not exactly counting on)."
1289,@marktenenholtz,2022-07-26 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1551900169687298050,"There is a *massive* misconception that Python code is ungodly slow for ML tasks.

It would actually be true... if nearly all of your code wasn't already running in heavily optimized C libraries."
1290,@marktenenholtz,2022-07-26 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1551900167036514305,"Really, there are two main reasons:

1. Reducing tech debt in existing libraries
2. Making it easier to extend those libraries

But first, let's talk about the marriage between Python, C, and machine learning."
1291,@marktenenholtz,2022-07-26 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1551900164519931906,"(One quick caveat: Carbon is experimental and, well, we all know Google's track record)"
1292,@marktenenholtz,2022-07-26 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1551900161386762240,"Google has launched Carbon: a successor to C++.

It matches 100% of the performance of C++ and aims to provide a significantly better developer experience.

I think this is huge news for ML.

Here's why you should care (even if you've never written a line of C/C++ in your life): https://t.co/BquWkytE9x"
1293,@marktenenholtz,2022-07-26 01:11:45+00:00,https://twitter.com/marktenenholtz/status/1551736998809018370,"@datafox21 @fastdotai I don’t use the library but I think it’s built perfectly for a course because of its progressive disclosure of complexity. 

The course is very PyTorch-centric so switching to pure PyTorch is no issue."
1294,@marktenenholtz,2022-07-25 20:23:19+00:00,https://twitter.com/marktenenholtz/status/1551664411982569474,@dickiebush How many packs do I have to open before I get a Dickie Bush rookie card?
1295,@marktenenholtz,2022-07-25 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1551537777933570048,Check it out here: https://t.co/vVl5xmNC4a
1296,@marktenenholtz,2022-07-25 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1551537774464942080,"The first ML course that truly stuck in my head was the @fastdotai course Practical Deep Learning for Coders.

It was the first ML course I found that was truly built for builders.

That was V3 of the course.

And fortunately for you (and me), V5 just dropped last week! https://t.co/5GVavGGK93"
1297,@marktenenholtz,2022-07-24 19:18:27+00:00,https://twitter.com/marktenenholtz/status/1551285703522271232,@jim_dowling @AhnafRyan4 @alighodsi Not my finest moment 😂
1298,@marktenenholtz,2022-07-24 19:18:03+00:00,https://twitter.com/marktenenholtz/status/1551285600363356162,@EhsanSafavieh @AhnafRyan4 @alighodsi 😂
1299,@marktenenholtz,2022-07-24 18:52:14+00:00,https://twitter.com/marktenenholtz/status/1551279105353695243,One that I just came across today:
1300,@marktenenholtz,2022-07-24 18:42:16+00:00,https://twitter.com/marktenenholtz/status/1551276598011351041,@JFPuget @hardmaru RAPIDS literally brought SVMs back from the dead lol
1301,@marktenenholtz,2022-07-24 18:38:00+00:00,https://twitter.com/marktenenholtz/status/1551275524265312259,Books count too!
1302,@marktenenholtz,2022-07-24 18:24:25+00:00,https://twitter.com/marktenenholtz/status/1551272104963837953,@alepiad Now THAT is a bold decision
1303,@marktenenholtz,2022-07-24 18:22:32+00:00,https://twitter.com/marktenenholtz/status/1551271629430423552,@simonw @__mharrison__ I would think DBT is probably the best option
1304,@marktenenholtz,2022-07-24 18:17:14+00:00,https://twitter.com/marktenenholtz/status/1551270296182734861,@AhnafRyan4 @alighodsi is a legend!
1305,@marktenenholtz,2022-07-24 18:15:39+00:00,https://twitter.com/marktenenholtz/status/1551269899766579204,"@CurtTigges 1. The outlook is fine. Data scientists/engineers will continue to be in high demand.

2. If a company doesn’t sound like it has a highly valuable, profitable product, don’t join."
1306,@marktenenholtz,2022-07-24 18:13:37+00:00,https://twitter.com/marktenenholtz/status/1551269385699999744,@LeopolisDream This is a great answer haha
1307,@marktenenholtz,2022-07-24 15:51:28+00:00,https://twitter.com/marktenenholtz/status/1551233611277357058,What’s your favorite non-beginner ML course?
1308,@marktenenholtz,2022-07-24 12:45:31+00:00,https://twitter.com/marktenenholtz/status/1551186818594443270,"@tunguz The job interview:

Write a Python function that calculates entropy.

The job:

Set the random forest criterion=“entropy” and call it a day"
1309,@marktenenholtz,2022-07-24 12:29:58+00:00,https://twitter.com/marktenenholtz/status/1551182905187225601,Chaff* 😂
1310,@marktenenholtz,2022-07-24 12:29:51+00:00,https://twitter.com/marktenenholtz/status/1551182874556112896,"@Jon_Cusack Ha, autocorrect got me. Good call"
1311,@marktenenholtz,2022-07-24 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1551175374104510465,"It seems like every time I open tech news:

• Layoffs
• Hiring freezes
• Offers rescinded

The wheat is about to be separated from the shaft in ML.

Productive, value-driven teams will have the pick of the litter and will continue to thrive."
1312,@marktenenholtz,2022-07-23 12:54:40+00:00,https://twitter.com/marktenenholtz/status/1550826731870863361,@TheZachMueller @elmidwill @jeremyphoward Amen to that
1313,@marktenenholtz,2022-07-23 12:40:46+00:00,https://twitter.com/marktenenholtz/status/1550823234936999936,"And if you don’t believe me, here’s a resource that teaches you just about everything you need:"
1314,@marktenenholtz,2022-07-23 12:27:30+00:00,https://twitter.com/marktenenholtz/status/1550819895306248192,@tugot17 Little bit of derivatives and a little bit of linear algebra thrown in together
1315,@marktenenholtz,2022-07-23 12:26:54+00:00,https://twitter.com/marktenenholtz/status/1550819743279550467,"@Observing2022 Exactly, nothing too spicy"
1316,@marktenenholtz,2022-07-23 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1550812998230351872,"If you think you can’t learn deep learning because you’ll never get the math, you’re probably flat-out wrong."
1317,@marktenenholtz,2022-07-22 20:37:36+00:00,https://twitter.com/marktenenholtz/status/1550580845769023488,"@jeremyphoward Funny you should say that — literally 2 hours ago I was writing something about the NLP ImageNet moment, saw a comment from you on HackerNews from 2018, and someone had replied with something nasty"
1318,@marktenenholtz,2022-07-22 19:13:53+00:00,https://twitter.com/marktenenholtz/status/1550559776106598407,@JesperDramsch Gotta love a crisp 💰
1319,@marktenenholtz,2022-07-22 16:01:24+00:00,https://twitter.com/marktenenholtz/status/1550511337662554114,@tunguz You’ve either outed yourself as a boomer or a European or both
1320,@marktenenholtz,2022-07-22 15:48:24+00:00,https://twitter.com/marktenenholtz/status/1550508066134704128,@tunguz We stan Optuna
1321,@marktenenholtz,2022-07-22 14:25:45+00:00,https://twitter.com/marktenenholtz/status/1550487266509950977,@xtinacomputes My gf: https://t.co/Ple8mst8zJ
1322,@marktenenholtz,2022-07-22 14:09:06+00:00,https://twitter.com/marktenenholtz/status/1550483075615842304,@Taylor_Street_ That could easily be 1-2 weeks depending on the problem
1323,@marktenenholtz,2022-07-22 14:08:24+00:00,https://twitter.com/marktenenholtz/status/1550482900881121281,@ArnaudMarechal Just the other day I realized I was softmax’ing my output twice 🤦‍♂️
1324,@marktenenholtz,2022-07-22 12:22:44+00:00,https://twitter.com/marktenenholtz/status/1550456309308264448,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
1325,@marktenenholtz,2022-07-22 12:22:44+00:00,https://twitter.com/marktenenholtz/status/1550456306741305345,"TL;DR:

1. Become one with the data
2. Follow the data
3. Rapid iteration
4. Evaluate, then train

Follow me  @marktenenholtz  for more high-signal ML content!"
1326,@marktenenholtz,2022-07-22 12:22:43+00:00,https://twitter.com/marktenenholtz/status/1550456304170242048,"Evaluation ALWAYS precedes training

If you've trained a few models before realizing that your cross-validation scheme was poor... well, those experiments are basically garbage.

Starting with a poorly-conceived CV setup is like lighting money on fire."
1327,@marktenenholtz,2022-07-22 12:22:43+00:00,https://twitter.com/marktenenholtz/status/1550456301662048256,"Even better: don't feel obligated to let a model/CV run finish if it looks like it's a dud of an attempt.

Just kill it early and move on with your life."
1328,@marktenenholtz,2022-07-22 12:22:42+00:00,https://twitter.com/marktenenholtz/status/1550456299103522817,"Rapid iteration

There's nothing worse than wasting 2 hours waiting on a model to train only to realize it ran with the wrong params/your feature engineering tactic didn't work/etc.

Test your pipeline early on with small models or even on a subset of your data."
1329,@marktenenholtz,2022-07-22 12:22:41+00:00,https://twitter.com/marktenenholtz/status/1550456296473780225,"Visualize it:

1. In raw form
2. After any preprocessing steps
3. After augmentation
4. Right before it goes into the model
5. The output the model produces

Catch any pipeline bugs before they waste any of your precious time."
1330,@marktenenholtz,2022-07-22 12:22:41+00:00,https://twitter.com/marktenenholtz/status/1550456293843841024,"Follow the data

No doubt you know you should be visualizing your data at the beginning, maybe analyzing your outputs, etc.

But why aren't you visualizing the data at every step of your pipeline?

ML models have a remarkably annoying way of running even on corrupted data."
1331,@marktenenholtz,2022-07-22 12:22:40+00:00,https://twitter.com/marktenenholtz/status/1550456290693943297,"Become one with the data

You could probably do 80% less debugging if you spent more time exploring the data.

Weird distributions, outliers, corrupted data, etc. etc.

You'll go full Charlie Day before realizing that there's just an edge case you missed. https://t.co/VSxI6Ar8Gq"
1332,@marktenenholtz,2022-07-22 12:22:38+00:00,https://twitter.com/marktenenholtz/status/1550456281302847490,"Remember that applied ML is a relatively new field, even by tech standards.

We don't have 100's of years of battle-hardened strategies to fall back on.

But, best practices are evolving and these are some of the best that would have helped me immensely."
1333,@marktenenholtz,2022-07-22 12:22:37+00:00,https://twitter.com/marktenenholtz/status/1550456278685650944,"I've spent 1000's of hours building ML models over the last couple years.

The inconvenient truth: a few more good habits could have saved me 100's of hours.

To save you the wasted time, here are 4 of those habits you can start implementing today:"
1334,@marktenenholtz,2022-07-22 00:01:14+00:00,https://twitter.com/marktenenholtz/status/1550269701158232065,"@jeremyphoward @PyTorch @huggingface @Gradio Congrats, Jeremy! Every iteration of this course is a huge benefit for all ML practitioners."
1335,@marktenenholtz,2022-07-21 14:37:29+00:00,https://twitter.com/marktenenholtz/status/1550127832562409479,@gusthema @MeganRisdal Meg knows how I feel 😜
1336,@marktenenholtz,2022-07-21 14:27:36+00:00,https://twitter.com/marktenenholtz/status/1550125341506539526,@machsci Yes! And cloud literacy is becoming more and more important for data scientists
1337,@marktenenholtz,2022-07-21 13:43:32+00:00,https://twitter.com/marktenenholtz/status/1550114253587243008,@shbfy Go to the library lol
1338,@marktenenholtz,2022-07-21 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1550088272092618753,"Question I get all the time:

""How much do I need to spend to start in ML?""

Answer: $0.00

You can:

• Use Kaggle to scrape your data
• Use Kaggle to clean your data
• Use Kaggle to train and run your model

Kaggle isn't just for competitions.

Take advantage of it."
1339,@marktenenholtz,2022-07-20 20:55:05+00:00,https://twitter.com/marktenenholtz/status/1549860471053979648,"@jim_dowling @l2k Used to use it at my old job and the limitations are absolutely staggering, even for corporate licenses that cost a fortune. Incredibly frustrating to work with sometimes"
1340,@marktenenholtz,2022-07-20 20:06:12+00:00,https://twitter.com/marktenenholtz/status/1549848167532855297,@l2k DataRobot is a leader while Databricks is barely a strong performer? That's a good joke.
1341,@marktenenholtz,2022-07-20 15:19:04+00:00,https://twitter.com/marktenenholtz/status/1549775906285174785,@nischay_twt @h2oai @vopani @bhutanisanyam1 Congrats Nischay!
1342,@marktenenholtz,2022-07-20 12:54:14+00:00,https://twitter.com/marktenenholtz/status/1549739460614672386,@RHortelanoS You’re just hamstringing yourself if you don’t!
1343,@marktenenholtz,2022-07-20 12:24:34+00:00,https://twitter.com/marktenenholtz/status/1549731993537200131,"@svpino @Cometml I prefer @weights_biases but yeah, that works too :)"
1344,@marktenenholtz,2022-07-20 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1549725826806992896,"ML has been called ""the high-interest credit card of technical debt.""

It's hard to minimize your tech debt, but one easy way:

Document everything.

Try documenting:

• ETL steps
• Future risks
• Assumptions
• Experiment logs
• Ideas for improvement

Thank yourself later."
1345,@marktenenholtz,2022-07-19 19:15:55+00:00,https://twitter.com/marktenenholtz/status/1549473126190284800,@svpino Hypeeee!
1346,@marktenenholtz,2022-07-19 18:59:17+00:00,https://twitter.com/marktenenholtz/status/1549468938752368640,@slippylolo @arankomatsuzaki Had a hearty laugh at that. Can’t wait to check out your work
1347,@marktenenholtz,2022-07-19 17:51:33+00:00,https://twitter.com/marktenenholtz/status/1549451893688209410,@tunguz I’ve been foiled again
1348,@marktenenholtz,2022-07-19 17:46:05+00:00,https://twitter.com/marktenenholtz/status/1549450517503426560,@tunguz Could this call for attempt #2 at a Twitter Space?
1349,@marktenenholtz,2022-07-19 17:43:31+00:00,https://twitter.com/marktenenholtz/status/1549449870167121921,"@GaelVaroquaux Wait a minute, some seemingly high-quality research on tabular data? 

And it actually seeks to explain the differences between NNs and GBDTs without just declaring superiority?

I love it. Thanks for the great work!"
1350,@marktenenholtz,2022-07-19 15:29:08+00:00,https://twitter.com/marktenenholtz/status/1549416054824239105,"@Grimezsz Also for both, good model evaluation is the difference between millions of $$$ of revenue and a lawsuit"
1351,@marktenenholtz,2022-07-19 14:12:10+00:00,https://twitter.com/marktenenholtz/status/1549396682391027712,@machsci We have 175B parameter language models and this is a question that was published in a paper only 6 months ago https://t.co/7Ct7iLgyR7
1352,@marktenenholtz,2022-07-19 14:10:00+00:00,https://twitter.com/marktenenholtz/status/1549396136577884160,"@jim_dowling I feel your pain, but remember that published results aren't everything. 

I'd say a large portion of published results aren't repeatable on any other task than the specific datasets they were benchmarked against. High-quality data products are much more resilient than that."
1353,@marktenenholtz,2022-07-19 12:14:31+00:00,https://twitter.com/marktenenholtz/status/1549367074484637697,"@svpino 100%. At the end of the day, we all want our research to accomplish something useful, right?"
1354,@marktenenholtz,2022-07-19 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1549363435330146306,"Data products are joyous when they are reliable, intuitive, and actionable.

Not just when they're 1% more accurate than the competition."
1355,@marktenenholtz,2022-07-18 18:38:41+00:00,https://twitter.com/marktenenholtz/status/1549101369197477890,"@thejustinwelsh And if you can’t write a concise-enough SOP for step 4, refer back to step 2!"
1356,@marktenenholtz,2022-07-18 18:20:40+00:00,https://twitter.com/marktenenholtz/status/1549096832944558080,@xtinacomputes for the sake of fucks in the set of all possible fucks
1357,@marktenenholtz,2022-07-18 18:18:36+00:00,https://twitter.com/marktenenholtz/status/1549096311894466560,"@Revolut_io Common tasks are a bit more streamlined in those languages. IMO it's a bit of a trap because the convenience of simple tasks draws you in to an ecosystem that actually lacks the depth of something like Python.

I can understand the argument for using R, but not Stata/SPSS/SAS"
1358,@marktenenholtz,2022-07-18 14:21:31+00:00,https://twitter.com/marktenenholtz/status/1549036646976278528,"@aroravanarp If you have any interest in working with data, I'd totally agree with you"
1359,@marktenenholtz,2022-07-18 14:20:44+00:00,https://twitter.com/marktenenholtz/status/1549036451781677056,@anishpdalal It's crazy how one library can transform an entire programming language
1360,@marktenenholtz,2022-07-18 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1549001057325305856,"It’s absolutely insane to think about the impact that:

• NumPy
• Pandas
• Scikit-learn
• Matplotlib

have had on the world.

Billions of $$$ in value created?"
1361,@marktenenholtz,2022-07-17 15:23:06+00:00,https://twitter.com/marktenenholtz/status/1548689759215779840,"@ph_singer @rasbt Admittedly I’ve never used H2O, but I’ve used DataRobot extensively and a couple other tools and trusting their default CV would have been a bad idea as a default"
1362,@marktenenholtz,2022-07-17 14:03:52+00:00,https://twitter.com/marktenenholtz/status/1548669820354306049,"@rasbt This is one of the main reason I love Kaggle + why I’m bearish on AutoML tools extending their user base beyond data scientists. 

I hate gatekeeping but it’s so easy to break the integrity of your evaluation if you don’t know what you’re doing."
1363,@marktenenholtz,2022-07-17 13:53:04+00:00,https://twitter.com/marktenenholtz/status/1548667099719122945,@SaraKubik That’s a great point. Low cognitive overhead is crucial
1364,@marktenenholtz,2022-07-17 13:44:45+00:00,https://twitter.com/marktenenholtz/status/1548665008791457792,"@avikumart_ If a deep learning model architecture is a well-established building block, is it still complex?"
1365,@marktenenholtz,2022-07-17 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1548638661096325122,"Complex models are sexy, but simple models scale."
1366,@marktenenholtz,2022-07-16 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1548276273205497856,"90% of convincing a stakeholder to trust your model is getting them to trust in ML at all.

If you try to shove a fancy solution on them without laying a foundation of trust, you're gonna have a bad time."
1367,@marktenenholtz,2022-07-15 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1547913892692516864,"“The goal of forecasting is not to predict the future but to tell you what you need to know to take meaningful action in the present.”

- Paul Saffo"
1368,@marktenenholtz,2022-07-14 15:13:13+00:00,https://twitter.com/marktenenholtz/status/1547600110162309124,"@__mharrison__ In the same vein as productivity tools — some people become obsessed with using the tools and not actually getting the work done.

Big difference between reading and learning!"
1369,@marktenenholtz,2022-07-14 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1547551541581709314,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfotOUZ"
1370,@marktenenholtz,2022-07-14 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1547551538721083394,"I hope you learned something!

These are skills I've built through 1000's of hours of building ML models.

Follow me @marktenenholtz to get more high-signal ML content!"
1371,@marktenenholtz,2022-07-14 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1547551535818743811,"TL;DR:

1. Cyclical EDA + error analysis
2. No model zealotry
3. Realistic baselines
4. Clear goals
5. Rapid iteration"
1372,@marktenenholtz,2022-07-14 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1547551532941557762,"Rapid iteration

If we could know what techniques would work before attacking a problem, then we wouldn't need data scientists.

After pruning the bad ideas, the best modelers efficiently explore the remaining possibilities to land on the best solution.

Start small, work fast."
1373,@marktenenholtz,2022-07-14 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1547551530131329025,"A clear definition of a good solution

You could spend nearly infinite time on any ML problem.

But what's the most important quality of your solution? Is it squeezing out a bit more accuracy? Or is it a model that will be cheap to run?

Good answers to this save tons of time."
1374,@marktenenholtz,2022-07-14 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1547551527274954755,"Realistic baselines

Baselines are for measuring improvement, not just a vacuous number.

The best know to use realistic ones, such as human performance and intelligent heuristics.

Ask yourself: is a simple average *really* the best you can do?"
1375,@marktenenholtz,2022-07-14 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1547551524410236934,"No model zealotry

I tend to stick to a few types of models because I know they work.

But I never discourage anyone from trying something new.

Always be willing to try a new one at the first sign of good evidence."
1376,@marktenenholtz,2022-07-14 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1547551521520250880,"Cyclical EDA + Error Analysis

Good data scientists always start with EDA.

But the best data scientists use error analysis as a way to continuously learn more about their data.

Try a model, see what doesn't work, fix that problem, and repeat."
1377,@marktenenholtz,2022-07-14 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1547551518798266372,5 qualities that turn good data scientists into the top 0.1%:
1378,@marktenenholtz,2022-07-13 20:30:46+00:00,https://twitter.com/marktenenholtz/status/1547317635649605634,"@dvassallo @simonsarris @micsolana @andy23tran IIRC Americans spend way more on end of life care, which is super expensive, because we can"
1379,@marktenenholtz,2022-07-13 19:57:42+00:00,https://twitter.com/marktenenholtz/status/1547309313198374913,@businessbarista Can't wait for the day that my calendar also looks like a highway rumble strip
1380,@marktenenholtz,2022-07-13 15:19:55+00:00,https://twitter.com/marktenenholtz/status/1547239407903817729,@__mharrison__ @Upwork Wow. That’s really bad
1381,@marktenenholtz,2022-07-13 13:30:04+00:00,https://twitter.com/marktenenholtz/status/1547211761681666050,@PrasoonPratham Now that is a true classic
1382,@marktenenholtz,2022-07-13 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1547189107259895808,"What's the best/funniest comment you've ever seen in a codebase?

For me, above a ridiculously complex bit of code: 

""# Forgive me."""
1383,@marktenenholtz,2022-07-12 19:56:30+00:00,https://twitter.com/marktenenholtz/status/1546946624236175367,@rasbt Let me know what good places you find to eat 😜
1384,@marktenenholtz,2022-07-12 16:35:59+00:00,https://twitter.com/marktenenholtz/status/1546896160840892421,@rasbt Sad that it couldn't be a few months later (i.e. when I move down there). I'll have to catch you the next time you come back!
1385,@marktenenholtz,2022-07-12 16:34:49+00:00,https://twitter.com/marktenenholtz/status/1546895868321828864,"@machsci @8451group Thanks, Kirsten! And congrats to you on your recent account growth :)"
1386,@marktenenholtz,2022-07-12 16:30:54+00:00,https://twitter.com/marktenenholtz/status/1546894883373387778,@hal_jpeg https://t.co/keK7ZU8GZE
1387,@marktenenholtz,2022-07-12 16:30:15+00:00,https://twitter.com/marktenenholtz/status/1546894717429993472,@musa_lazer alright you got me there
1388,@marktenenholtz,2022-07-12 16:29:53+00:00,https://twitter.com/marktenenholtz/status/1546894624366665728,@michael_at_work okay but what if I REALLY like milk?
1389,@marktenenholtz,2022-07-12 16:29:19+00:00,https://twitter.com/marktenenholtz/status/1546894482221694980,"@jonathanrlarkin Haha I'm with you. But there are far too many beginners that take an ""all-or-nothing"" approach and feel they can't use GPU workloads at all until they drop $1500 on a GPU"
1390,@marktenenholtz,2022-07-12 16:24:03+00:00,https://twitter.com/marktenenholtz/status/1546893160370294785,"Surprised I have to say this but…

IT’S A JOKE.

I, too, have an expensive GPU that saves me from the scary “start instance” button"
1391,@marktenenholtz,2022-07-12 12:14:47+00:00,https://twitter.com/marktenenholtz/status/1546830428761464832,@svpino 😂 we all know those are the best datasets to learn from
1392,@marktenenholtz,2022-07-12 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1546826718169612288,Data scientists will spend $3k on GPUs but bristle at the thought of renting a GPU for $0.60/hr for 2 hours.
1393,@marktenenholtz,2022-07-11 18:05:55+00:00,https://twitter.com/marktenenholtz/status/1546556405393592328,@waydegilliam @8451group You got that right 🥷
1394,@marktenenholtz,2022-07-11 15:39:48+00:00,https://twitter.com/marktenenholtz/status/1546519632940568576,@AmandaMGoetz @BornFitness We’re going to absolutely crush it together. I’m so excited.
1395,@marktenenholtz,2022-07-11 13:32:47+00:00,https://twitter.com/marktenenholtz/status/1546487668736917504,@richdatasci @8451group Thanks Rich!
1396,@marktenenholtz,2022-07-11 13:16:09+00:00,https://twitter.com/marktenenholtz/status/1546483484063145985,@LBacaj @8451group Much appreciate Louie! 🙏
1397,@marktenenholtz,2022-07-11 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1546464340244500481,"Announcement:

I'd like to thank my colleagues at @8451group for giving me such great opportunities, teaching me so much, and making it fun the whole time.

That being said, all good things come to an end, and I'm on to something that's preeeeetty exciting 👀 https://t.co/1yKb1c7VL1"
1398,@marktenenholtz,2022-07-10 14:05:38+00:00,https://twitter.com/marktenenholtz/status/1546133548255649792,@psyfico @BrainfmApp No
1399,@marktenenholtz,2022-07-10 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1546101944044769280,"I get 10x more work done when I:

• Work out
• Block off 4+ hours on my calendar
• Drink a big coffee and a bigger water bottle
• Listen to @BrainfmApp 

A meeting thrown in there non-linearly decreases my productivity."
1400,@marktenenholtz,2022-07-09 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1545739555298426881,"80% of the time you spend training models is making small tweaks to your code.

The rest of the 20% is overhauling it to test out entirely new approaches.

It's worth writing well-structured code up-front, but accept that your approach might entirely change down the line."
1401,@marktenenholtz,2022-07-08 21:08:31+00:00,https://twitter.com/marktenenholtz/status/1545515197355393031,@pkmurthy1308 You don't think it's exploratory if you're exploring anomalies in your model outputs?
1402,@marktenenholtz,2022-07-08 15:07:06+00:00,https://twitter.com/marktenenholtz/status/1545424242916921352,@PathikGhugare Because top Kaggle notebooks are not Kaggle-winning solutions
1403,@marktenenholtz,2022-07-08 14:59:35+00:00,https://twitter.com/marktenenholtz/status/1545422349318635521,@tunguz You finally got me back for making you agree that XGBoost isn’t the only solution
1404,@marktenenholtz,2022-07-08 14:24:36+00:00,https://twitter.com/marktenenholtz/status/1545413545063600135,@0verfit RAPIDS is one of my favorite libraries. Feels like a hidden superpower when I’m talking to non-Kagglers
1405,@marktenenholtz,2022-07-08 13:20:12+00:00,https://twitter.com/marktenenholtz/status/1545397338738507778,"@rasbt while True:
        do EDA"
1406,@marktenenholtz,2022-07-08 13:19:40+00:00,https://twitter.com/marktenenholtz/status/1545397207075115010,@__mharrison__ That’s the way to go. I add GradCAM to image workflows
1407,@marktenenholtz,2022-07-08 12:30:46+00:00,https://twitter.com/marktenenholtz/status/1545384898046951426,"@neilgcurrie Yep, you should always go back to step 1!"
1408,@marktenenholtz,2022-07-08 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1545377180502220803,"You’re often told (and I’m guilty of this):

1. Do EDA
2. Do feature engineering
3. Fit a model

What’s wrong with this?

Fitting a model can tell you a lot about your data. EDA is 10x better when you a have a model telling you where to look"
1409,@marktenenholtz,2022-07-07 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1545014794016546816,"My best tips for managing ML projects:

1. Start with stakeholder convos
2. Do EDA until you’re sick
3. Document everything
4. Think long and hard about evaluation
5. Always use git
6. Refactor code aggressively
7. Log every experiment

Meticulous planning and structure is key!"
1410,@marktenenholtz,2022-07-06 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1544652406113284096,Forecasting is the non-sexy ML that probably brings the most value to our economy today.
1411,@marktenenholtz,2022-07-05 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1544290013608742915,"""Simple"" ML solutions are often the result of small, incremental improvements...

...made over the course of dozens/100s of experiments
...from specific insights into the data
...tailored to meet the requirements of the stakeholder

Even the simplest models take a lot of R&amp;D."
1412,@marktenenholtz,2022-07-04 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1543927618935762945,"Wikipedia is amazing but I have a hard time parsing the articles on ML methods and math topics that I've deeply understood for years.

Specificity/rigor and simplicity will battle forever (and you can take advantage of it by learning to communicate)."
1413,@marktenenholtz,2022-07-03 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1543565230256533504,"Your ML models will get 10x better when you focus more on:

• Smaller architectures
• Understanding the data
• Rapid iteration

...than if you focus on:

• Huge models
• Expensive compute
• Hyperparameter tuning

Better features + more experiments = success"
1414,@marktenenholtz,2022-07-02 12:51:34+00:00,https://twitter.com/marktenenholtz/status/1543215804866494464,Maybe if we could just let the Codex model look at our dataset… wait a minute
1415,@marktenenholtz,2022-07-02 12:49:43+00:00,https://twitter.com/marktenenholtz/status/1543215342394261504,But I do it anyways
1416,@marktenenholtz,2022-07-02 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1543202838482161667,Using Copilot to write ML code faster is an impressive solution to the wrong problem.
1417,@marktenenholtz,2022-07-01 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1542840451555663872,"The sign of a perfect ML product is making a non-deterministic process feel deterministic and effortless.

High reliability, low cognitive overhead."
1418,@marktenenholtz,2022-06-30 14:16:38+00:00,https://twitter.com/marktenenholtz/status/1542512438901256196,@AiSimonThompson @curiovana Model evaluation is the skill that leads to the most success on Kaggle. I’d say this is pretty unfair to say.
1419,@marktenenholtz,2022-06-30 13:55:11+00:00,https://twitter.com/marktenenholtz/status/1542507039678574592,"@curiovana @AiSimonThompson They generally perform about the same. I just like LightGBM because it's really fast on the CPU. If you have a GPU, XGBoost might be your best bet."
1420,@marktenenholtz,2022-06-30 13:43:42+00:00,https://twitter.com/marktenenholtz/status/1542504150868627456,@Seun_A_Ajayi It... doesn't?
1421,@marktenenholtz,2022-06-30 13:43:28+00:00,https://twitter.com/marktenenholtz/status/1542504090353008641,@tunguz @don_nme :)
1422,@marktenenholtz,2022-06-30 13:43:14+00:00,https://twitter.com/marktenenholtz/status/1542504031364567042,@AiSimonThompson Either one works
1423,@marktenenholtz,2022-06-30 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1542478063455199234,"XGBoost/LightGBM is not all you need.

…but they’re probably the fastest way to a good solution on tabular data."
1424,@marktenenholtz,2022-06-30 01:25:48+00:00,https://twitter.com/marktenenholtz/status/1542318450323906560,@JFPuget @giffmana Early stopping is another example
1425,@marktenenholtz,2022-06-30 01:13:39+00:00,https://twitter.com/marktenenholtz/status/1542315394454659072,"@JFPuget @giffmana Sometimes your degraded validation set accuracy correctly mirrors the degradation you’ll see on the testing set. Other times, it doesn’t. For instance, knowledge distillation can lead to over-optimistic validation set accuracy that will be much worse on the test set."
1426,@marktenenholtz,2022-06-29 20:58:54+00:00,https://twitter.com/marktenenholtz/status/1542251283062689792,@JFPuget @giffmana I’ve always believed we need separate terms for overfitting on the training data (what @giffmana is getting at) and overfitting on the validation data (what you’re getting at). They’re both phenomena but I think they don’t always occur simultaneously
1427,@marktenenholtz,2022-06-29 17:00:07+00:00,https://twitter.com/marktenenholtz/status/1542191191617196032,When I get a data quality alert from engineering https://t.co/DkIebYMsZv
1428,@marktenenholtz,2022-06-29 15:30:37+00:00,https://twitter.com/marktenenholtz/status/1542168667840024581,@SimonHoiberg Congrats Simon! Your channel is awesome
1429,@marktenenholtz,2022-06-29 13:09:01+00:00,https://twitter.com/marktenenholtz/status/1542133034916499457,"They probably help if you have many more problems than you have data scientists, but I’m convinced they don’t save nearly as much time as they claim."
1430,@marktenenholtz,2022-06-29 12:55:08+00:00,https://twitter.com/marktenenholtz/status/1542129541166006272,@avikumart_ You have to set up all of the scaffolding for data preprocessing/evaluation/etc whether you're using them or not. After that it takes me very little time to fit a model that I know will work pretty well. Why spend all of that money for such little time savings?
1431,@marktenenholtz,2022-06-29 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1542115683651031040,"The best part about learning by building:

I've worked on such a variety of ML problems that I have a ready-to-go framework that I understand completely for 95% of the problems I see.

New frameworks (like Hugging Face) help a ton, but no-code/low-code ML tools don't help at all."
1432,@marktenenholtz,2022-06-28 18:33:00+00:00,https://twitter.com/marktenenholtz/status/1541852180096892928,I will never learn how to use VLOOKUP. I don't even really know what it is.
1433,@marktenenholtz,2022-06-28 16:06:22+00:00,https://twitter.com/marktenenholtz/status/1541815277414932480,"@__mharrison__ Dreaming in code seems different because the main way we interact with it is written, and IIRC written text is messed up in dreams.

That being said, I have verbally described Python code in my dreams before."
1434,@marktenenholtz,2022-06-28 16:03:05+00:00,https://twitter.com/marktenenholtz/status/1541814451917230081,"@alfcnz Not an instructor, but I used edstem in my Master's. It was a significant upgrade over Piazza and I never had any issues with it."
1435,@marktenenholtz,2022-06-28 15:05:07+00:00,https://twitter.com/marktenenholtz/status/1541799863163183111,@ph_singer @JFPuget Reading that thread is pretty funny. I hope he livestreams himself eating the hat. https://t.co/E1Sc5VDrRM
1436,@marktenenholtz,2022-06-28 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1541753289397506049,"Everyone assumes I'm amazing at Excel because I'm a data scientist.

I have the Excel skills of an 8th grader."
1437,@marktenenholtz,2022-06-27 12:02:33+00:00,https://twitter.com/marktenenholtz/status/1541391532477108228,"@svpino I usually start with:

1. Getting more data
2. Getting better data"
1438,@marktenenholtz,2022-06-27 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1541390900328181762,"A prerequisite to training a great model is training a bad model.

Start from a dead-simple baseline and iterate, iterate, iterate."
1439,@marktenenholtz,2022-06-26 19:01:49+00:00,https://twitter.com/marktenenholtz/status/1541134654773788672,@LSTMeow it tells us this and yet we still can't pass -1 as a value like in sklearn
1440,@marktenenholtz,2022-06-26 16:52:48+00:00,https://twitter.com/marktenenholtz/status/1541102189724188673,@MeganRisdal My gut tells me roasted butternut squash would be delicious in there but depends on how much time you’re willing to spend making one bowl 😂
1441,@marktenenholtz,2022-06-26 16:04:11+00:00,https://twitter.com/marktenenholtz/status/1541089951953084416,@tunguz Congrats Bojan!
1442,@marktenenholtz,2022-06-26 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1541028511321690112,"When a data scientist tells you:

""I spend 80% of my time on machine learning.""

...they actually mean:

""I spend 80% of my time on data processing and the rest on ML"""
1443,@marktenenholtz,2022-06-25 12:42:12+00:00,https://twitter.com/marktenenholtz/status/1540676735804420096,"@tunguz It’s funny because we usually say that tabular data == structured, text/images == unstructured.

But those domains have enough structure to create models that generalize in their domain, whereas we can’t find that in the “structured” world of tabular data."
1444,@marktenenholtz,2022-06-25 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1540666135430844416,"Data management is key for building your product.

Great data management ensures you and the folks across your company will build great products for 5+ years to come."
1445,@marktenenholtz,2022-06-24 21:57:28+00:00,https://twitter.com/marktenenholtz/status/1540454084699983879,@mervenoyann Happy birthday!
1446,@marktenenholtz,2022-06-24 18:57:52+00:00,https://twitter.com/marktenenholtz/status/1540408887655940097,"@andrehaykaljr Even in Kentucky, I sweat like an animal on a short 30 minute walk with my dog"
1447,@marktenenholtz,2022-06-24 14:21:07+00:00,https://twitter.com/marktenenholtz/status/1540339240042565632,@tibo_maker Easy bookmark. Great thread!
1448,@marktenenholtz,2022-06-24 14:05:41+00:00,https://twitter.com/marktenenholtz/status/1540335354473062402,"@machsci One join on a text field? Okay, Satan 😂"
1449,@marktenenholtz,2022-06-24 12:29:55+00:00,https://twitter.com/marktenenholtz/status/1540311253666697216,"@WellPaidGeek It’s definitely mentally taxing.

Brutal to complete a high-pressure round and then think “that could’ve been the second to last round, or there could be 3 more for all I know!”"
1450,@marktenenholtz,2022-06-24 12:26:19+00:00,https://twitter.com/marktenenholtz/status/1540310348267388928,@svpino 😂
1451,@marktenenholtz,2022-06-24 12:20:19+00:00,https://twitter.com/marktenenholtz/status/1540308840289378305,@alepiad AutoML is cool but it’s the ultimate trap if you don’t know how to set up proper cross validation
1452,@marktenenholtz,2022-06-24 12:15:17+00:00,https://twitter.com/marktenenholtz/status/1540307570652577793,"@michaelroker Often, the “real world” dataset is no dataset at all!"
1453,@marktenenholtz,2022-06-24 12:14:22+00:00,https://twitter.com/marktenenholtz/status/1540307341047992320,@AndrewLaganaro Takes a long time to get your feature engineering right
1454,@marktenenholtz,2022-06-24 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1540303742662090752,"95% of ML tutorials:

• Curated dataset
• Basic features
• Random 20% k-fold
• Hyperparams that just work

Real life:

• Messy/nonexistent dataset
• Weeks of feature engineering
• Complex cross-validation setup
• Thoroughly tuned hyperparams

We need to bridge this gap."
1455,@marktenenholtz,2022-06-24 00:13:09+00:00,https://twitter.com/marktenenholtz/status/1540125840473726976,@Adrian_Szymczak This is really well put
1456,@marktenenholtz,2022-06-23 18:10:04+00:00,https://twitter.com/marktenenholtz/status/1540034469377314819,"@evidenceSE @GordonMcGregor @machsci @KrutMarcin Wow, great research. Thanks for sharing"
1457,@marktenenholtz,2022-06-23 17:08:49+00:00,https://twitter.com/marktenenholtz/status/1540019056794902529,@GordonMcGregor @machsci @KrutMarcin It’s not even the time frame that I’m referring to so much as I am the end result
1458,@marktenenholtz,2022-06-23 17:05:03+00:00,https://twitter.com/marktenenholtz/status/1540018108215214080,"@GordonMcGregor @machsci @KrutMarcin Sure, I'm plenty aware of that. But the fact that you can commit to a fairly specific result within a general time frame makes it significantly more predictable than ML."
1459,@marktenenholtz,2022-06-23 17:01:36+00:00,https://twitter.com/marktenenholtz/status/1540017238656385024,"@ericjang11 This is really cool.

I always try to relate this to intuition, and the first thing that came to mind was how you practice your finish in a golf swing.

You focus on how get to a good swing finish b/c it properly informs what mechanics you need to use to get there. Sorta?"
1460,@marktenenholtz,2022-06-23 16:30:23+00:00,https://twitter.com/marktenenholtz/status/1540009384469831680,"@machsci @KrutMarcin Well said! Whether folks realize it or not, there's always artificial pressure and expectations that are created in agile systems.

It's great for deterministic systems like SWE. Not so much for ML."
1461,@marktenenholtz,2022-06-23 15:15:48+00:00,https://twitter.com/marktenenholtz/status/1539990611243286531,@nischay_twt Nice! At one point I tried to include the text difference between the target and anchor. Can’t wait to check your approach out in more detail!
1462,@marktenenholtz,2022-06-23 14:39:15+00:00,https://twitter.com/marktenenholtz/status/1539981413826658306,@sacdallago Many times (unsuccessfully)
1463,@marktenenholtz,2022-06-23 14:38:55+00:00,https://twitter.com/marktenenholtz/status/1539981330762633217,@thejamesvance And who knows what shows up during that!
1464,@marktenenholtz,2022-06-23 14:38:34+00:00,https://twitter.com/marktenenholtz/status/1539981244397756418,@don_nme Everything is relative :)
1465,@marktenenholtz,2022-06-23 14:38:22+00:00,https://twitter.com/marktenenholtz/status/1539981191419514880,@cor3bit These... do not work well
1466,@marktenenholtz,2022-06-23 13:56:03+00:00,https://twitter.com/marktenenholtz/status/1539970541943021568,@gusthema Genuinely curious -- do they think this is good PR?
1467,@marktenenholtz,2022-06-23 13:51:43+00:00,https://twitter.com/marktenenholtz/status/1539969450945916929,@gusthema Absolutely not.
1468,@marktenenholtz,2022-06-23 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1539941359498407936,"Software engineering sprints are highly predictable.

Machine learning sprints are highly unpredictable.

You should expect to try 10+ things that don't work for every 1 that does."
1469,@marktenenholtz,2022-06-23 11:20:48+00:00,https://twitter.com/marktenenholtz/status/1539931474790580230,"@PrasoonPratham Nice thread, Pratham!"
1470,@marktenenholtz,2022-06-23 11:05:06+00:00,https://twitter.com/marktenenholtz/status/1539927523072643073,@PredaGabi @kaggle Thanks Gabriel!
1471,@marktenenholtz,2022-06-23 00:23:43+00:00,https://twitter.com/marktenenholtz/status/1539766114464440321,@waydegilliam Thanks Wayde!
1472,@marktenenholtz,2022-06-22 20:52:28+00:00,https://twitter.com/marktenenholtz/status/1539712949916401664,@JFPuget @kaggle @huggingface @Tim_Dettmers Haha okay I was referring to “strategies that didn’t only involve concating other targets to the prompt” but from that angle you’re probably right
1473,@marktenenholtz,2022-06-22 20:48:57+00:00,https://twitter.com/marktenenholtz/status/1539712063945203712,"@JFPuget @kaggle @huggingface @Tim_Dettmers It's this one: https://t.co/azWdGnaYaD

I tried to do this with a nearest neighbors search on embeddings but it didn't work too well."
1474,@marktenenholtz,2022-06-22 20:09:57+00:00,https://twitter.com/marktenenholtz/status/1539702249810796548,"@JFPuget @kaggle @huggingface @Tim_Dettmers Yup, I saw that. I’m not saying teams that outperformed me 100% used the row magic. I’m saying they probably found some feature engineering strategy that I missed.

I saw one creative idea where they found the CPC context at a lower level, for example."
1475,@marktenenholtz,2022-06-22 17:36:10+00:00,https://twitter.com/marktenenholtz/status/1539663549462589440,"@JFPuget @kaggle @huggingface @Tim_Dettmers So far, every writeup I've seen where someone beat me by a significant margin (&gt;0.002 or so) included some clever bit of feature engineering that I didn't use. I don't think using other rows was the only way to beat me.

If you didn't, I'd love to hear how you did it."
1476,@marktenenholtz,2022-06-22 16:59:26+00:00,https://twitter.com/marktenenholtz/status/1539654306743582720,@wightmanr @huggingface Congrats Ross! Talk about an iconic duo
1477,@marktenenholtz,2022-06-22 14:48:45+00:00,https://twitter.com/marktenenholtz/status/1539621417658077184,"@_clashluke @kaggle @JFPuget @huggingface @Tim_Dettmers Great question. I was interested in trying it, but I usually find optimizers give less ""bang for your buck"" than trying new feature engineering strategies and whatnot.

I'll give it a shot in an upcoming competition though. Do you have any references for hyperparams to try?"
1478,@marktenenholtz,2022-06-22 13:10:03+00:00,https://twitter.com/marktenenholtz/status/1539596580856238080,@NainaChaturved8 @kaggle Thank you so much!
1479,@marktenenholtz,2022-06-22 12:00:20+00:00,https://twitter.com/marktenenholtz/status/1539579033049935873,"@kaggle @JFPuget @huggingface @Tim_Dettmers I went over my:

• Model evaluation
• Feature engineering
• Training methodology

...but there's so much more to talk about:

• 8-bit optimizers
• @huggingface's Trainer
• DeBERTa and why it's so powerful

Follow me @marktenenholtz so you don't miss those threads!"
1480,@marktenenholtz,2022-06-22 12:00:19+00:00,https://twitter.com/marktenenholtz/status/1539579030407524352,"@kaggle @JFPuget @huggingface @Tim_Dettmers At the end of the day, the teams that beat me had a much smaller ensemble built on top of much better feature engineering.

And that's why they beat me!

If you think Kaggle is just throwing compute at the competition, I invite you to get torched in an upcoming competition 😂"
1481,@marktenenholtz,2022-06-22 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1539579027807096832,"@kaggle @JFPuget @huggingface @Tim_Dettmers A final note:

I'm confident someone will read this and think ""look how stupid that solution is, you had to ensemble all of those models! That's totally impractical!""

Yeah, it is. I only did it because I ran out of time to do feature engineering."
1482,@marktenenholtz,2022-06-22 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1539579025303121920,"@kaggle @JFPuget @huggingface @Tim_Dettmers The linear optimizer is my go-to since I find it to be the most reliable, and it worked the best for me here.

Stacking and boosting gave me worse (or equivalent) results."
1483,@marktenenholtz,2022-06-22 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1539579022790668289,"@kaggle @JFPuget @huggingface @Tim_Dettmers When you have that many models, it's hard to ensemble them properly by hand.

So, I don't!

You have a few options:

1. Stacking (stacked model on predictions)
2. Boosting (stacked model on last layer's embeddings)
3. Linear optimizer"
1484,@marktenenholtz,2022-06-22 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1539579017484849152,"@kaggle @JFPuget @huggingface @Tim_Dettmers I was even able to train a V3 DeBERTa Base model with a batch size of 32.

Insane."
1485,@marktenenholtz,2022-06-22 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1539579014804688896,"@kaggle @JFPuget @huggingface The most unique ""trick"" that I can contribute is the use of @Tim_Dettmers' 8-bit Adam optimizer.

The only GPU I have easy access to is an RTX 2070, which ain't much for this stuff.

This optimizer saves a ton of memory at no accuracy hit (none that I could see, at least)."
1486,@marktenenholtz,2022-06-22 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1539579012153872385,"@kaggle @JFPuget For this competition, my focus was learning how to use @huggingface's Trainer.

I used the Trainer for all of my models.

It's an incredibly useful domain-specialized API. I'm not sure if I'll always use it or not, but that's for another thread :)"
1487,@marktenenholtz,2022-06-22 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1539579009469517824,"@kaggle @JFPuget The models:

It should be of no surprise that pretrained transformers dominated here.

I try to pick a focus area for learning during each competition (if you wanna learn how to do this, go here: https://t.co/VPaDKkJSZU)"
1488,@marktenenholtz,2022-06-22 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1539579006869045248,"@kaggle Things I tried that didn't work:

• FAISS for finding additional CPC information
• MLM pretraining on CPC descriptions
• Adding patent vocabulary to my tokenizers
• Ordinal regression head (copied from @JFPuget)
• Using Pearson correlation as a loss function

and much more."
1489,@marktenenholtz,2022-06-22 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1539579004293828608,"@kaggle The real trick (that I missed) was that there was high correlation between the different targets for each anchor.

So, some of the high-ranked finishers added the other targets for each anchor to the end of their prompt, which helped a lot."
1490,@marktenenholtz,2022-06-22 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1539579001676566528,"@kaggle This was good, but I wanted some more variety.

So, I trained a couple more models using the prompt:

""similarity of {anchor} to {target} in relation to {CPC description}""

This lead to a reliable boost in my cross-validation when I ensembled."
1491,@marktenenholtz,2022-06-22 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1539578999097044992,"@kaggle I (along with most others) found the best way to do this was to structure your prompt as:

""{anchor} + [SEP] + {target} + [SEP] + {CPC description}"""
1492,@marktenenholtz,2022-06-22 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1539578996479823873,"@kaggle The goal in this competition was to properly represent the context of the patent to the model.

We were provided with CPC codes, which are basically brief text descriptions of the subject matter of the patent."
1493,@marktenenholtz,2022-06-22 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1539578993908674560,"@kaggle To combine insights 1 and 2, sklearn has a relatively new splitter called StratifiedGroupKFold that does the trick.

God bless sklearn!

Let's move on to the feature engineering:"
1494,@marktenenholtz,2022-06-22 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1539578991228571648,"@kaggle Insight 2:

This isn't entirely necessary, but for perfection's sake it's usually smart to ensure each validation set has a comparable target distribution.

This just means that we're less at risk of having folds that are significantly easier/harder than others."
1495,@marktenenholtz,2022-06-22 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1539578988602896384,"@kaggle There were some anchors that were also used as targets. It's probably smart to hold those out, too.

This strategy is quite straightforward to implement using sklearn's GroupKFold."
1496,@marktenenholtz,2022-06-22 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1539578986006622208,"@kaggle Insight 1:

This means we need to ensure that the anchors included in our validation set are *not* in the training set.

Doing so means our validation score is a strong indicator of how our model generalizes to anchors it's never seen before."
1497,@marktenenholtz,2022-06-22 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1539578983506882561,"@kaggle The key insights here are:

1. Anchors in production != anchors in training
2. Different anchors have different score distributions"
1498,@marktenenholtz,2022-06-22 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1539578980608516096,"@kaggle Here's a sample of the phrases you were asked to compare (anchors and targets):

Anchors were repeated several times with a variety of targets, and similarities were on a scale from 0 to 1.

The evaluation metric was the Pearson correlation between your predictions and the score. https://t.co/tpasKgnblH"
1499,@marktenenholtz,2022-06-22 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1539578973683712000,"@kaggle My solution is pretty simple since I took a long break in the middle of the competition and only showed back up at the end.

But, it should go to show how a properly-evaluated solution can be effective regardless of simplicity.

Let's start with the model evaluation:"
1500,@marktenenholtz,2022-06-22 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1539578971049758720,@kaggle You can check the competition out here: https://t.co/Pexqh078KN
1501,@marktenenholtz,2022-06-22 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1539578968394719233,"@kaggle Briefly, the competition gave you phrases found in patents and asked you to rate how similar they are.

Your similarity score was in the context of the patent they were found.

E.g., ""computer"" and ""bug"" are very similar in the context of tech, but not in the context of wildlife."
1502,@marktenenholtz,2022-06-22 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1539578965920083968,How I jumped 187 positions on the leaderboard and got a solo silver medal in a @Kaggle NLP competition:
1503,@marktenenholtz,2022-06-22 02:08:46+00:00,https://twitter.com/marktenenholtz/status/1539430160247693316,@dr_hb_ai Best of luck! May the CV/LB gods be in your favor
1504,@marktenenholtz,2022-06-22 01:20:24+00:00,https://twitter.com/marktenenholtz/status/1539417988415250437,@tunguz Ha I know I shouldn’t open that can of worms. But it definitely explains a lot
1505,@marktenenholtz,2022-06-22 01:17:09+00:00,https://twitter.com/marktenenholtz/status/1539417173956911104,@tunguz Why is there so much toxicity in academia…
1506,@marktenenholtz,2022-06-21 22:00:00+00:00,https://twitter.com/marktenenholtz/status/1539367557911695360,@capetorch https://t.co/OHcxEEyU9U
1507,@marktenenholtz,2022-06-21 17:00:11+00:00,https://twitter.com/marktenenholtz/status/1539292106732150784,"GitHub Copilot is now on sale for $10/mo ($100/year).

Will you be subscribing? https://t.co/SWnADeSOrf"
1508,@marktenenholtz,2022-06-21 16:16:44+00:00,https://twitter.com/marktenenholtz/status/1539281172685107201,@Gillesvdwiele @leassis91 @kaggle This is it exactly. I have the most consistent results using a linear optimizer but occasionally a stacking model works a bit better.
1509,@marktenenholtz,2022-06-21 13:10:33+00:00,https://twitter.com/marktenenholtz/status/1539234318442278912,@businessbarista I’m in. I’ll use my data viz skills to make a whole dashboard to track our progress.
1510,@marktenenholtz,2022-06-21 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1539216571872079873,"Just grabbed another solo silver in a @Kaggle  NLP competition!

It involved:

• Robust cross-validation
• Several transformer architectures
• Optimized weights for ensembling

A simple solution, but well-executed enough to get a decent placement.

Full write-up to follow :)"
1511,@marktenenholtz,2022-06-21 11:47:33+00:00,https://twitter.com/marktenenholtz/status/1539213430162345985,@tunguz They’re JUST matrix multiplication like Kaggle is JUST throwing a huge ensemble at the problem
1512,@marktenenholtz,2022-06-20 13:47:42+00:00,https://twitter.com/marktenenholtz/status/1538881280028418048,@tunguz Average Intelligence
1513,@marktenenholtz,2022-06-20 13:11:20+00:00,https://twitter.com/marktenenholtz/status/1538872127046225922,@AlexReibman Been considering adding this for a while but haven’t yet. I keep hearing good things about Black.
1514,@marktenenholtz,2022-06-20 12:46:13+00:00,https://twitter.com/marktenenholtz/status/1538865806716674048,"@parker_brydon No, I find most templates to not be too helpful. One exception (for me at least) is https://t.co/hN6nLCW8nl"
1515,@marktenenholtz,2022-06-20 12:44:26+00:00,https://twitter.com/marktenenholtz/status/1538865357900898308,"@ammaryh92 No, it’s a CLI configurable file that lets me specify everything from architectures to hyperparameters"
1516,@marktenenholtz,2022-06-20 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1538854184233017345,"I start every ML project by creating these files, more or less in this order:

1. .gitignore/README.md
2. eda.ipynb
3. data_notes.md

Days later...

4. baseline.(py/ipynb)
5. https://t.co/W9EVm4QIk9
6. https://t.co/ssL5g4E2CJ
7. error_analysis.ipynb

Structure -&gt; creativity"
1517,@marktenenholtz,2022-06-20 00:55:17+00:00,https://twitter.com/marktenenholtz/status/1538686894321680384,@iScienceLuvr @jeremyphoward @NmaViv Didn’t know he had a twitter account. Thanks for tagging him!
1518,@marktenenholtz,2022-06-20 00:33:21+00:00,https://twitter.com/marktenenholtz/status/1538681372725989376,"@jeremyphoward This is a very ubiquitous style that I’ve always associated with this Kaggler: https://t.co/RKFXTYhENx

Honestly not sure exactly where it comes from. It’s evolved over many, many years of public notebook sharing."
1519,@marktenenholtz,2022-06-19 13:26:44+00:00,https://twitter.com/marktenenholtz/status/1538513614163828737,@svpino This is it exactly. It’s part of the solution engineering phase.
1520,@marktenenholtz,2022-06-19 12:34:58+00:00,https://twitter.com/marktenenholtz/status/1538500585103601669,"Every data scientist needs to know Parkinson’s Law:

It says that work expands to fill the time you have available to complete it.

In ML projects, there’s always something else to try.

But, at some point, you gotta know when “good” is “good enough” and deliver some value."
1521,@marktenenholtz,2022-06-19 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1538491797038501889,"If you were a NaN, which layer of my transformer would you be coming from?"
1522,@marktenenholtz,2022-06-18 14:24:45+00:00,https://twitter.com/marktenenholtz/status/1538165827387723776,"@giffmana @ajmooch Can’t blame you, my experiment tracking dashboards always start to look crazy 😅

I’d love to see informal blog-style posts where the authors talk about these things. Much easier on the authors and the readers get the same benefit."
1523,@marktenenholtz,2022-06-18 14:20:40+00:00,https://twitter.com/marktenenholtz/status/1538164799976132609,"@tibo_maker Well for example, if I started posting tweets that looked like JK’s, they wouldn’t do well because my audience doesn’t like that stuff, and vice-versa.

Most of the creativity is in giving context to the model on both what works for others AND what works for that tweeter."
1524,@marktenenholtz,2022-06-18 14:14:24+00:00,https://twitter.com/marktenenholtz/status/1538163223697948672,"@tibo_maker I built my own personal ML model to do this one time. 

It’s awesome, but I think there are some augmentations necessary to make it *really* good.

Let me know if you ever want any ideas :)"
1525,@marktenenholtz,2022-06-18 13:53:06+00:00,https://twitter.com/marktenenholtz/status/1538157863243243520,@giffmana Love that you published this. Ever consider doing a “gory details” write up like Meta did for OPT?
1526,@marktenenholtz,2022-06-18 13:03:53+00:00,https://twitter.com/marktenenholtz/status/1538145478096965633,"@rasbt @PyTorchLightnin I always get worried that if I set up those notifications, it’ll no longer be me training my model. It’ll be my model training me."
1527,@marktenenholtz,2022-06-18 13:00:00+00:00,https://twitter.com/marktenenholtz/status/1538144499939135489,@giffmana How long did the original ViT’s take to train?
1528,@marktenenholtz,2022-06-18 12:58:47+00:00,https://twitter.com/marktenenholtz/status/1538144190550491138,@giffmana Recently had to train 100+ models (all on different data) in a loop for a project. It definitely tested my organizational capacity 😅
1529,@marktenenholtz,2022-06-18 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1538129407935586305,What do you do to pass the time while your models train?
1530,@marktenenholtz,2022-06-18 02:18:35+00:00,https://twitter.com/marktenenholtz/status/1537983079347675140,@alepiad Congrats! It’s been fun to follow your journey!
1531,@marktenenholtz,2022-06-17 14:26:39+00:00,https://twitter.com/marktenenholtz/status/1537803917953753088,@Danny_Mogere It means that you should try to solve the problem however you would intuitively think to solve it if you had to do it yourself.
1532,@marktenenholtz,2022-06-17 12:59:16+00:00,https://twitter.com/marktenenholtz/status/1537781927423496192,@avikumart_ That’s part of step 4. The two are inextricable!
1533,@marktenenholtz,2022-06-17 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1537767058846322688,"Approaching ML problems, 101:

1. Try to do the task yourself by intuition
2. See how accurate you were
3. Write down your method as steps
4. Create a model that represents those steps

It’s an incredibly simple and powerful mental model."
1534,@marktenenholtz,2022-06-16 17:00:18+00:00,https://twitter.com/marktenenholtz/status/1537480194470031360,"Model evaluation seems easy.

…until someone asks you for a confident answer on how your model will perform in production."
1535,@marktenenholtz,2022-06-16 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1537404649262551040,My goal is to teach every data scientist what it took me years of Kaggling and putting models in production to learn.
1536,@marktenenholtz,2022-06-16 00:46:45+00:00,https://twitter.com/marktenenholtz/status/1537235195229011968,@zonkd Yeah! Rust is awesome for that stuff
1537,@marktenenholtz,2022-06-16 00:46:02+00:00,https://twitter.com/marktenenholtz/status/1537235014831972352,@Echeney69 I said lesser :)
1538,@marktenenholtz,2022-06-15 17:41:10+00:00,https://twitter.com/marktenenholtz/status/1537128091629527040,@tunguz This is solved by web3. Your customer service agent becomes a slightly conscious large language model
1539,@marktenenholtz,2022-06-15 15:30:39+00:00,https://twitter.com/marktenenholtz/status/1537095248979103752,@blackhatwizardd It’s a pain to get initially but effortless to renew once you have it. 100% recommend.
1540,@marktenenholtz,2022-06-15 14:36:47+00:00,https://twitter.com/marktenenholtz/status/1537081690933411841,@nischay_twt @tunguz Someone did it to me too lol
1541,@marktenenholtz,2022-06-15 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1537042257739517957,"Build your own notebook and make your own conclusions.

Then, check out the winning notebooks and see how others approached the problem.

There's a lot more variety than typical Kaggle competitions!

Check them out here: https://t.co/W7hl5RqhuB"
1542,@marktenenholtz,2022-06-15 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1537042254241509376,"Machine learning models aren't just about predictions. They're also about drawing insights.

Kaggle does great supervised learning competitions, but these don't build storytelling skills.

Want some real practice?

Check out the Kaggle's lesser-known Analytics competitions! https://t.co/sbS6aoKRhy"
1543,@marktenenholtz,2022-06-15 11:37:46+00:00,https://twitter.com/marktenenholtz/status/1537036639884689408,"@SEdwardHolland It’s a really cool programming language, it’s just not there yet."
1544,@marktenenholtz,2022-06-15 11:36:57+00:00,https://twitter.com/marktenenholtz/status/1537036432874815488,"@Kevbonham @theabhimanyu @psk90_ai @JFPuget Folks, it’s just a joke.

I have dabbled in the Julia ecosystem. I think it’s pretty cool, but it just hasn’t panned out yet."
1545,@marktenenholtz,2022-06-14 21:57:55+00:00,https://twitter.com/marktenenholtz/status/1536830319432712192,@mervenoyann @huggingface This is wonderful. Well done!
1546,@marktenenholtz,2022-06-14 16:29:44+00:00,https://twitter.com/marktenenholtz/status/1536747727509413889,"@prthgo Don’t worry, though. You’ll learn patterns over time that make it so, so much easier."
1547,@marktenenholtz,2022-06-14 16:28:41+00:00,https://twitter.com/marktenenholtz/status/1536747464992116738,@prthgo And the most important part :)
1548,@marktenenholtz,2022-06-14 14:35:40+00:00,https://twitter.com/marktenenholtz/status/1536719023202783232,@svpino Yeah but the metric system made my engineering degree so much easier 😂
1549,@marktenenholtz,2022-06-14 14:18:01+00:00,https://twitter.com/marktenenholtz/status/1536714579257982982,@psk90_ai I think @JFPuget put it pretty well here:
1550,@marktenenholtz,2022-06-14 12:56:56+00:00,https://twitter.com/marktenenholtz/status/1536694173939687427,@roydanroy It’s what enables me to tweet so much
1551,@marktenenholtz,2022-06-14 12:17:53+00:00,https://twitter.com/marktenenholtz/status/1536684350208720896,@slowburninfuse R is basically in the same situation
1552,@marktenenholtz,2022-06-14 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1536679864979161089,"Python is a slow language.

But it’s NOT slow for machine learning.

If you use any popular ML framework, 95% of your code is running in C anyways.

It’s hard to beat the development speed of Python, too. So you should probably stick with it."
1553,@marktenenholtz,2022-06-13 17:47:45+00:00,https://twitter.com/marktenenholtz/status/1536404972321005568,@PrasoonPratham I’m in the business of keeping it real
1554,@marktenenholtz,2022-06-13 17:37:58+00:00,https://twitter.com/marktenenholtz/status/1536402510126235651,@JeremyFromEarth I think it’s better to start by learning top-down rather than bottom-up
1555,@marktenenholtz,2022-06-13 17:00:15+00:00,https://twitter.com/marktenenholtz/status/1536393018562842624,"You can start with any deep learning framework. It doesn't matter.

You can always switch to PyTorch later."
1556,@marktenenholtz,2022-06-13 13:03:14+00:00,https://twitter.com/marktenenholtz/status/1536333371042938881,@think___y No doubt. It starts with us experienced practitioners communicating in less jargon (where possible)
1557,@marktenenholtz,2022-06-13 12:45:54+00:00,https://twitter.com/marktenenholtz/status/1536329011722440705,@akshay_pachaar Takes more skill!
1558,@marktenenholtz,2022-06-13 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1536317482910158848,Your biggest barrier to becoming a great data scientist is resisting the urge to build fancy solutions to simple problems.
1559,@marktenenholtz,2022-06-13 00:44:13+00:00,https://twitter.com/marktenenholtz/status/1536147394626211841,@rasbt Logistic regression isn’t on the chopping block yet. Much more capable employee
1560,@marktenenholtz,2022-06-12 22:17:26+00:00,https://twitter.com/marktenenholtz/status/1536110456217255936,@DataScienceRyan Exactly why we can’t have this kind of dead weight around our office. Even if we’re remote.
1561,@marktenenholtz,2022-06-12 19:57:17+00:00,https://twitter.com/marktenenholtz/status/1536075183865331714,@oliverjumpertz @Alkafaweey @0xbnomial @svpino More like an iconic duo!
1562,@marktenenholtz,2022-06-12 14:42:22+00:00,https://twitter.com/marktenenholtz/status/1535995932793311233,"@tunguz One time I accidentally deleted python3 on my Ubuntu partition.

Had to take it to the shed out back."
1563,@marktenenholtz,2022-06-12 13:59:17+00:00,https://twitter.com/marktenenholtz/status/1535985089703706628,Naive bayes better be sweating
1564,@marktenenholtz,2022-06-12 13:57:57+00:00,https://twitter.com/marktenenholtz/status/1535984755115708418,@Alkafaweey @oliverjumpertz @0xbnomial @svpino ❤️
1565,@marktenenholtz,2022-06-12 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1535955086890766336,"ANNOUNCEMENT: Due to a worsening economy, I've decided to lay off simple linear regression.

Ridge regression will, from this point forward, assume all of their duties."
1566,@marktenenholtz,2022-06-11 16:23:22+00:00,https://twitter.com/marktenenholtz/status/1535658962414190594,@xtinacomputes What’s your most valuable social acct? Is it twitter? You’re a lot more diversified than I am
1567,@marktenenholtz,2022-06-11 16:21:40+00:00,https://twitter.com/marktenenholtz/status/1535658535668826112,@xtinacomputes This account is the most valuable thing I own lol
1568,@marktenenholtz,2022-06-11 14:34:21+00:00,https://twitter.com/marktenenholtz/status/1535631525798084610,@rasbt @tunguz @LeopolisDream Interesting. Is there any reason to use this over ForestInference from cuML?
1569,@marktenenholtz,2022-06-11 00:47:46+00:00,https://twitter.com/marktenenholtz/status/1535423510801154048,@tunguz Installing from cache
1570,@marktenenholtz,2022-06-11 00:12:50+00:00,https://twitter.com/marktenenholtz/status/1535414718457946113,@IttaiSvidler @OnChainDen Bruh
1571,@marktenenholtz,2022-06-10 22:21:02+00:00,https://twitter.com/marktenenholtz/status/1535386583012081665,"@JFPuget This is wonderful!

It looks like this is the case but just to clarify -- does it support ""unrolling"" for multiple-horizon forecasts? I.e., if my forecast horizon is N, can it use all of the day up until today and then predict t+1, t+2, ... , t+N?"
1572,@marktenenholtz,2022-06-10 17:33:57+00:00,https://twitter.com/marktenenholtz/status/1535314336259907584,@charles_irl That’s absolutely terrifying
1573,@marktenenholtz,2022-06-10 16:33:33+00:00,https://twitter.com/marktenenholtz/status/1535299137704542208,"@tunguz People keep telling me you should always start with an RF before a GBDT.

It’s really underestimated how good GBDT models with default hyper parameters are."
1574,@marktenenholtz,2022-06-10 16:02:50+00:00,https://twitter.com/marktenenholtz/status/1535291408231301121,"@WalterReade @__mharrison__ Man, a whole 2kb of RAM? Basically a supercomputer"
1575,@marktenenholtz,2022-06-10 15:58:22+00:00,https://twitter.com/marktenenholtz/status/1535290282773823488,"@__mharrison__ I learned through a mix of codeacademy, classes, and personal projects.

If I was starting over, I’d place a lot more emphasis on personal projects and reading others’ code (whether it be from a book or online)."
1576,@marktenenholtz,2022-06-10 15:48:35+00:00,https://twitter.com/marktenenholtz/status/1535287821828694016,"@ChristophMolnar I feel we as an ML community are really bad at naming things, but occasionally we get a 🔥title like that"
1577,@marktenenholtz,2022-06-10 13:15:29+00:00,https://twitter.com/marktenenholtz/status/1535249292218228737,@bhavik_m You got it
1578,@marktenenholtz,2022-06-10 12:23:06+00:00,https://twitter.com/marktenenholtz/status/1535236111014010881,@RobertSiegmund I prefer AWS 😂
1579,@marktenenholtz,2022-06-10 12:18:30+00:00,https://twitter.com/marktenenholtz/status/1535234952052318209,"@renegadesilicon I’m happy to give them my money if they make great products. What’s wrong with that?

They’ve gotta make money like everyone else."
1580,@marktenenholtz,2022-06-10 12:10:16+00:00,https://twitter.com/marktenenholtz/status/1535232878027628548,"@renegadesilicon If good product design and research is putting lipstick on a pig, then I’m going to a farm with a stick of Dior"
1581,@marktenenholtz,2022-06-10 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1535230309418995712,"Microsoft went from boring tech company to the company behind:

• The OS I'm running
• The IDE I'm using on that OS
• The code completion tool I'm using in that IDE
• The model I'm training with that code
• The place I'm storing that code

Talk about a transformation 🤯"
1582,@marktenenholtz,2022-06-10 00:20:02+00:00,https://twitter.com/marktenenholtz/status/1535054143760392192,@tunguz Go blue!
1583,@marktenenholtz,2022-06-09 23:12:27+00:00,https://twitter.com/marktenenholtz/status/1535037136231096320,"@svpino Not only that, but most people don't even want to learn everything. Not everything will capture your interest the same way.

No reason to burn yourself out learning what you don't want to."
1584,@marktenenholtz,2022-06-09 19:22:13+00:00,https://twitter.com/marktenenholtz/status/1534979193838223371,@ylecun Grabbing a drink while we watch AI twitter burn to the ground 😂
1585,@marktenenholtz,2022-06-09 17:40:11+00:00,https://twitter.com/marktenenholtz/status/1534953516783046656,"@tunguz @LBacaj My thoughts mirror yours almost entirely. 

I think hyperfocus on the end goal of AGI makes us miss all the benefits we can extract in the mean time.

I love the mission statement of @comma_ai: solve AI while delivering shippable intermediaries."
1586,@marktenenholtz,2022-06-09 12:07:33+00:00,https://twitter.com/marktenenholtz/status/1534869810282172416,"@TheZachMueller I know I thought the same thing for a bit, but when you see how folks recommend you prepare for interviews and whatnot…"
1587,@marktenenholtz,2022-06-09 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1534867918705418241,"Unpopular opinion: data scientists at top tech companies aren't experts with every deep learning architecture and framework. 

They're hired for their ability to learn them and implement them when required."
1588,@marktenenholtz,2022-06-08 23:44:21+00:00,https://twitter.com/marktenenholtz/status/1534682774782849024,"@TanweerMHasan1 @MeetRandomQuant Hey Shah! I've never been compromised (as far as I know), but just a few lines of code and you can set up a remote code execution attack. Check it out: https://t.co/lXnfajroih"
1589,@marktenenholtz,2022-06-08 17:00:05+00:00,https://twitter.com/marktenenholtz/status/1534581038017437696,Link here: https://t.co/VPaDKkJSZU
1590,@marktenenholtz,2022-06-08 17:00:04+00:00,https://twitter.com/marktenenholtz/status/1534581035391803393,"Just finished up a coaching call for The Autonomous Data Scientist.

It's thrilling to see someone go from no/little plan to a clear roadmap in such a short amount of time.

If you want to do the same, pick up the course!"
1591,@marktenenholtz,2022-06-08 13:40:21+00:00,https://twitter.com/marktenenholtz/status/1534530772748713984,"@Nicolascole77 @thejustinwelsh The way I think of it is that the algorithm is training me to appeal to a least common denominator, i.e. the worst audience to focus on"
1592,@marktenenholtz,2022-06-08 13:17:42+00:00,https://twitter.com/marktenenholtz/status/1534525075306729473,@MeetRandomQuant I don’t love pickles. They are a major security risk.
1593,@marktenenholtz,2022-06-08 13:08:57+00:00,https://twitter.com/marktenenholtz/status/1534522871053271041,@sjogren_rickard That’s what we hire those smart data engineers for!
1594,@marktenenholtz,2022-06-08 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1534505559818268673,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
1595,@marktenenholtz,2022-06-08 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1534505557238743040,"TL;DR:

Databases:
• Literally any SQL flavor
• KV DBs, like DynamoDB

File formats:
• Parquet
• Feather
• CSVs if all else fails

Follow me @marktenenholtz if you enjoyed this and want more high-signal ML content!"
1596,@marktenenholtz,2022-06-08 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1534505554655055872,"Note: both Parquet and Feather stored in columnar fashion.

This means that, while you can choose to skip reading in some columns, you can't skip rows when reading in the data.

If this matters to you, CSVs might actually be your best friend."
1597,@marktenenholtz,2022-06-08 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1534505552079773696,"Feather

Another Apache invention!

Feather is quite similar to Parquet. The major difference is that you trade off compression efficiency for data loading speed.

My rule of thumb:

• Use parquet when file size is the priority
• Use feather when reading speed is the priority"
1598,@marktenenholtz,2022-06-08 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1534505549584142336,"Apache Parquet

Parquet is a simple-to-use format that is deeply integrated with your favorite data tools.

Not only does it apply efficient compression to your data, but it preserves all of your data types and is quite fast to load into memory."
1599,@marktenenholtz,2022-06-08 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1534505546945945601,"These are great, but I would say most of my projects don't even need databases.

Data-specific file formats have come a long way and they're quite useful.

Here are my favorites:"
1600,@marktenenholtz,2022-06-08 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1534505544337108992,"Key-value DBs

These are tools like AWS's DynamoDB.

They're usually very cheap and give you a ton of flexibility.

The downside is that you lose the convenience of SQL functions.

When my data is mostly lookup tables and I haven't nailed down a tight schema, this is my go-to."
1601,@marktenenholtz,2022-06-08 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1534505541292044289,"Any SQL flavor

Okay, I know I said you don't always need SQL.

But, it's a pretty solid choice for many applications.

I usually use this when:

• My tech stack is pretty low-code
• I need to aggregate/join my data a lot
• I have highly structured data"
1602,@marktenenholtz,2022-06-08 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1534505538792218624,"I'm no data engineer, but I usually think about data being stored in 2 ways:

1. Databases
2. Fancy file formats

Let's start with databases:"
1603,@marktenenholtz,2022-06-08 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1534505536191795201,"If you're storing your data in CSV's, it's time for an upgrade.

You don't always need a SQL database, though.

Here are my favorite formats for data storage (and why you should use them):"
1604,@marktenenholtz,2022-06-07 15:15:28+00:00,https://twitter.com/marktenenholtz/status/1534192322270863360,Update: @TheZachMueller may have just converted me to Team @fastdotai
1605,@marktenenholtz,2022-06-07 14:13:51+00:00,https://twitter.com/marktenenholtz/status/1534176817543843840,@TheZachMueller @huggingface @rwightman @PyTorchLightnin I’m using the HF trainer for the Patent Phrase matching Kaggle competition right now. I’ll be sure to post my thoughts after it’s over (spoiler: it’s awesome)
1606,@marktenenholtz,2022-06-07 14:12:56+00:00,https://twitter.com/marktenenholtz/status/1534176584516632576,"@TheZachMueller @huggingface @rwightman @PyTorchLightnin Wonderful news. I’ve been craving something with the simplicity of fastai’s high level API but looks like lightning when you get into the mid level.

And I’m almost morally opposed to using Lightning Flash for… reasons… lol"
1607,@marktenenholtz,2022-06-07 14:04:06+00:00,https://twitter.com/marktenenholtz/status/1534174364601556993,"@TheZachMueller @huggingface @rwightman @PyTorchLightnin I want to love and use fastai but every time I try to learn it, it just doesn't jive with my brain. I don't know why... maybe it's a style thing?"
1608,@marktenenholtz,2022-06-07 14:01:41+00:00,https://twitter.com/marktenenholtz/status/1534173753353043970,@TheZachMueller @huggingface @rwightman @PyTorchLightnin I have not! That's a great feature -- there's something similar in pytorch lightning but I don't use it because its implementation confuses me lol
1609,@marktenenholtz,2022-06-07 14:00:42+00:00,https://twitter.com/marktenenholtz/status/1534173506249928706,@ludwig_stumpp @huggingface @rwightman @PyTorchLightnin That's another one I forgot. Definitely @weights_biases
1610,@marktenenholtz,2022-06-07 12:18:41+00:00,https://twitter.com/marktenenholtz/status/1534147835255476225,@TheZachMueller @huggingface @rwightman @PyTorchLightnin Recently started using the HF Trainer and it’s amazing
1611,@marktenenholtz,2022-06-07 12:04:41+00:00,https://twitter.com/marktenenholtz/status/1534144311805284353,"And how could I forget:

8. scikit-learn (RFs and data splitting)
9. cuDF and cuML"
1612,@marktenenholtz,2022-06-07 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1534143142726754306,"My machine learning (software) toolbelt:

1. JupyterLab (experimentation)
2. VSCode (everything else)
3. @huggingface (pretrained NLP)
4. @rwightman's timm (vision)
5. Hugging Face &amp; Kaggle datasets
6. XGBoost/LightGBM (tabular)
7. @PyTorchLightnin (deep learning)

What's yours?"
1613,@marktenenholtz,2022-06-06 19:55:42+00:00,https://twitter.com/marktenenholtz/status/1533900458103910401,@JannicHorst The trouble is that including that material nearly doubles the size of the course. Makes it hard for me to focus on the stuff that isn’t already well-covered
1614,@marktenenholtz,2022-06-06 18:41:11+00:00,https://twitter.com/marktenenholtz/status/1533881706574491655,"@GergelyOrosz Automotive industry is horrendous about this. When I was a mechanical engineering student at a school that feeds into the large automakers, I heard about employees that wouldn't even physically see their manager in the office for months on end."
1615,@marktenenholtz,2022-06-06 17:43:09+00:00,https://twitter.com/marktenenholtz/status/1533867102142070784,"@apothauth So far, all data is coming from Kaggle competitions that I feel represent real-world data from a diverse set of industries.

That being said, I plan on including a data scraping tutorial to show folks how to build their own datasets."
1616,@marktenenholtz,2022-06-06 17:32:15+00:00,https://twitter.com/marktenenholtz/status/1533864358631153666,"@prthgo I've always looked at it as ""no news is good news"" lmao"
1617,@marktenenholtz,2022-06-06 17:30:24+00:00,https://twitter.com/marktenenholtz/status/1533863892035719170,"In addition: what don't you want to see?

For example: should I even include a ""building NNs from scratch"" type of section, or do you feel that's too overdone?

Would you feel cheated if a course referenced you to great (free) beginner materials instead of doing it themselves?"
1618,@marktenenholtz,2022-06-06 17:29:13+00:00,https://twitter.com/marktenenholtz/status/1533863595582300165,"@apothauth I built out some materials already on model evaluation and I'm incredibly excited to share it.

Probably the most valuable content I've ever built, and it'll definitely be in the course."
1619,@marktenenholtz,2022-06-06 17:12:20+00:00,https://twitter.com/marktenenholtz/status/1533859344567123968,"@thejustinwelsh 100%

Algorithms work on repeatable and automatable problems.

Humans work on unsolved, complex, non-deterministic problems.

Unsolved problems are where the $$$ are."
1620,@marktenenholtz,2022-06-06 17:00:11+00:00,https://twitter.com/marktenenholtz/status/1533856287829610496,"I'm working on an applied ML masterclass.

The goal is to teach you how to approach problems for tabular, vision, and text data and take them through to production.

Everything from the state-of-the-art models to the best modern MLOps tools.

What do you want to see in it?"
1621,@marktenenholtz,2022-06-06 16:06:43+00:00,https://twitter.com/marktenenholtz/status/1533842831831404544,"@fchollet Crazy to think of the impact Kaggle has made on the world of ML.

Not just with popularizing approaches, but also building much stronger data scientists up."
1622,@marktenenholtz,2022-06-06 11:47:31+00:00,https://twitter.com/marktenenholtz/status/1533777602019741696,"@VaclavPechtor Generally an hour because it’s usually all we need, but can run over if I feel like it should."
1623,@marktenenholtz,2022-06-05 20:32:30+00:00,https://twitter.com/marktenenholtz/status/1533547329994862593,@AlejandroPiad It’s even worse when you see that the ambitious web3 builders that are doing incredible work are inextricably linked to the loony speculators
1624,@marktenenholtz,2022-06-05 20:28:15+00:00,https://twitter.com/marktenenholtz/status/1533546263110684673,@osanseviero I just opened up Untitled2.ipynb and thought that was bad 😂
1625,@marktenenholtz,2022-06-05 14:46:18+00:00,https://twitter.com/marktenenholtz/status/1533460206289571841,@al_byzov Equations are too abstract. Code is very concrete.
1626,@marktenenholtz,2022-06-05 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1533418366475677697,I've learned so much more math from reading code than I have from whiteboards.
1627,@marktenenholtz,2022-06-04 18:49:59+00:00,https://twitter.com/marktenenholtz/status/1533159145964294144,@nafiz_h Yep
1628,@marktenenholtz,2022-06-04 18:19:26+00:00,https://twitter.com/marktenenholtz/status/1533151456873922560,@ArnabArnabi This course isn't tool-specific. It teaches you the best way to build any data science-related skills
1629,@marktenenholtz,2022-06-04 18:04:38+00:00,https://twitter.com/marktenenholtz/status/1533147730960424960,If you want to learn the same way I did… https://t.co/VPaDKksi8m
1630,@marktenenholtz,2022-06-04 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1533055977901084672,"4 years ago, I would have been lucky to even successfully train a model.

Now, I set up projects and get a simple baseline out in less than a day.

Nothing is better for improvement than reps."
1631,@marktenenholtz,2022-06-04 01:51:53+00:00,https://twitter.com/marktenenholtz/status/1532902933532131334,@0xbnomial Lines up well with what I do!
1632,@marktenenholtz,2022-06-03 20:22:38+00:00,https://twitter.com/marktenenholtz/status/1532820073894772742,@OpokuNKO @svpino Thanks!
1633,@marktenenholtz,2022-06-03 20:17:55+00:00,https://twitter.com/marktenenholtz/status/1532818887959617537,@OpokuNKO @svpino https://t.co/VPaDKksi8m
1634,@marktenenholtz,2022-06-03 14:37:57+00:00,https://twitter.com/marktenenholtz/status/1532733329279180800,@svpino It fits in nicely with what you talk about a lot!
1635,@marktenenholtz,2022-06-03 14:14:30+00:00,https://twitter.com/marktenenholtz/status/1532727427956453376,"@svpino Yup, this is built heavily into the course I just released.

The learning process starts with hands-on."
1636,@marktenenholtz,2022-06-03 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1532693621509455872,"Most data scientists scoff at ideas that are based on human intuition. 

This is arrogance at its finest. Don't dismiss intuition.

Intuition tells you what to look into.

Data science tells you if it's worth believing."
1637,@marktenenholtz,2022-06-02 15:17:08+00:00,https://twitter.com/marktenenholtz/status/1532380803870638081,@jessehman Thanks for the support!
1638,@marktenenholtz,2022-06-02 13:47:20+00:00,https://twitter.com/marktenenholtz/status/1532358205875048448,"@LBacaj Ha! Yeah, last time I checked, they still have 24 hours in a day.

But it sounds hardcore, right?"
1639,@marktenenholtz,2022-06-02 13:44:50+00:00,https://twitter.com/marktenenholtz/status/1532357577266405376,@PrasoonPratham Thanks Pratham! It seems to have worked out for me at least haha
1640,@marktenenholtz,2022-06-02 12:46:36+00:00,https://twitter.com/marktenenholtz/status/1532342920153530370,"@marco_gorelli Definitely. If you learn something you’re not going to use, you’re not going to remember it anyways!"
1641,@marktenenholtz,2022-06-02 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1532331202434043904,"I see far too many data scientists trying to learn everything in the ocean of ML knowledge and burning out.

This is a huge mistake.

1. Build a foundation
2. Find what you love
3. Go all-in
4. Find the next thing and repeat

Solve the tough problems that excite you."
1642,@marktenenholtz,2022-06-02 01:53:15+00:00,https://twitter.com/marktenenholtz/status/1532178501649973248,@HamelHusain @TheZachMueller @bobbypinero I think https://t.co/3hJsuBwOj5 by @LBacaj is perfect for sharing drafts of threads and getting feedback on them
1643,@marktenenholtz,2022-06-01 18:45:35+00:00,https://twitter.com/marktenenholtz/status/1532070874685186049,@tunguz @LBacaj Yeah there’s nothing to keep us busy while our models train so we just need to buy more GPUs to make sure we don’t wait for very long
1644,@marktenenholtz,2022-06-01 18:40:06+00:00,https://twitter.com/marktenenholtz/status/1532069495249915904,"@VicVijayakumar @JonathanSumDL @LBacaj Well I live in Kentucky, so that’s technically the south, right? …right?"
1645,@marktenenholtz,2022-06-01 18:38:50+00:00,https://twitter.com/marktenenholtz/status/1532069176743198720,@tunguz @LBacaj Go. Freaking. Hoosiers.
1646,@marktenenholtz,2022-06-01 18:36:34+00:00,https://twitter.com/marktenenholtz/status/1532068603381821440,@JonathanSumDL @LBacaj Cost of living is way lower. With even a pretty bad budget you can live really well on $250k in the Midwest
1647,@marktenenholtz,2022-06-01 18:28:42+00:00,https://twitter.com/marktenenholtz/status/1532066626748567553,@LBacaj This should be advertising material for states in the Midwest lmao
1648,@marktenenholtz,2022-06-01 13:36:52+00:00,https://twitter.com/marktenenholtz/status/1531993182573146113,"@andrehaykaljr I'm on a quest to 50k by the end of the year.

Join me and we can pop a bottle together when we hit it 👊"
1649,@marktenenholtz,2022-06-01 12:40:34+00:00,https://twitter.com/marktenenholtz/status/1531979015803834369,@e8aeca30898d4d8 Not sure I would go that far. They're very smart people.
1650,@marktenenholtz,2022-06-01 12:22:06+00:00,https://twitter.com/marktenenholtz/status/1531974366459744256,@svpino Thanks Santiago 🙏
1651,@marktenenholtz,2022-06-01 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1531968860798132225,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
1652,@marktenenholtz,2022-06-01 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1531968858243813376,"TL;DR:

1. Don't frontload the theory
2. Value comes first
3. Unused models are worthless
4. Model evaluation &gt;&gt;&gt; model building
5. Learn to reinvent yourself

I WISH someone told me this when I started learning ML.

Follow me @marktenenholtz for more high-signal ML content!"
1653,@marktenenholtz,2022-06-01 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1531968855651721216,"Learn how to re-learn the field

Your college courses are out-of-date because the field moves so quickly.

Want to stay relevant in industry? 

Commit yourself to continual learning.

Seek out new models, MLOps tools, libraries, etc., and always be one step ahead."
1654,@marktenenholtz,2022-06-01 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1531968853000871937,"Model evaluation &gt;&gt;&gt; model building

How do you expect to explain your model to a user if you can't tell how good it is?

If you can't build a reliable evaluation framework and communicate your model's performance, your model can't be relied upon."
1655,@marktenenholtz,2022-06-01 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1531968850475905025,"Your model is worthless if nobody uses it

If you can't communicate the value of your model, nobody will use it.

If nobody uses your model, then its development was an expensive waste of time.

Stakeholder management is crucial. Learning a bit of sales goes a long way."
1656,@marktenenholtz,2022-06-01 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1531968847892213760,"ML is only as useful as it creates value

Nobody cares about your fancy model for a problem that could be solved by an if-statement. 

(It's actually a bad thing)

Data scientists are hired for their problem-solving creativity first, and the ability to use ML to do it second."
1657,@marktenenholtz,2022-06-01 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1531968845363093506,"Learn the theory as needed, not all at once

It's a BAD idea to try to calculate the derivative of a convolution before you've applied them to a few problems.

Data science is an inherently applied field, so learn it that way.

Learn enough theory to get started, and then start!"
1658,@marktenenholtz,2022-06-01 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1531968842791915520,"A note:

Machine learning is a very new field.

Just as we're still learning new ML methods, we're still learning how to teach it, too.

We need these programs to be strong if we want to cultivate strong data scientists, and these are some of the major problems I see out there."
1659,@marktenenholtz,2022-06-01 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1531968840027975681,"College machine learning programs are FULL of holes. 

Static courses are rarely up-to-date in a field moving this fast.

Plus, they tend to skip over a ton of practical topics that are common in industry. 

Here 5 things everyone in industry WISHES your professor taught you:"
1660,@marktenenholtz,2022-05-31 22:36:09+00:00,https://twitter.com/marktenenholtz/status/1531766509378363394,@fritz_sluzala Really appreciate it! A 5 star review would make my day 😊
1661,@marktenenholtz,2022-05-31 13:55:03+00:00,https://twitter.com/marktenenholtz/status/1531635372110893056,"@__mharrison__ The real secret is using those books and courses as reference material to learn something. 

Then you're guided through the important stuff to know and you have an applied method of getting a feel for it."
1662,@marktenenholtz,2022-05-31 13:20:52+00:00,https://twitter.com/marktenenholtz/status/1531626767445786626,"@LMonarchist It's 100% for you. It's course about how to learn, build, and stand out, so you'll learn the basics much more effectively if you take it!"
1663,@marktenenholtz,2022-05-31 13:07:00+00:00,https://twitter.com/marktenenholtz/status/1531623278858715136,@thedankoe It’s also (unquestionably) the best way to learn how to do something
1664,@marktenenholtz,2022-05-31 12:55:39+00:00,https://twitter.com/marktenenholtz/status/1531620421568806912,@LBacaj Thanks man! Couldn’t have done it without you.
1665,@marktenenholtz,2022-05-31 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1531606448483889159,Link to the course: https://t.co/VPaDKkJSZU
1666,@marktenenholtz,2022-05-31 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1531606445363302401,"""The Autonomous Data Scientist"" is LIVE!

Get it for $15 and you'll learn how to make $150k+ to work on some of the most exciting problems out there in data science.

All for less than you'll pay for coffee this month.

This price is only good until Thursday (it's going up)! https://t.co/8nHZXvMPYB"
1667,@marktenenholtz,2022-05-31 00:38:07+00:00,https://twitter.com/marktenenholtz/status/1531434817342013447,@tvykruta Love it Tom! Looks like a blast
1668,@marktenenholtz,2022-05-30 22:42:07+00:00,https://twitter.com/marktenenholtz/status/1531405623207804928,@LBacaj Smoked a 10 lb brisket today! Hope you love your Memorial Day!
1669,@marktenenholtz,2022-05-30 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1531244079421435905,"@GergelyOrosz @david_perell Tomorrow, I'm releasing a course.

“The Autonomous Data Scientist"" 

It’ll teach you how to crush these problems.

On top of that, I'll teach you a method I've perfected over the years called ""The Infinite Wheel of Learning"" which builds your skills with incredible momentum."
1670,@marktenenholtz,2022-05-30 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1531244076808294401,"@GergelyOrosz @david_perell The second reason: your reach

Build an unimpressive resume without building anything, and you get as many eyeballs as resumes you send in.

Build something exciting and share it with the world, and you may get millions of impressions per month.

Which do you think is better?"
1671,@marktenenholtz,2022-05-30 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1531244074165886976,"@GergelyOrosz @david_perell If you can't write about complex ML topics in a jargon-free manner, you don't understand them.

On top of that, ML topics are highly interconnected. Writing about a project you've already completed gives you incredible ""a-ha!"" moments you'd overlooked and reveals deeper meaning."
1672,@marktenenholtz,2022-05-30 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1531244071083094016,"@GergelyOrosz You aren't writing about it

There are two reasons writing as you learn is important.

First: it's the only way to truly understand what you're learning.

This image from @david_perell is the perfect illustration. https://t.co/hZfpqSJrnI"
1673,@marktenenholtz,2022-05-30 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1531244062086336512,"@GergelyOrosz The best part?

You can start with the building, and learn the theory that you need to do the building as you go.

No need to spend 3 years learning the theory before you build anything. That's nonsense."
1674,@marktenenholtz,2022-05-30 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1531244059397746688,"@GergelyOrosz Courses and books are incredibly useful as reference material along your journey.

But, they are not your whole journey!

Building projects and useful outcomes (even at small scale) is the only way to truly learn how those interactions work and build deep knowledge of the craft."
1675,@marktenenholtz,2022-05-30 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1531244056788930562,"@GergelyOrosz The rest of the 95%:

• Solution engineering
• Stakeholder management
• Data collection
• Data engineering
• Model deployment/monitoring
etc.

To fully appreciate how that math-y parts fit together and interact with the engineer-y parts, you MUST build projects."
1676,@marktenenholtz,2022-05-30 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1531244054209392640,"@GergelyOrosz You're over-reliant on courses and books

Data science is an inherently applied field.

It's built on the foundations of statistics, and you most certainly need to learn that.

When data science is applied to a problem, though, *maybe* 5% of it is reliant on that knowledge."
1677,@marktenenholtz,2022-05-30 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1531244051365695488,"This graph is an incredible illustration of this.

This is made for software engineers by @GergelyOrosz, but it holds equally true for data scientists.

Only the folks with unique, specialized skillsets are appealing to companies in 3rd bucket. They've differentiated themselves. https://t.co/9Yzey1vcIt"
1678,@marktenenholtz,2022-05-30 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1531244043685900288,"Not only this, but not everything in the field appeals to everyone... so don't force it.

Data scientists who find their passion and get really good at it are 3-5x more valuable that those who try (and fail) to generalize and be an expert at everything."
1679,@marktenenholtz,2022-05-30 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1531244041039335427,"You aren't specializing

Data scientists need to have a wide variety of skills to be successful.

But, the ""Unicorn"" doesn't exist. If you want to become an expert, you can't do it by generalizing.

Valuable data scientists are capable of many skills, but are experts in a few."
1680,@marktenenholtz,2022-05-30 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1531244038422028288,"It's really hard to learn machine learning.

Do it poorly, and you get lost in math and never find your way out.

Do it well, and you make $150k+ to work on some of the most exciting problems out there.

Here are the 3 things in your way (even if you're not a beginner):"
1681,@marktenenholtz,2022-05-29 20:50:55+00:00,https://twitter.com/marktenenholtz/status/1531015250786537473,"@svpino One time a guy said some nasty shit in a quote RT to me. I replied and he said “oh oops sorry I didn’t think you’d see this. I actually like your tweets 😅”

Like, wtf???"
1682,@marktenenholtz,2022-05-29 20:50:02+00:00,https://twitter.com/marktenenholtz/status/1531015030107426816,"@svpino On one hand it’s good because it accidentally filters out a lot of toxicity.

On the other hand, I’ve found ppl will use quote RTs because they know they’re harder to see, so they can be more toxic with impunity."
1683,@marktenenholtz,2022-05-29 20:42:37+00:00,https://twitter.com/marktenenholtz/status/1531013162501275648,"@svpino 100%. Not only are so many of the quote RTs toxic themselves, but so many folks will pile on without even reading the damn tweet + any follow ups/clarifications from the original tweet"
1684,@marktenenholtz,2022-05-29 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1530881661457399808,You can check it out here: https://t.co/TVgZjEOaHO
1685,@marktenenholtz,2022-05-29 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1530881657950961665,"The best way to grow your ML skills is project-based learning.

If you can't scrape your own data, you'll need interesting datasets.

Google Research has released 110 datasets spanning text to images to time series data.

Want to try a new method? Use one of these as a basis! https://t.co/reSbH2Wqw4"
1686,@marktenenholtz,2022-05-28 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1530519264997744641,"The most valuable data scientists are:

• Specialists
• Builders
• Great communicators.

They build products few others can to solve hard, uncertain problems.

All while making it comprehensible for their stakeholders."
1687,@marktenenholtz,2022-05-27 17:06:01+00:00,https://twitter.com/marktenenholtz/status/1530233879826776064,"@thejustinwelsh Been very curious about private communities. Seems like a lot of work for much less potential revenue than a high-ticket course. 

Can’t wait to see what you have to say!"
1688,@marktenenholtz,2022-05-27 14:13:29+00:00,https://twitter.com/marktenenholtz/status/1530190457736638465,"@MereSophistry Yes, but data science is an extreme example. It’s a newer subject and entirely new fields within it are popping up fairly regularly."
1689,@marktenenholtz,2022-05-27 12:56:49+00:00,https://twitter.com/marktenenholtz/status/1530171163267239936,@nivasgopi30 Yes!
1690,@marktenenholtz,2022-05-27 12:56:40+00:00,https://twitter.com/marktenenholtz/status/1530171128265703426,@MuhamedAbuJabal It’ll be out on Tuesday!
1691,@marktenenholtz,2022-05-27 12:28:36+00:00,https://twitter.com/marktenenholtz/status/1530164065443667968,"@svpino This is why I always recommend people learn to scrape their own data for some of their personal projects.

It’s the only way to prepare you for this difficulty."
1692,@marktenenholtz,2022-05-27 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1530156890679672832,"If you want to be an effective data scientist, you need to learn how to learn.

The field is expanding exponentially. 

If you can't learn efficiently, you'll drown.

But, perfect your process, and you'll thrive

(My course will show you exactly how to do that)"
1693,@marktenenholtz,2022-05-27 01:57:51+00:00,https://twitter.com/marktenenholtz/status/1530005331132153872,@Splittestingcom Carvana 😱
1694,@marktenenholtz,2022-05-26 23:55:44+00:00,https://twitter.com/marktenenholtz/status/1529974598858690573,@svpino Thanks Santiago!
1695,@marktenenholtz,2022-05-26 12:32:25+00:00,https://twitter.com/marktenenholtz/status/1529802635523637256,@prthgo Thanks Parth!
1696,@marktenenholtz,2022-05-26 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1529794496631738368,"If you want to know more about it, I just gave a more detailed sneak preview in my newsletter.

Get a free preview:
https://t.co/34sBfoLpMx"
1697,@marktenenholtz,2022-05-26 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1529794493855113216,"If you want to 10x your data science skills (whether you're junior or senior+ level), building must be central to you.

My new course will teach you the Infinite Wheel of Learning.

A.k.a., the most efficient path to a $150k+ salary working on the most exciting problems."
1698,@marktenenholtz,2022-05-25 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1529432108703264771,"Sign up here:
https://t.co/34sBfoLpMx"
1699,@marktenenholtz,2022-05-25 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1529432105666482176,"HUGE announcement tomorrow.

I’m launching a course!

You’re going to learn how to navigate this extremely crowded field like a pro, to work on the most exciting problems, and get a $150k+ salary doing it.

I’m sending out a sneak preview on my newsletter tomorrow!"
1700,@marktenenholtz,2022-05-24 23:58:14+00:00,https://twitter.com/marktenenholtz/status/1529250452088528896,@AlejandroPiad I play softball with a bunch of lawyers. It’s a similar powder keg 😂
1701,@marktenenholtz,2022-05-24 22:36:15+00:00,https://twitter.com/marktenenholtz/status/1529229820080664576,@xtinacomputes You’ve gone too far
1702,@marktenenholtz,2022-05-24 19:04:16+00:00,https://twitter.com/marktenenholtz/status/1529176473491341312,"@jrnld Marrying what you'll learn from a Ph.D. program with the context of the practical problems faced in industry makes for a formidable data scientist.

This sort of ""applied research"" path makes for some of the most incredible data scientists."
1703,@marktenenholtz,2022-05-24 16:04:15+00:00,https://twitter.com/marktenenholtz/status/1529131168934899712,@giffmana Not sure why he thinks citations are the issue. I think the issue is that most folks are bad at properly motivating complex ideas. I think @jeremyphoward’s book is a great example of how you can do both.
1704,@marktenenholtz,2022-05-24 16:03:59+00:00,https://twitter.com/marktenenholtz/status/1529131104455823361,"@AlejandroPiad My shot was maybe a bit too hot, though. Academics already have an itchy trigger finger and I try to be pretty positive on here 😜"
1705,@marktenenholtz,2022-05-24 16:01:37+00:00,https://twitter.com/marktenenholtz/status/1529130508965318658,"@AlejandroPiad Yeah, I think PhD's do incredible work that I use every day. 

Some of the disagreement is fair, but most of it misses the point, which is that the main problems PhD's face is very, very different than the problems folks in industry face (and that's not necessarily bad)."
1706,@marktenenholtz,2022-05-24 15:09:33+00:00,https://twitter.com/marktenenholtz/status/1529117404248387584,"It's great that some/many Ph.D. programs have you collecting your own data, running experiments on those datasets, and communicating the results. It's incredibly valuable work.

But the problems folks in industry face are pernicious in quite different ways."
1707,@marktenenholtz,2022-05-24 15:02:44+00:00,https://twitter.com/marktenenholtz/status/1529115690220797952,@radekosmulski @kaggle I love your public passion for a topic most don't get to work on (even if you get paid to talk about it 😜). Keep it up!
1708,@marktenenholtz,2022-05-24 14:57:33+00:00,https://twitter.com/marktenenholtz/status/1529114383867322368,"@JFPuget I didn't say that Ph.D.'s don't collect data. I said that the process will teach anyone these things, regardless of whether you're a Ph.D. or not.

I hope some Ph.D.'s do, because they'll learn plenty of things that their peers won't."
1709,@marktenenholtz,2022-05-24 13:50:06+00:00,https://twitter.com/marktenenholtz/status/1529097411964153859,@adyashunya No shots taken. They’re simply different worlds.
1710,@marktenenholtz,2022-05-24 12:55:59+00:00,https://twitter.com/marktenenholtz/status/1529083790177587201,"@SeguraAndres7 Maybe, but academia suffers from a huge problem. The benchmark datasets that are used to prove most high-impact research are becoming increasingly detached from reality.

I appreciate applied research, but most of that work doesn’t get published."
1711,@marktenenholtz,2022-05-24 12:41:36+00:00,https://twitter.com/marktenenholtz/status/1529080173034934272,"@SeguraAndres7 You’re right, and Ph.D.’s are valuable. You need that deep understanding to contribute new methods.

But they simply don’t teach you many problems you run into in the real world."
1712,@marktenenholtz,2022-05-24 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1529069716160978944,"If you build models from data you collected yourself, you'll learn things a Ph.D. won't teach you.

Knowing how to derive the math behind an SVM won't save you when there's sneaky target leakage in your dataset."
1713,@marktenenholtz,2022-05-23 12:48:08+00:00,https://twitter.com/marktenenholtz/status/1528719426765348864,@SirBaum Course!
1714,@marktenenholtz,2022-05-23 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1528707325095137280,"The secrets to becoming an incredible data scientist?

Specializing and building.

Gain an appreciation for the whole process while becoming an expert in a specific part.

Want to learn how to do it? I’ve got something coming your way this week 👀"
1715,@marktenenholtz,2022-05-22 13:17:34+00:00,https://twitter.com/marktenenholtz/status/1528364448326492160,"@svpino Great point. 

I always tell learners that they should scrape their own data for a lot of their projects. Really forces you to reckon with this problem."
1716,@marktenenholtz,2022-05-22 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1528344937762238465,What's one machine learning topic that you wish was taught better in courses/books?
1717,@marktenenholtz,2022-05-21 14:36:03+00:00,https://twitter.com/marktenenholtz/status/1528021809697792003,"@AlejandroPiad Exactly. At the end of the day, your model has to do something in the real world, so it should be evaluated as such."
1718,@marktenenholtz,2022-05-21 14:35:07+00:00,https://twitter.com/marktenenholtz/status/1528021574057701376,@sGx_tweets Thanks Srishti! Glad you enjoyed it!
1719,@marktenenholtz,2022-05-21 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1527982557706366978,It's amazing (and scary) how many data scientists come out of college thinking model evaluation == simple random cross-validation
1720,@marktenenholtz,2022-05-20 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1527620163167604737,"Too many data scientists force fancy solutions onto problems.

The ideal solution is the simplest one that gets the job done effectively."
1721,@marktenenholtz,2022-05-19 23:29:56+00:00,https://twitter.com/marktenenholtz/status/1527431390169645064,@svpino Likewise! You’re an inspiration
1722,@marktenenholtz,2022-05-19 16:35:09+00:00,https://twitter.com/marktenenholtz/status/1527327009114038272,@rohanpaul_ai Thanks Rohan!
1723,@marktenenholtz,2022-05-19 16:10:12+00:00,https://twitter.com/marktenenholtz/status/1527320728630009859,@MeganRisdal This is an amazing list! Kaggle is a treasure trove of knowledge.
1724,@marktenenholtz,2022-05-19 15:55:38+00:00,https://twitter.com/marktenenholtz/status/1527317064184000512,@prthgo Thanks Parth!
1725,@marktenenholtz,2022-05-19 13:21:51+00:00,https://twitter.com/marktenenholtz/status/1527278361357127680,@LBacaj We’ll be on top together too!
1726,@marktenenholtz,2022-05-19 13:21:32+00:00,https://twitter.com/marktenenholtz/status/1527278282890063872,@aakashg0 Thanks Aakash!
1727,@marktenenholtz,2022-05-19 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1527257817459007488,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
1728,@marktenenholtz,2022-05-19 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1527257814648774661,"To sum it up:

1. Consistency is king
2. We need more techies
3. Creating = learning
4. Up your luck surface area
5. Writing is a blessing
6. Don't network. Tweet.

If you enjoyed this, drop me a follow @marktenenholtz for more of this and even more high-signal ML content"
1729,@marktenenholtz,2022-05-19 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1527257811826012160,"The best and brightest are here

I've conversed with folks ranging from celebrities to the data scientists pushing applied ML beyond the cutting edge just because of my Twitter account.

Screw going to conferences to network.

Start tweeting."
1730,@marktenenholtz,2022-05-19 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1527257809028456448,"Writing is a skill for everybody

Communication skills are at a premium, especially in tech.

Those who can learn to write (copywrite, even) are at a supreme advantage.

Whether you're selling a product or convincing a PM your forecast is solid, good writing will save your tail."
1731,@marktenenholtz,2022-05-19 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1527257806302089217,"Your luck surface area is crucial

When I first entered data science, I prayed 1 of the dozens of places I applied would have me.

Now, I get so many offers I can't reply to them all.

My only exposure then was a resume in the middle of a pile.

Now, it's 5M impressions/month."
1732,@marktenenholtz,2022-05-19 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1527257803546427392,"Creating is the key to learning

One of the most important stages of learning is teaching.

For me, writing on Twitter has partially served that purpose.

Teaching and creating are two of the easiest ways to find gaps in your knowledge, and it's crazy how much it helps."
1733,@marktenenholtz,2022-05-19 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1527257800690216960,"Technical creators are in short supply

There are a million creators in marketing, sales, fitness, etc.

We techies are vastly underrepresented, though.

There are (proportionally) far more consumers of technical content than there are consistent, quality creators."
1734,@marktenenholtz,2022-05-19 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1527257797317959682,"Consistency is king

Sure, it's probably good for the algorithm to tweet every day.

But the real benefit of consistency is exponential growth of both your writing skill and the audience you reach. https://t.co/X6TkjpnjOt"
1735,@marktenenholtz,2022-05-19 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1527257788899917824,"5 months ago, I dusted off my dormant Twitter account with 101 followers and decided to start tweeting.

1 month in, I had 500 followers.

Yesterday, I just hit 30k, and tweeting has given me more opportunity than I could have imagined.

Here's what I wish I knew when I started: https://t.co/NSWXQfNjHC"
1736,@marktenenholtz,2022-05-18 20:27:29+00:00,https://twitter.com/marktenenholtz/status/1527023089669767173,@mlops2themoon I just mean creating a project structure that you know inside and out.
1737,@marktenenholtz,2022-05-18 19:23:48+00:00,https://twitter.com/marktenenholtz/status/1527007060679970816,@xtinacomputes That’s what Kaggle is
1738,@marktenenholtz,2022-05-18 18:00:36+00:00,https://twitter.com/marktenenholtz/status/1526986121804120073,@RAPIDSai Thanks for letting my old 2070 do all the work!
1739,@marktenenholtz,2022-05-18 12:24:49+00:00,https://twitter.com/marktenenholtz/status/1526901620944130050,@iamsteph Thank you Stephanie!
1740,@marktenenholtz,2022-05-18 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1526895416918220800,"@RAPIDSai Pipelines &gt; models

Most of the code that goes into training ML models is written either for getting the data to the model or getting the predictions out.

Want fast and reliable models? Spend more time improving your pipelines."
1741,@marktenenholtz,2022-05-18 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1526895414107963393,"@RAPIDSai Refactor, refactor, refactor

ML code can get out of hand really quickly.

With rapid iteration comes a rapid mess.

Don't be afraid to spend 30 minutes cleaning up your code instead of training models."
1742,@marktenenholtz,2022-05-18 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1526895411281002496,"@RAPIDSai Create your own framework

99% of my projects follow the exact same structure.

Down to the names of the files, what goes in them, and when they are created.

It's probably not the most efficient setup, but having such a deep knowledge of the structure makes me work much faster."
1743,@marktenenholtz,2022-05-18 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1526895408412172289,"@RAPIDSai Reuse old Kaggle solutions

Kaggle solutions are like a database for battle-tested solutions.

If you have a similar enough problem to an old competition, those solution writeups are an absolute goldmine.

My first stop on any problem for ""literature review"" is Kaggle."
1744,@marktenenholtz,2022-05-18 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1526895405601935366,"@RAPIDSai Pick 1 model and max it out

Your first goal should be to take one of those 3-5 models and improve it until you can't anymore.

On image tasks, I usually start by improving efficientnet-b0 until I can't anymore.

On tabular tasks, it's usually LightGBM or a random forest."
1745,@marktenenholtz,2022-05-18 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1526895402783363077,"@RAPIDSai Skip to the good models

For any given problem, there are 3-5 model architectures you should focus (unless you have a really good reason).

The less time you're playing with models, the more time you're spending on high-leverage tasks (feature engineering, data exploration)."
1746,@marktenenholtz,2022-05-18 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1526895400036184069,"Use a GPU wherever you can

@RAPIDSai is an absolute blessing.

For 95% of my tabular problems, I can change 2 lines of code to make the whole thing run on my GPU. 

Everything from feature engineering to model training.

I've seen instances where this speeds pipelines up by 50x."
1747,@marktenenholtz,2022-05-18 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1526895397301432320,"Model evaluation is the best time saver

Experiments done with a bad evaluation strategy are a waste of time.

Conversely, a good evaluation setup is the most powerful tool for iterating to a good solution.

This is arguably the highest-leverage time investment in an ML project."
1748,@marktenenholtz,2022-05-18 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1526895394440896517,"I've spent 1000's of hours building ML models over the last couple of years.

Here are some tips that would have made me work 10x faster (that you can read in 2 minutes):"
1749,@marktenenholtz,2022-05-17 15:24:24+00:00,https://twitter.com/marktenenholtz/status/1526584428406915072,@LBacaj Yep. That intuition is the key towards building a truly robust understanding of your field.
1750,@marktenenholtz,2022-05-17 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1526533004830470145,"Powerful rule of thumb:

The best thing to learn next is what tripped you up on your last project."
1751,@marktenenholtz,2022-05-16 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1526170620089708544,"The best ensemble isn’t between multiple models. 

It’s the ensemble of two brilliant data scientists."
1752,@marktenenholtz,2022-05-15 14:40:35+00:00,https://twitter.com/marktenenholtz/status/1525848623291957248,@parker_brydon @HermanMiller Already have one!
1753,@marktenenholtz,2022-05-15 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1525808228646256641,"Data scientists: ""My back hurts so bad""

Also data scientists, while their models train: https://t.co/OzCENxzXp0"
1754,@marktenenholtz,2022-05-14 15:59:43+00:00,https://twitter.com/marktenenholtz/status/1525506150464163842,@Splittestingcom One of the easiest follows on twitter dot com. Keep showing the world how it’s done!
1755,@marktenenholtz,2022-05-14 15:27:35+00:00,https://twitter.com/marktenenholtz/status/1525498062856134656,@nedbat @HamelHusain By far the most practical use of map() I think I’ve ever seen. Thanks for sharing!
1756,@marktenenholtz,2022-05-14 15:23:35+00:00,https://twitter.com/marktenenholtz/status/1525497056365772800,"@AlejandroPiad @jessehman Great point! But hey, might as well try it and see what your cross-validation tells you!"
1757,@marktenenholtz,2022-05-14 15:20:34+00:00,https://twitter.com/marktenenholtz/status/1525496299222716416,"@AlejandroPiad @jessehman It can work and it’s probably faster than an RNN, but it really just depends on how much autocorrelation is at play. I’m sure it would help for problems with high autocorrelation, but probably not for highly seasonal problems like demand forecasting."
1758,@marktenenholtz,2022-05-14 14:58:42+00:00,https://twitter.com/marktenenholtz/status/1525490794718707713,"@eulerdomy @rasbt @ChrSzegedy It’s like a random crop but instead of a fixed size, you crop out a random size and then resize it"
1759,@marktenenholtz,2022-05-14 14:49:09+00:00,https://twitter.com/marktenenholtz/status/1525488393957822464,"@jessehman @AlejandroPiad Well really what happens is you encode those things as features, i.e. you feed your model day of week, month, etc.

Then your model learns how to factor those in. So yeah I guess you’re basically right in a sense, but it just needs a little feature engineering."
1760,@marktenenholtz,2022-05-14 14:39:06+00:00,https://twitter.com/marktenenholtz/status/1525485862842146817,@rasbt Definitely! I only mention Inception because I actually have some success using it while transfer learning
1761,@marktenenholtz,2022-05-14 14:33:09+00:00,https://twitter.com/marktenenholtz/status/1525484363579895808,@rasbt @ChrSzegedy I think it’s actually from the original inception paper — random resized crops IIRC
1762,@marktenenholtz,2022-05-14 14:21:24+00:00,https://twitter.com/marktenenholtz/status/1525481409070104577,@rasbt Wonder why inception isn’t on there. That’s the one I hear referenced the most
1763,@marktenenholtz,2022-05-14 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1525445832278802433,"95% of the ML code you write is not code for fitting a model.

It’s for:

• Data collection
• Data cleaning
• Data exploration
• Feature engineering
• Model evaluation
• Model inference
• Model monitoring

Want to work more efficiently? Write more code and practice."
1764,@marktenenholtz,2022-05-13 22:55:06+00:00,https://twitter.com/marktenenholtz/status/1525248296750661634,"@rasbt Wow, your matrix multiply is so efficient that it actually frees memory!"
1765,@marktenenholtz,2022-05-13 18:06:13+00:00,https://twitter.com/marktenenholtz/status/1525175598385188866,"@sl8rv The best textbook would convince you to go seek out your own project.

This is why I'm biased towards builders. You know they've done that already."
1766,@marktenenholtz,2022-05-13 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1525083475048468482,Building out an ML project will teach you more about applied ML than any textbook.
1767,@marktenenholtz,2022-05-12 14:53:27+00:00,https://twitter.com/marktenenholtz/status/1524764697697964033,"@turboCodr Haha fair enough. Most people only spend minutes, so hours is quite an upgrade."
1768,@marktenenholtz,2022-05-12 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1524721088852230144,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
1769,@marktenenholtz,2022-05-12 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1524721086016880640,"I hope you learned something!

These are skills I've built through 1000's of hours of building ML models.

Follow me @marktenenholtz to get more high-signal ML content!"
1770,@marktenenholtz,2022-05-12 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1524721083269533697,"TL;DR:

1. Raw data observation
2. Model evaluation
3. Smart baselines
4. Strong starting points
5. Rapid iteration
6. Error analysis"
1771,@marktenenholtz,2022-05-12 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1524721080455159808,"Error analysis

The only reliable way of improving your models is by thoroughly analyzing their predictions.

The average modeler stares at their error metric and hopes it gets better.

The best find specific errors and tailor solutions to them."
1772,@marktenenholtz,2022-05-12 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1524721077615616000,"Rapid iteration

If we could know what techniques would work before attacking a problem, then we wouldn't need data scientists.

After pruning the bad ideas, the best modelers efficiently explore the remaining possibilities to land on the best solution.

Start small, work fast."
1773,@marktenenholtz,2022-05-12 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1524721074780315648,"Starting strong

Think of the potential solutions to an ML problem as a nearly-infinite tree.

The most successful modelers can prune all but 0.1% of that tree before they even start.

If you know something won't work, why waste your valuable time?"
1774,@marktenenholtz,2022-05-12 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1524721071986905089,"Useful baselines

Baselines are for measuring improvement, not just a vacuous number.

The best know to use realistic ones, such as human performance and intelligent heuristics.

The best model is the simplest, so always start there and see how much better you can get."
1775,@marktenenholtz,2022-05-12 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1524721069193523200,"Ingenious model evaluation

The best modelers are the best model evaluators.

If there was a Venn Diagram of the two groups, it would be a circle.

This is a skill that anyone can learn, but it takes several hundred hours of modeling experience to become an expert."
1776,@marktenenholtz,2022-05-12 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1524721066358153216,"Observation of raw data

EDA from aggregations is good.

EDA by looking at thousands of samples is the best.

EDA is for becoming one with your data, with all of its bumps and sharp edges. This is how to do it.

Measure this in hours, not minutes."
1777,@marktenenholtz,2022-05-12 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1524721063594135552,6 habits of the top 0.1% of machine learning modelers:
1778,@marktenenholtz,2022-05-12 00:29:12+00:00,https://twitter.com/marktenenholtz/status/1524547200986005505,@tunguz Got one coming your way tomorrow
1779,@marktenenholtz,2022-05-10 19:27:59+00:00,https://twitter.com/marktenenholtz/status/1524109012173787138,"@Giba1 Thanks for making such an awesome package. Using it has enabled me to approach problems in completely new ways.

I think y'all are entirely responsible for making SVMs relevant again."
1780,@marktenenholtz,2022-05-10 19:25:19+00:00,https://twitter.com/marktenenholtz/status/1524108340573392896,"@brshallo Wonderfully, in fact. It can be a great way to tell if you're likely going to need to retrain your models.

Check this out: https://t.co/tmoy1HCENB"
1781,@marktenenholtz,2022-05-10 18:28:54+00:00,https://twitter.com/marktenenholtz/status/1524094142560755714,"@LBacaj Day traders tend to get real quiet in down markets.

Bad strategies can win in rising markets, but the rug is pulled out from you when the market is down."
1782,@marktenenholtz,2022-05-10 18:19:17+00:00,https://twitter.com/marktenenholtz/status/1524091720400576514,"@Giba1 One thing I’d add that I wish was documented:

If you have an existing environment with PyTorch @ cudatoolkit=11.3, you can directly install rapids packages in there.

Instead of conda create, you just conda install without specifying cudatoolkit version."
1783,@marktenenholtz,2022-05-10 16:46:29+00:00,https://twitter.com/marktenenholtz/status/1524068369221173249,@JustinSaaS I’d “pull the trigger” on something you were selling even if it was $450+. It’s a testament to how much trust you’ve built by delivering a ton of value!
1784,@marktenenholtz,2022-05-10 15:26:39+00:00,https://twitter.com/marktenenholtz/status/1524048279297839105,"@SahilBloom Real winners discover winning tactics when their back is against the wall.

The rest, well, chase a curveball on a 3-2 count in the bottom of the 9th."
1785,@marktenenholtz,2022-05-10 13:56:10+00:00,https://twitter.com/marktenenholtz/status/1524025505690918912,"@delai50 Depends on what context we’re talking about. 

If I’m convinced my CV is as rock solid as it’s gonna get, I just remember that the test set is basically just another fold."
1786,@marktenenholtz,2022-05-10 12:17:25+00:00,https://twitter.com/marktenenholtz/status/1524000655802871809,"@__mharrison__ An untuned, fast-to-fit model, like a random forest or logistic regression"
1787,@marktenenholtz,2022-05-10 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1523996311917268994,"My 6 step pipeline for training tabular models:

1. EDA
2. Simple baseline
3. Create+test evaluation setup
4. Simple model with simple features
5. Error analysis
6. Feature engineering

Repeat 5+6 until you run out of time!"
1788,@marktenenholtz,2022-05-10 02:25:10+00:00,https://twitter.com/marktenenholtz/status/1523851609226289152,@tunguz You mean like this? https://t.co/lCdSoBn1CK
1789,@marktenenholtz,2022-05-10 01:44:51+00:00,https://twitter.com/marktenenholtz/status/1523841464341614593,"@AlejandroPiad @tunguz Haha yeah I couldn’t agree more. It’s very empirical. 

Find me a rigorous proof of why ResNets work (it doesn’t exist)."
1790,@marktenenholtz,2022-05-09 22:40:50+00:00,https://twitter.com/marktenenholtz/status/1523795156537065473,@xtinacomputes I use Typefully
1791,@marktenenholtz,2022-05-09 20:44:38+00:00,https://twitter.com/marktenenholtz/status/1523765914620993537,@bethanylazulon That’s lit
1792,@marktenenholtz,2022-05-09 20:44:01+00:00,https://twitter.com/marktenenholtz/status/1523765758614192128,"@svpino @rasbt @PyTorchLightnin In @bnomial, veritas https://t.co/5lwOakcy8E"
1793,@marktenenholtz,2022-05-09 19:38:46+00:00,https://twitter.com/marktenenholtz/status/1523749338358292480,"@svpino @PyTorchLightnin If I don't get this right, my whole life is a lie"
1794,@marktenenholtz,2022-05-09 18:30:04+00:00,https://twitter.com/marktenenholtz/status/1523732046048227328,"@tunguz and ""you need to spend 5 years learning all the theory before building models"" guys"
1795,@marktenenholtz,2022-05-09 18:26:06+00:00,https://twitter.com/marktenenholtz/status/1523731051243196416,@tunguz Data science Twitter has a massive divide between academic data science (horribly toxic) and applied ML (amazingly awesome human beings).
1796,@marktenenholtz,2022-05-09 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1523633906670465025,"90% of building an ML model is good model evaluation.

100% of getting anyone to use it is communicating its value.

Don’t just learn to evaluate your models — learn how to sell them."
1797,@marktenenholtz,2022-05-09 00:18:34+00:00,https://twitter.com/marktenenholtz/status/1523457365122510848,@LBacaj Intuition scales better than people will admit
1798,@marktenenholtz,2022-05-08 21:03:38+00:00,https://twitter.com/marktenenholtz/status/1523408304923639808,@AlejandroPiad Totally agree. We have an evaluation problem masquerading as a fairness problem.
1799,@marktenenholtz,2022-05-08 20:59:34+00:00,https://twitter.com/marktenenholtz/status/1523407283387985920,"@RDub2 If your baseline is really good, then why add machine learning?"
1800,@marktenenholtz,2022-05-08 20:52:20+00:00,https://twitter.com/marktenenholtz/status/1523405461336170498,@Aella_Girl Jumping in is literally the best way to learn it lol
1801,@marktenenholtz,2022-05-08 20:39:54+00:00,https://twitter.com/marktenenholtz/status/1523402335958970368,@apachaves That’s part of step 2!
1802,@marktenenholtz,2022-05-08 20:39:39+00:00,https://twitter.com/marktenenholtz/status/1523402269437218817,@kevinschawinski Improving the data == improving the model
1803,@marktenenholtz,2022-05-08 20:38:52+00:00,https://twitter.com/marktenenholtz/status/1523402073156452352,@tomar840 Improving the data == improving the model
1804,@marktenenholtz,2022-05-08 20:38:16+00:00,https://twitter.com/marktenenholtz/status/1523401921490419713,@rcsaxe 1000%. Huge gap out there right now.
1805,@marktenenholtz,2022-05-08 20:35:44+00:00,https://twitter.com/marktenenholtz/status/1523401284232962048,@RDub2 You don’t think modeling is an iterative process?
1806,@marktenenholtz,2022-05-08 20:35:14+00:00,https://twitter.com/marktenenholtz/status/1523401159775457283,@fa_shimshi That’s part of step 2 😜
1807,@marktenenholtz,2022-05-08 20:34:52+00:00,https://twitter.com/marktenenholtz/status/1523401068129882115,@Ne_oL You’ll get better at it the more you do it!
1808,@marktenenholtz,2022-05-08 20:28:31+00:00,https://twitter.com/marktenenholtz/status/1523399469907148800,"@LBacaj Iterate, iterate, iterate, and know when to stop"
1809,@marktenenholtz,2022-05-08 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1523271510273576960,"2 steps to train great models:

1. Train a bad one
2. Improve it

Any data scientist can do step 1, and most can attempt step 2.

But &lt;0.1% are truly great at step 2."
1810,@marktenenholtz,2022-05-07 16:05:53+00:00,https://twitter.com/marktenenholtz/status/1522970985468678145,@xtinacomputes Once the quote RTs start I usually find it’s time to mute the thread lol
1811,@marktenenholtz,2022-05-07 16:03:30+00:00,https://twitter.com/marktenenholtz/status/1522970388048850944,"@dvassallo In statistics, uncertainty basically is proportional to variance divided by number of attempts.

We have little control over the variance. But we have a lot of control over the attempts."
1812,@marktenenholtz,2022-05-07 15:55:23+00:00,https://twitter.com/marktenenholtz/status/1522968344323715073,"@xtinacomputes I can’t understand why people are upset with you.

Do they really think all of us started our accounts just to be influencers? 

And even if that’s the case, how is it hypocritical to say ppl should focus more on tech when you clearly do?

Folks never cease to amaze"
1813,@marktenenholtz,2022-05-07 15:07:00+00:00,https://twitter.com/marktenenholtz/status/1522956167001030657,@OneJKMolina @dickiebush Just write 20 things for men in their 20’s to do
1814,@marktenenholtz,2022-05-07 15:03:30+00:00,https://twitter.com/marktenenholtz/status/1522955288311173121,"@JustinSaaS @dickiebush The sign of great beginner advice is that it even makes experts think “damn, I should be doing that”"
1815,@marktenenholtz,2022-05-07 14:49:29+00:00,https://twitter.com/marktenenholtz/status/1522951759249002498,"@gib_hurst Yup, and trying new things is the best way to learn what actually works and what doesn’t"
1816,@marktenenholtz,2022-05-07 14:45:32+00:00,https://twitter.com/marktenenholtz/status/1522950767740538880,"Aspiring data scientists often ask:

“What’s the better way to solve this problem, X or Y?”

Sometimes there’s an obvious answer.

But 95% of the time, you just need to go try it and see what works best."
1817,@marktenenholtz,2022-05-07 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1522909119547514881,"Tons of SaaS builders are using GPT-3. 

And 98% of them use it poorly.

It’s an incredible tool, but skilled prompt engineering can get you quality results 10x more often."
1818,@marktenenholtz,2022-05-06 12:07:34+00:00,https://twitter.com/marktenenholtz/status/1522548626140057601,@svpino It’s a fact of life!
1819,@marktenenholtz,2022-05-06 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1522546744810041345,"What data scientists think they need for a good solution:

• Bigger servers
• Bigger data
• Bigger models

What they SHOULD be using for a good solution:

• A smaller model
• A deep knowledge of the data
• A debugger

Rapid iteration + solid theory + solid coding = success"
1820,@marktenenholtz,2022-05-05 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1522184376313733121,"Momentum, explained:
https://t.co/57K2shXi3k"
1821,@marktenenholtz,2022-05-05 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1522184373369409536,"Deep Learning for Coders:

https://t.co/r8qGhGLvAa"
1822,@marktenenholtz,2022-05-05 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1522184370366271488,"NYU's Deep Learning course:

https://t.co/4ju1S7HoVl"
1823,@marktenenholtz,2022-05-05 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1522184367417659392,"Matrix Calculus for Deep Learning:

https://t.co/7L7ISqyQqU"
1824,@marktenenholtz,2022-05-05 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1522184364313825285,"Essence of Linear Algebra from @3blue1brown:

https://t.co/rGpmE3kNXZ"
1825,@marktenenholtz,2022-05-05 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1522184361239465986,"If you want to apply the material you learned in the above course, use this library:
https://t.co/r6tXmACAhm"
1826,@marktenenholtz,2022-05-05 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1522184358240538624,Deep Learning for Computer Vision from University of Michigan: https://t.co/aEAfggK0UT
1827,@marktenenholtz,2022-05-05 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1522184355275104256,"Deep Learning with PyTorch: 
https://t.co/p2otzAh7xC"
1828,@marktenenholtz,2022-05-05 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1522184352255283200,"The best paper ever written on model evaluation from @rasbt:

https://t.co/eZ7NcXzZe6"
1829,@marktenenholtz,2022-05-05 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1522184349109473281,"@huggingface's own book on NLP:
https://t.co/aPUYZupi3V"
1830,@marktenenholtz,2022-05-05 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1522184346324459520,"I've shared tons of free and inexpensive material for learning machine learning.

Combined, their value is easily greater than a $145,092 machine learning degree.

Here are the best ones:"
1831,@marktenenholtz,2022-05-04 16:46:10+00:00,https://twitter.com/marktenenholtz/status/1521893960645545988,"@andrehaykaljr Better data dictionaries, explanations of collection methods, and more generous rate limits!"
1832,@marktenenholtz,2022-05-04 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1521821990113267712,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
1833,@marktenenholtz,2022-05-04 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1521821987399618560,"TL;DR:

1. Learn by building
2. Specialize
3. Build mental maps
4. Learn independently, share with others.
5. Get your sleep
6. Find your zone

If you got value from this, follow me @marktenenholtz for more high-signal ML content!"
1834,@marktenenholtz,2022-05-04 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1521821984673263616,"A little confusion is a good thing

There's a concept called the ""Zone of Proximal Development.""

The idea is if you don't stretch yourself at all, you won't learn a thing.

Stretch yourself too much and the same thing will happen.

Find your own frontier."
1835,@marktenenholtz,2022-05-04 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1521821981879853056,"Learn with a community

Techincal topics are hard. Nobody gets it all themselves.

I learn the most when I spend some time learning on my own and then sit down with a few friends to discuss it."
1836,@marktenenholtz,2022-05-04 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1521821979124269056,"Write about what you learn

(This one isn't just about learning)

It's not only for the Feynman method.

Apply to jobs with no personal brand and your resume is just a piece of paper in a large pile.

Make yourself recognizable and you get moved to the top."
1837,@marktenenholtz,2022-05-04 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1521821976393777157,"Embrace the Feynman method

If you can't explain a complicated topic in simple words, you don't understand it.

The act of explaining it or writing about it this way exposes holes in your understanding.

If you find yourself stuck using jargon, that's the part you don't get."
1838,@marktenenholtz,2022-05-04 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1521821973633851392,"Build a mental map of your field

New methods in technical fields are usually minor modifications of previous methods.

It becomes really easy to pick up new topics if you keep in mind how they're all related.

""X is like Y, but improves it by adding Z"""
1839,@marktenenholtz,2022-05-04 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1521821970844639232,"Time box your focused learning

Nobody can handle 10 hours per day, every day.

You'll burn out and your productivity will grind to a halt.

Come back to it after getting your 8 hours and you'll absorb material like a sponge."
1840,@marktenenholtz,2022-05-04 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1521821968093249536,"Get 8 hours of sleep every night

If you don't, you'll take 2x as long to learn 0.5x the material.

Also, your brain reinforces what you learned while you sleep."
1841,@marktenenholtz,2022-05-04 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1521821962418327552,"Build T-shaped skills

You can spend all your time becoming average at everything or focus on what truly interests you.

You need to have exposure to a variety of topics.

But having a specialty is what makes you valuable."
1842,@marktenenholtz,2022-05-04 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1521821959637463040,"If you learn without doing, you won't learn anything at all

Not only will you not truly understand, but you also won't retain any of it.

Blog posts, courses, and bootcamps are step 1, not the whole process."
1843,@marktenenholtz,2022-05-04 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1521821956860825601,10 principles for learning anything technical (that will teach you more than a $70k Master's degree):
1844,@marktenenholtz,2022-05-04 00:23:17+00:00,https://twitter.com/marktenenholtz/status/1521646612384260096,@millhousecodes Thanks! Appreciated your question!
1845,@marktenenholtz,2022-05-03 20:53:26+00:00,https://twitter.com/marktenenholtz/status/1521593802389180416,Starting this in just a couple minutes!
1846,@marktenenholtz,2022-05-03 20:43:10+00:00,https://twitter.com/marktenenholtz/status/1521591217213034499,@julien_c Multi-task evaluation is something I'm incredibly interested in myself!
1847,@marktenenholtz,2022-05-03 18:17:48+00:00,https://twitter.com/marktenenholtz/status/1521554634120351745,@prthgo hoot hoot bitch! looking forward to seeing you there.
1848,@marktenenholtz,2022-05-03 18:12:42+00:00,https://twitter.com/marktenenholtz/status/1521553349606350849,@prthgo So sorry about that bro. Definitely gonna do more in the future at different times for that reason 😔
1849,@marktenenholtz,2022-05-03 17:52:21+00:00,https://twitter.com/marktenenholtz/status/1521548230949064711,"Word has it, the XGBoost king himself @tunguz may be dropping by at some point 👀"
1850,@marktenenholtz,2022-05-03 15:21:25+00:00,https://twitter.com/marktenenholtz/status/1521510247130619905,"Tons of great questions in here.

Gonna host a Space to talk about some of these topics later today (5pm EDT)!

https://t.co/pP81r0mDRB"
1851,@marktenenholtz,2022-05-03 12:46:17+00:00,https://twitter.com/marktenenholtz/status/1521471206356135936,@tng_konrad @PredaGabi Fantastic series. I’ve watched a couple of the videos
1852,@marktenenholtz,2022-05-03 12:02:19+00:00,https://twitter.com/marktenenholtz/status/1521460141589073920,What's an ML topic you are struggling to learn or wish you knew more about?
1853,@marktenenholtz,2022-05-02 18:43:40+00:00,https://twitter.com/marktenenholtz/status/1521198757601320962,@OneJKMolina What are 20 things someone should accomplish in their 20’s?
1854,@marktenenholtz,2022-05-02 16:51:18+00:00,https://twitter.com/marktenenholtz/status/1521170476688715777,@danbri @AlejandroPiad @tunguz @svpino @haltakov @yudivian @__mharrison__ It sounds like it's enabled by compiling CPython down to WASM
1855,@marktenenholtz,2022-05-02 16:42:10+00:00,https://twitter.com/marktenenholtz/status/1521168180844146688,@tunguz More like HTMLpy-ve
1856,@marktenenholtz,2022-05-02 16:22:28+00:00,https://twitter.com/marktenenholtz/status/1521163220811235330,@AlejandroPiad @tunguz @svpino @haltakov @yudivian @__mharrison__ We can finally give up on nonsense like Tensorflow.js 😂
1857,@marktenenholtz,2022-05-02 16:15:29+00:00,https://twitter.com/marktenenholtz/status/1521161465666887681,"@tunguz @AlejandroPiad @svpino @haltakov @yudivian @__mharrison__ Yup. Every new technology introduces security concerns, so I'm usually not swayed by those arguments unless there's something that is inherently impossible to fix, which I doubt is the case here."
1858,@marktenenholtz,2022-05-02 16:10:21+00:00,https://twitter.com/marktenenholtz/status/1521160171300470785,"@AlejandroPiad @tunguz @svpino @haltakov @yudivian @__mharrison__ As far as the computational cost, I'm not sure that's 100% obvious. Node is probably more efficient because of V8, but plenty of Python code is just a thin layer over C anyways."
1859,@marktenenholtz,2022-05-02 16:08:36+00:00,https://twitter.com/marktenenholtz/status/1521159730579841025,"@AlejandroPiad @tunguz @svpino @haltakov @yudivian @__mharrison__ I don't know much about it but I think a lot of it comes down to what enables it under the hood.

Personally, the biggest barrier to creating awesome web apps for me is knowledge of Node, so if this bypasses that need I'm 100% for it."
1860,@marktenenholtz,2022-05-02 14:01:15+00:00,https://twitter.com/marktenenholtz/status/1521127684016852992,"@sGx_tweets Ha, would probably bother me more if it didn’t happen all the time."
1861,@marktenenholtz,2022-05-02 13:47:29+00:00,https://twitter.com/marktenenholtz/status/1521124217294663680,"@mervenoyann 1. EDA
2. Simple baseline
3. Create+test evaluation setup
4. Simple model, no FE
5. Error analysis
6. Feature engineering

Repeat 5+6 until you run out of time!"
1862,@marktenenholtz,2022-05-02 13:10:50+00:00,https://twitter.com/marktenenholtz/status/1521114996419272704,"@moveera Generalists tend to just be okay at a lot of things. Employers want people that are really good at a few things while having a decent understanding of the rest.

Specialize, don’t generalize."
1863,@marktenenholtz,2022-05-02 13:09:44+00:00,https://twitter.com/marktenenholtz/status/1521114719880462337,@BCopela80821946 I self-taught and it’s working out well for me. I think folks that rely too much on college are at a higher risk of these issues tbh
1864,@marktenenholtz,2022-05-02 13:04:06+00:00,https://twitter.com/marktenenholtz/status/1521113299617820673,"@philipvollet 😂 spoken from experience, trust me"
1865,@marktenenholtz,2022-05-02 12:46:02+00:00,https://twitter.com/marktenenholtz/status/1521108753478791168,@rcsaxe Glad you liked it!
1866,@marktenenholtz,2022-05-02 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1521097236930695169,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
1867,@marktenenholtz,2022-05-02 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1521097228571451392,You've never spent enough time exploring the data in your projects.
1868,@marktenenholtz,2022-05-02 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1521097225878728704,You've built models but never deployed them.
1869,@marktenenholtz,2022-05-02 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1521097223223726080,You understand the theory but don't understand the practical problems that show up in real data.
1870,@marktenenholtz,2022-05-02 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1521097220589711361,You copy projects you found on the internet rather than tackling ones that inspire you.
1871,@marktenenholtz,2022-05-02 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1521097217972400129,You try to tailor your projects to a solution rather than tailoring your solutions to the problem.
1872,@marktenenholtz,2022-05-02 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1521097215292235776,You aren't writing about what you've built.
1873,@marktenenholtz,2022-05-02 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1521097212658257921,You haven't spent enough time learning how to write quality code.
1874,@marktenenholtz,2022-05-02 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1521097209957154816,You don't know how to efficiently read research papers.
1875,@marktenenholtz,2022-05-02 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1521097207234981894,You learn alone instead of with a community.
1876,@marktenenholtz,2022-05-02 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1521097204542320640,You think university courses teach you everything you need to be successful.
1877,@marktenenholtz,2022-05-02 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1521097201887326209,You don't know how to iteratively improve your models.
1878,@marktenenholtz,2022-05-02 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1521097199186112512,You think handling missing data is a consistent formula instead of a problem-specific endeavor.
1879,@marktenenholtz,2022-05-02 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1521097196497629184,You use curated datasets for all of your projects instead of scraping your own data.
1880,@marktenenholtz,2022-05-02 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1521097193830043648,You are too focused on the science and not focused enough on the product it empowers.
1881,@marktenenholtz,2022-05-02 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1521097191174991874,"You focus too much on ""journey to data scientist"" roadmaps and not enough on learning what interests you."
1882,@marktenenholtz,2022-05-02 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1521097185802158080,You don't understand how to evaluate your models.
1883,@marktenenholtz,2022-05-02 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1521097183142895618,You spend too much time learning theory without applying it to anything.
1884,@marktenenholtz,2022-05-02 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1521097180357922816,19 qualities holding you back from your dream machine learning job:
1885,@marktenenholtz,2022-05-01 13:00:34+00:00,https://twitter.com/marktenenholtz/status/1520750023079829505,"@parker_brydon Once you have enough experience to know how to tune them quickly, the boost in accuracy they provide is better than random forests"
1886,@marktenenholtz,2022-05-01 12:59:34+00:00,https://twitter.com/marktenenholtz/status/1520749772692406273,@lospabloss They’re basically interchangeable
1887,@marktenenholtz,2022-05-01 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1520734792894935041,"Why is XGBoost so incredible?

It's not because:

• It's always the most accurate
• It's always the fastest
• It's always the most interpretable

It's because:

• It's always the fastest to a good (tabular) solution.

Yeah, I can beat it with deep learning. But time is money."
1888,@marktenenholtz,2022-04-30 22:18:46+00:00,https://twitter.com/marktenenholtz/status/1520528109904277506,@rasbt @chrisalbon @jeremyphoward @HelloPaperspace FWIW I make heavy use of Colab Pro+ for Kaggle competitions. It’s not ideal but you can definitely make a working pattern around it.
1889,@marktenenholtz,2022-04-30 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1520372405352292353,"How most data scientists want to spend their time:

• 90% on building models
• 10% on solution engineering

But, the value comes

• 10% from building models
• 90% from solution engineering

ML models are incredibly valuable, but remember: they are still software."
1890,@marktenenholtz,2022-04-29 13:22:33+00:00,https://twitter.com/marktenenholtz/status/1520030778696032256,@svpino You can tell how well a data scientist understands their image data by how they augment it during training.
1891,@marktenenholtz,2022-04-29 12:13:06+00:00,https://twitter.com/marktenenholtz/status/1520013303468236802,"@rasbt @JFPuget @kaggle I bet if your proposed that, you’d be showered with “but kaggle doesn’t translate to real life” as if their current benchmarks do 🤦‍♂️"
1892,@marktenenholtz,2022-04-29 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1520010047610179584,"Improving ML models is an iterative process.

You should always:

1. Evaluate the model
2. Figure out what it's missing
3. Transform your data to surface it
4. Repeat

Model evaluation, error analysis, feature engineering. The more times you try, the better.

Wash, rinse, repeat."
1893,@marktenenholtz,2022-04-28 21:43:23+00:00,https://twitter.com/marktenenholtz/status/1519794430982643712,"@MathNerdJeremy I never use ResNet’s on the raw waveforms, only on the 2D spectrograms"
1894,@marktenenholtz,2022-04-28 21:14:16+00:00,https://twitter.com/marktenenholtz/status/1519787104892592128,"@DienhoaT Wow was I really the one that inspired you? 

If so that’s so awesome! Congrats!"
1895,@marktenenholtz,2022-04-28 16:17:26+00:00,https://twitter.com/marktenenholtz/status/1519712403235160068,@bhutanisanyam1 @kaggle https://t.co/C25BGgNEi5
1896,@marktenenholtz,2022-04-28 13:41:09+00:00,https://twitter.com/marktenenholtz/status/1519673074102288385,@0verfit @nyanp It's trending that way for me too https://t.co/HBpH8FmyzK
1897,@marktenenholtz,2022-04-28 13:21:20+00:00,https://twitter.com/marktenenholtz/status/1519668087917789185,@nyanp @0verfit It seems like a lot of top Kagglers have generally moved away from tabular. I've been trying to find more folks to team with but it's hard to find familiar faces.
1898,@marktenenholtz,2022-04-28 13:14:44+00:00,https://twitter.com/marktenenholtz/status/1519666426675671043,"@0verfit @nyanp I would say join Foursquare to inspire your fellow Japanese but, selfishly, that would make my life a lot harder 😬"
1899,@marktenenholtz,2022-04-28 13:08:43+00:00,https://twitter.com/marktenenholtz/status/1519664912427012097,"@0verfit Well, at least @nyanp is really good at tabular :)"
1900,@marktenenholtz,2022-04-28 12:48:36+00:00,https://twitter.com/marktenenholtz/status/1519659847133933569,@prthgo I did the same going from mechanical engineering to data science. Congrats!
1901,@marktenenholtz,2022-04-28 12:00:18+00:00,https://twitter.com/marktenenholtz/status/1519647691973701632,"I hope this list inspired you!

Follow me @marktenenholtz for more high-signal ML content."
1902,@marktenenholtz,2022-04-28 12:00:17+00:00,https://twitter.com/marktenenholtz/status/1519647689293516800,"This is by no means a comprehensive list.

Got something else? Reply to the first tweet and I’ll add it to the list."
1903,@marktenenholtz,2022-04-28 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1519647686541987840,Try running PCA on your data and either building a model on those features or adding the PCA features to your original dataset.
1904,@marktenenholtz,2022-04-28 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1519647683639603200,Try running clustering like k-means on your data and adding each row’s cluster label as a feature.
1905,@marktenenholtz,2022-04-28 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1519647680888143872,Time series data? Create rolling or lagged features.
1906,@marktenenholtz,2022-04-28 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1519647678115708928,"Have group fields like product hierarchy data, dates, cities, etc.?

Try creating aggregations on those groups (mean, median, stddev, etc.)"
1907,@marktenenholtz,2022-04-28 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1519647675058393089,"Create interaction terms between columns, such as subtracting or multiplying them. 

Even tree-based models have a hard time picking up on interactions"
1908,@marktenenholtz,2022-04-28 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1519647672340140032,"Try splitting categorical data into multiple columns, or combining two categorical fields into one."
1909,@marktenenholtz,2022-04-28 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1519647669680975872,Now let’s talk about creating features:
1910,@marktenenholtz,2022-04-28 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1519647666774306816,"For continuous data:

If you’re using a tree-based model, you probably don’t need to do anything.

For other models, try standardizing or normalizing.

If a variable has a weird distribution, try a log or sqrt transformation, or even something like Box-Cox."
1911,@marktenenholtz,2022-04-28 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1519647663972487168,"For categorical data:

Using a GBDT/RF?  Try label encoding.

Linear model/SVM/similar model? Try dummy encoding.

Using a neural net? Try label encoding and using embedding layers."
1912,@marktenenholtz,2022-04-28 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1519647661267193856,Now let’s talk transforming values:
1913,@marktenenholtz,2022-04-28 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1519647658578644992,"Once you’ve imputed, consider adding a corresponding binary column that indicates whether the other column was missing in that row."
1914,@marktenenholtz,2022-04-28 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1519647655881691136,"Many popular gradient-boosting libraries like XGBoost or LightGBM can handle NaN’s, so you can try that too."
1915,@marktenenholtz,2022-04-28 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1519647652928901120,"Always start by filling them with a value that makes sense.

If there isn’t an obvious value, try:

• Filling with the mean
• Filling with the median
• Filling with zero

For categorical features: 
• Filling with the mode
• Filling with a negative value
• Frequency encoding"
1916,@marktenenholtz,2022-04-28 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1519647650252947456,Let’s start with handling NaN’s:
1917,@marktenenholtz,2022-04-28 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1519647647526576129,"The point of this thread is not to tell you what works, it’s to give you ideas for what MAY work.

Always start with what makes sense from your EDA and make sure to cross-validate anything you try here to see if it actually helps!

Let’s get started."
1918,@marktenenholtz,2022-04-27 19:53:59+00:00,https://twitter.com/marktenenholtz/status/1519404513916076035,@PHuenermund @causal_science Love it!
1919,@marktenenholtz,2022-04-27 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1519285272428486657,"TL;DR:

1. Avoid generalism
2. Be T-shaped
3. Build to learn
4. Write it down
5. Step back and broaden

Wash, rinse, repeat.

Follow me @marktenenholtz for more about learning and high-signal ML content!"
1920,@marktenenholtz,2022-04-27 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1519285269601521667,"Step back and broaden your scope

After diving deep into one area and writing about it, it's time to learn something else.

At this point, you'll probably be very ready for something new.

Think back to where you felt weakest while you were building -- that's a good next step."
1921,@marktenenholtz,2022-04-27 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1519285266753589248,"Write about your learnings

After a while, it's vital to compress and reinforce it by teaching or writing about it.

This is the secret to long-term retention.

You will always find yourself forgetting small details, but this is the best way to maintain the big picture."
1922,@marktenenholtz,2022-04-27 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1519285263846969344,"Build to learn

You can know all the theory in the world, but the only way to provide value is to build.

So, what's the best way to learn?

By building.

Data science is all about understanding extremely granular problem-solving details, and you only learn those by building."
1923,@marktenenholtz,2022-04-27 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1519285260550254596,"Build T-shaped skills

The first step is to figure out what you really enjoy doing.

Most figure this out when they're building their basic skills.

Once you find what gets you going, go all-in and specialize. This is what I mean by T-shaped: https://t.co/28CgJtXHXl"
1924,@marktenenholtz,2022-04-27 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1519285251507294208,"Avoid being a generalist

You definitely need to have a wide, baseline set of skills.

But, being a generalist makes you inherently generic and replaceable.

Data science is such a wide field now that you can carve out a niche for yourself, and that should be your goal."
1925,@marktenenholtz,2022-04-27 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1519285248583905280,"Data science superpower in 2022:

Specialization.

""Data science"" is a catch-all term. It can describe tons of unrelated skill-sets.

The only way to stand out is to build a strong, unique set of skills.

Here's how to do it:"
1926,@marktenenholtz,2022-04-26 22:03:19+00:00,https://twitter.com/marktenenholtz/status/1519074670326292481,@BillFreeman44 The literal interpretation. I love it.
1927,@marktenenholtz,2022-04-26 15:37:57+00:00,https://twitter.com/marktenenholtz/status/1518977690061225985,@tunguz @Twitter Congrats! Keep spreading the good word from the Book of XGBoost
1928,@marktenenholtz,2022-04-26 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1518922859757723648,"Underrated tip for learning ML:

Write about what you just learned.

• Exposes holes in your knowledge
• Promotes long-term comprehension
• Improves your communication skills

Einstein did it. Feynman did it.

Why not you?"
1929,@marktenenholtz,2022-04-25 13:10:28+00:00,https://twitter.com/marktenenholtz/status/1518578189118971904,"@tibo_maker It takes a few years before most data scientists can think with product in mind. 

There’s a bigger gap between the theory and the user than most software features."
1930,@marktenenholtz,2022-04-25 12:00:20+00:00,https://twitter.com/marktenenholtz/status/1518560536555782144,What is a machine learning course/book that shaped you?
1931,@marktenenholtz,2022-04-25 11:24:13+00:00,https://twitter.com/marktenenholtz/status/1518551449185173504,@AlejandroPiad Great advice. Beginners would be surprised to see how hard it can be to beat a “predict the last known value” baseline or something like that for time series problems.
1932,@marktenenholtz,2022-04-24 21:52:33+00:00,https://twitter.com/marktenenholtz/status/1518347186160943107,@aakashg0 1000% haha
1933,@marktenenholtz,2022-04-24 21:06:40+00:00,https://twitter.com/marktenenholtz/status/1518335640949121024,"@aakashg0 Those are usually the ones who communicate our value.

I’m sure you’d never do it badly :)"
1934,@marktenenholtz,2022-04-24 18:00:56+00:00,https://twitter.com/marktenenholtz/status/1518288897125588997,@svpino This is the best piece of advice you can give a data scientist
1935,@marktenenholtz,2022-04-24 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1518198075428196352,"When are you officially a data scientist?

Not when:

• You build a model
• You build a database
• You perform a hypothesis test

It's when:

• A PM promises your model does something it definitely doesn't

Welcome to the land of both ethically fraught tasks and sales tactics"
1936,@marktenenholtz,2022-04-23 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1517835696853069824,"Timeless truth of model evaluation:

Every time-series model looks good in a rising market.

It's the falling markets that separate the wheat from the chaff."
1937,@marktenenholtz,2022-04-22 17:34:12+00:00,https://twitter.com/marktenenholtz/status/1517557393131032577,@SIDonCODE Not saying that
1938,@marktenenholtz,2022-04-22 15:31:19+00:00,https://twitter.com/marktenenholtz/status/1517526470104596480,@__mharrison__ And I quite love it! https://t.co/pb6FcUB2YW
1939,@marktenenholtz,2022-04-22 12:57:45+00:00,https://twitter.com/marktenenholtz/status/1517487823535890433,"@ph_singer I totally agree. That being said, it’s gonna be several years before “everyone” is doing it.

I think learning the fundamentals (model evaluation, error analysis, etc.) is transferable enough that you can still be ahead of the wave if you focus on tabular now."
1940,@marktenenholtz,2022-04-22 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1517473318743384073,"ML hype is focused heavily on tasks that take insane amounts of compute.

Images, text, etc.

The tough reality?

90%+ of the problems in industry rely on tabular data.

Beginners: focus your energy there.

Learn the fundamentals on tabular data, and you'll provide value quickly."
1941,@marktenenholtz,2022-04-22 09:50:38+00:00,https://twitter.com/marktenenholtz/status/1517440736773521410,@ChristophMolnar Great point! It was even useful back when I was studying mechanical engineering.
1942,@marktenenholtz,2022-04-21 22:05:11+00:00,https://twitter.com/marktenenholtz/status/1517263201934716929,"@CornelliusYW Once you learn how to do these things, you learn how to automate them :)"
1943,@marktenenholtz,2022-04-21 22:04:46+00:00,https://twitter.com/marktenenholtz/status/1517263096783556608,@kevinschawinski @Modulos_ai I still advocate that beginners do it a lot manually to gain an appreciation for what it takes
1944,@marktenenholtz,2022-04-21 22:04:04+00:00,https://twitter.com/marktenenholtz/status/1517262922686337026,@reeledapollo You grow numb over time :)
1945,@marktenenholtz,2022-04-21 19:42:13+00:00,https://twitter.com/marktenenholtz/status/1517227223996063746,@GaryMarcus My mission: create neuro-symbolic AI only with code generated by GPT-3
1946,@marktenenholtz,2022-04-21 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1517110923856515073,"If you're a data scientist that wants to grow as a builder, here's how.

Pick a project that makes you:

1. Scrape data
2. Clean data
3. Organize data
4. Explore data
5. Try many models
6. Deploy models
7. Monitor models

If you want to be a builder, master those skills."
1947,@marktenenholtz,2022-04-20 19:41:09+00:00,https://twitter.com/marktenenholtz/status/1516864569309814788,@MolasAlex I mean... I literally am a Kaggle Master...
1948,@marktenenholtz,2022-04-20 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1516748549219753985,"I hope you found this insightful!

Follow me @marktenenholtz for more high-signal ML and let's all become elevate towards expert together."
1949,@marktenenholtz,2022-04-20 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1516748546489257986,"Beginners lose the forest for the trees.

Experts keep the objective in mind."
1950,@marktenenholtz,2022-04-20 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1516748543813320715,"Beginners wonder if something will work.

Experts try it quickly and let you know."
1951,@marktenenholtz,2022-04-20 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1516748541057658881,"Beginners make the model figure out what to do with the data.

Experts use the data to figure out what to do with the model."
1952,@marktenenholtz,2022-04-20 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1516748538348154880,"Beginners hope someone else has made the whole pipeline they want.

Experts marry their own code with the best of stackoverflow to create their own pipelines."
1953,@marktenenholtz,2022-04-20 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1516748535709896707,"Beginners spend 100 hours trying 1 thing.

Experts spend 1 hour trying 100 things."
1954,@marktenenholtz,2022-04-20 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1516748532979363843,"Beginners create every feature imaginable.

Experts transform their data into powerful representations."
1955,@marktenenholtz,2022-04-20 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1516748530274078721,"Beginners complicate their models.

Experts simplify theirs."
1956,@marktenenholtz,2022-04-20 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1516748527451267073,"Becoming an expert data scientist doesn't just ""happen.""

In 1000's of hours at my job, Kaggle, and elsewhere I've seen the qualities of the top 0.01% of them all.

Here are 7 that will open your eyes to what makes them tick:"
1957,@marktenenholtz,2022-04-20 02:34:38+00:00,https://twitter.com/marktenenholtz/status/1516606235776274433,"@LBacaj What ultimately saved them was a bold CEO with an even bolder vision. Maybe not even remotely reproducible, but it worked for them."
1958,@marktenenholtz,2022-04-20 00:16:01+00:00,https://twitter.com/marktenenholtz/status/1516571353373106178,@aakashg0 Thanks Aakash!
1959,@marktenenholtz,2022-04-20 00:14:47+00:00,https://twitter.com/marktenenholtz/status/1516571043397308428,@Splittestingcom Likewise! We have far too much to learn from each other
1960,@marktenenholtz,2022-04-19 12:24:21+00:00,https://twitter.com/marktenenholtz/status/1516392256625946628,@alfcnz I’ve never watched it 😅
1961,@marktenenholtz,2022-04-19 12:18:57+00:00,https://twitter.com/marktenenholtz/status/1516390895670677505,@CornelliusYW Glad you liked it!
1962,@marktenenholtz,2022-04-19 12:18:40+00:00,https://twitter.com/marktenenholtz/status/1516390826334633992,@alfcnz Talking about the one Netflix created here
1963,@marktenenholtz,2022-04-19 12:18:10+00:00,https://twitter.com/marktenenholtz/status/1516390697405923333,@alfcnz There was a British version of the show that came out in 1990. The new one that Netflix created is an adaptation of that one
1964,@marktenenholtz,2022-04-19 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1516386192438091778,"I hope you had as much fun reading that as I had writing it!

Follow me @marktenenholtz for more breakdowns of data science in the business world and high-signal ML content."
1965,@marktenenholtz,2022-04-19 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1516386189728628738,"It may have been a surprise to us, but Netflix knew it was going to take off before they even released it.

House of Cards was one of the biggest successes in the history of TV, and it's because it hit its audience squarely in the forehead.

Largely, thanks to data science."
1966,@marktenenholtz,2022-04-19 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1516386186981376000,"So, subscribers watching shows featuring Spacey saw advertisements for the show featuring him.

Those watching Thelma and Louise, for example, saw advertisements featuring the female characters.

Serious film lovers saw ones that showed off Finch's style."
1967,@marktenenholtz,2022-04-19 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1516386184154390533,"Ideation is great, but execution is everything.

They could have confidence in the talent on the show, but they needed to make sure their audience was excited."
1968,@marktenenholtz,2022-04-19 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1516386181398732811,"They noticed 3 key things:

1. The British version of House of Cards was a hit.

2. Films with Kevin Spacey were, too.

3. Their subscribers loved director of ""The Social Network"", David Fincher.

At the intersection of those 3 key insights was a huge potential audience."
1969,@marktenenholtz,2022-04-19 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1516386178651414529,"Netflix hadn't slowed down on their data science since the early 2000's.

They now were a huge streaming company, with rich customer data and oodles of computing power.

As a result, they knew so much more about what their subscribers wanted than traditional media companies."
1970,@marktenenholtz,2022-04-19 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1516386173144297474,"They weren't the only ones that noticed, though.

In 2012, their relationships with publishers became strained and a lot more expensive.

Rather than overpay, Netflix decided to reinvent itself yet again.

This time, as a studio."
1971,@marktenenholtz,2022-04-19 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1516386170371858433,"While analysts were predicting their downfall, Netflix went all-in on streaming.

From 2007-2011, grew nearly 4x and had roughly 23 million subscribers.

They were a couple of years deep into a huge contract with Starz that gave them access to roughly 2500 titles."
1972,@marktenenholtz,2022-04-19 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1516386167385731082,"Fast-forward (no pun intended) 6 years, and things were looking bleak.

Despite some impressive growth, they completely botched an attempt to spin off their DVD rental service as a separate subscription.

This is what lost them those 800,000 subscribers"
1973,@marktenenholtz,2022-04-19 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1516386164365619200,"Netflix knew how important these recommendations were, so they even sponsored a competition with a $1,000,000 prize for anyone that could improve on CineMatch.

(It was claimed 3 years later.)

In 2006, Netflix announced they were making the move to streaming."
1974,@marktenenholtz,2022-04-19 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1516386161605849090,"CineMatch basically worked by predicting how a user would rate content.

According to Netflix, it was accurate to within half a star 75% of the time.

Additionally, when users rented a CineMatch-recommended movie, they'd give it 5 stars 50% of the time."
1975,@marktenenholtz,2022-04-19 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1516386158787194886,"Despite consistent losses in the early 2000's, their business model offered them some unique opportunities.

Since they knew what every individual subscriber watched, they were able to build a powerful recommendation algorithm.

They called it CineMatch."
1976,@marktenenholtz,2022-04-19 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1516386155410853888,"If you know the history of Netflix at all, you know they started as a DVD rental company.

Their first foray into subscriptions was a $19.95 monthly subscription plan in 2000.

Look at that landing page! https://t.co/J08UKQ1aGS"
1977,@marktenenholtz,2022-04-19 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1516386146321780737,"In 2012, Netflix had just lost 800,000 subscribers and lost a $30,000,000 licensing agreement with Starz.

In Q4 of 2021, they added 10x that number of subscribers and are worth $151,000,000,000.

The key? World-class data science.

Here's how they outfoxed the media industry 🧵"
1978,@marktenenholtz,2022-04-19 00:11:32+00:00,https://twitter.com/marktenenholtz/status/1516207837483053063,@jeremyphoward This will be my fourth (fifth?) time following along. Can’t wait!
1979,@marktenenholtz,2022-04-18 22:46:23+00:00,https://twitter.com/marktenenholtz/status/1516186405969756162,@JFPuget Well @wightmanr might be able to help you there. He released a ton of CV model weights trained on TPUs a couple weeks ago.
1980,@marktenenholtz,2022-04-18 22:33:42+00:00,https://twitter.com/marktenenholtz/status/1516183215370821638,"@JFPuget Don’t have hard numbers but TPUs are stupid fast when you make the inputs the perfect shape and whatnot.

The problem is they suck with PyTorch and the A100 is way more flexible to things the TPU can’t handle at all (perfect example is Longformer)"
1981,@marktenenholtz,2022-04-18 15:47:34+00:00,https://twitter.com/marktenenholtz/status/1516081008680001543,@AtharvaIngle7 @jarvislabsai I love @jarvislabsai! Fantastic option for GPUs. The ability to pause instances is super powerful.
1982,@marktenenholtz,2022-04-18 15:36:03+00:00,https://twitter.com/marktenenholtz/status/1516078110143356931,"@rohanpaul_ai Data centers are not allowed to house consumer GPUs like the 30 series, so I didn’t list it. You won’t find those through major providers like AWS, GCP, and Azure."
1983,@marktenenholtz,2022-04-18 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1516023774579097608,"I hope you learned something!

Follow me @marktenenholtz for more actionable, high-signal ML content."
1984,@marktenenholtz,2022-04-18 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1516023771773034502,"Addendum: TPUs are incredible!

Yes, they're very expensive. 

But compared to GPUs, they cut through data like a hot knife through butter.

I'd highly recommend giving them a shot on Colab/Kaggle to see if they fit your use-case (they don't work with everything)."
1985,@marktenenholtz,2022-04-18 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1516023769051000840,"Production, need fast inference and/or huge models?

A100.

The price can be hard to swallow for production, but you can throw so much at this cards.

With batched inference, these cards are absolute beasts."
1986,@marktenenholtz,2022-04-18 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1516023766278553601,"Production, low budget, some latency concerns?

T4.

Inference was the main use-case for T4's when they were released.

It's hard to get better than a cheap, low-power card with tensor cores and plenty of memory."
1987,@marktenenholtz,2022-04-18 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1516023763573174275,"Production, tight budget, no latency concerns?

Don't use a GPU!

You'd be surprised what you can get away with just serving your model on CPUs.

Even better: deploy everything on something like AWS Lambda or Azure Functions and you'll pay next to nothing."
1988,@marktenenholtz,2022-04-18 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1516023760838569993,"Just experimenting, but need a lot of compute? 

A100.

I skipped over V100s because I think A100s are just simply more efficient.

They're only ~$0.50 more per card usually but give you way more memory and computing power."
1989,@marktenenholtz,2022-04-18 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1516023758082912262,"Just experimenting, but have some cash? 

T4.

T4's aren't the most powerful, but they're super cheap and they have tensor cores.

At $0.35 per card on GCP, why not use 2 or 4 of them instead of a V100?"
1990,@marktenenholtz,2022-04-18 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1516023755268517894,"Just experimenting on a tight budget? 

P100.

You can find P100's for free on Kaggle and free/for a fixed cost on Google Colab.

They've got enough VRAM to fit most models and, while they don't get the boost from tensor cores, you can still use mixed precision."
1991,@marktenenholtz,2022-04-18 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1516023752521179137,"If you want to get serious about training big deep learning models, you'll need some serious compute.

However, knowing what GPU to rent can be very intimidating and hard to research.

So, here's the right GPU type for most situations you'll find yourself in 🧵"
1992,@marktenenholtz,2022-04-18 01:11:52+00:00,https://twitter.com/marktenenholtz/status/1515860633031516160,@LBacaj That looks so freaking good! Had smoked turkey and ham with my girlfriend’s family.
1993,@marktenenholtz,2022-04-17 22:59:54+00:00,https://twitter.com/marktenenholtz/status/1515827421488025603,"@LesGuessing @TheZachMueller I’ve found the same thing. It’s doable if you do a lot of inspecting elements in a live browser but still can be annoying.

Not sure if it’s really a flaw of BS4 or just a generally domain difficulty, though."
1994,@marktenenholtz,2022-04-17 22:13:58+00:00,https://twitter.com/marktenenholtz/status/1515815862866960384,@chrisalbon Pronounced “Chris or Albon”
1995,@marktenenholtz,2022-04-17 17:39:59+00:00,https://twitter.com/marktenenholtz/status/1515746909222223873,@AlejandroPiad Easy choice! Can’t wait to see what you put out!
1996,@marktenenholtz,2022-04-17 16:28:59+00:00,https://twitter.com/marktenenholtz/status/1515729043420819470,@RDub2 Yes. Good solution engineering is key
1997,@marktenenholtz,2022-04-17 12:24:36+00:00,https://twitter.com/marktenenholtz/status/1515667540462166018,"@AlejandroPiad @statsepi Don’t worry, I have enough empty-sounding hot takes for the both of us :)"
1998,@marktenenholtz,2022-04-17 12:14:30+00:00,https://twitter.com/marktenenholtz/status/1515665002568597511,@AlejandroPiad @statsepi Exactly. The only thing that makes all that theory that you know worth anything is your ability to manifest it through a built product.
1999,@marktenenholtz,2022-04-17 11:30:39+00:00,https://twitter.com/marktenenholtz/status/1515653965849014278,"@statsepi Sometimes we all say things we regret, and this is one of them for me.

You obviously need theory to do a good job, but I think you can teach anyone theory. 

It’s much harder to teach someone to be a builder. That’s a personality trait."
2000,@marktenenholtz,2022-04-17 11:20:14+00:00,https://twitter.com/marktenenholtz/status/1515651345629519874,@ivanlenin I’ve found the best way to learn theory is to keep building. The math begins to make more sense when I see it play out in real time.
2001,@marktenenholtz,2022-04-16 21:26:04+00:00,https://twitter.com/marktenenholtz/status/1515441420789854216,"@SHussainAther Sir, this is twitter"
2002,@marktenenholtz,2022-04-16 19:32:54+00:00,https://twitter.com/marktenenholtz/status/1515412940601634819,"@SenecaWidvey I disagree. I love interviewing data scientists with no job experience that have built a couple of personal projects.

That’s a much better way to prove that you’re a builder than just having taken classes, and no job experience necessary."
2003,@marktenenholtz,2022-04-16 19:03:53+00:00,https://twitter.com/marktenenholtz/status/1515405638318407685,"@ivanlenin I’ll rephrase to what I meant to get across: every data scientist needs a base level of theory. Beyond that, all that matters is your ability to build"
2004,@marktenenholtz,2022-04-15 18:56:38+00:00,https://twitter.com/marktenenholtz/status/1515041425599217675,@mervenoyann I’d kill for some Turkish coffee again. Absolutely love it!
2005,@marktenenholtz,2022-04-15 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1514936586751803398,"Machine learning projects are ridiculously complex.

And, you probably don't have a photographic memory.

So, document:

• Your tech debt
• Your ETL pipeline
• Your ideas for improvement
• Your experiments
• Your assumptions
• Your potential risks

Thank yourself later."
2006,@marktenenholtz,2022-04-14 18:11:52+00:00,https://twitter.com/marktenenholtz/status/1514667772202078210,"@gdb There’s good complexity and there’s bad complexity.

Saving 10 keystrokes for 5x higher cognitive load is the bad kind."
2007,@marktenenholtz,2022-04-14 16:42:15+00:00,https://twitter.com/marktenenholtz/status/1514645220562030593,@HamelHusain @radekosmulski Do you think dplyr syntax in R is any better? I’ve always felt that it feels a bit too “magicky”
2008,@marktenenholtz,2022-04-14 16:30:54+00:00,https://twitter.com/marktenenholtz/status/1514642360797442048,"@HamelHusain @radekosmulski Yeah I love pandas but even I can't get over how annoyingly redundant the filtering syntax is.

What's your favorite non-SQL API for data processing tasks?"
2009,@marktenenholtz,2022-04-14 16:26:41+00:00,https://twitter.com/marktenenholtz/status/1514641300846489608,"@HamelHusain @radekosmulski I'm totally ignorant to the engineering complexity under the hood, but I'm curious why this has to be so different than cuDF."
2010,@marktenenholtz,2022-04-14 16:21:51+00:00,https://twitter.com/marktenenholtz/status/1514640083084886021,"@JustinSaaS It’s sad how jargon is seen as a status symbol among us techies. 

I learn 10x more from clearly-stated, intuitive communication. Especially when it’s about tough, highly-technical topics."
2011,@marktenenholtz,2022-04-14 13:11:03+00:00,https://twitter.com/marktenenholtz/status/1514592067023953921,@DavidOkpare One of my favorite quotes of all time
2012,@marktenenholtz,2022-04-14 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1514574219803172866,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
2013,@marktenenholtz,2022-04-14 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1514574217093681152,"TLDR:

1. Stakeholder management
2. Raw data EDA
3. Strong baselines
4. Demo tools
5. Deliver, deliver, deliver

Follow me @marktenenholtz for more high-signal ML content!"
2014,@marktenenholtz,2022-04-14 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1514574214375772161,"5. Deliver, deliver, deliver

Don't be a perfectionist. 

An imperfect but well-evaluated model is worth delivering early.

A working model &gt; no model.

Start making your team $$$ early while you work on improvements in the background."
2015,@marktenenholtz,2022-04-14 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1514574211620098049,"4. Use a low-code demo tool

Demo tools like @Gradio and @streamlit are ridiculously powerful.

The faster it is to play with your models, the less time you'll spend figuring out how to improve them."
2016,@marktenenholtz,2022-04-14 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1514574208843485190,"3. A strong baseline gets you 90% of the way

Using a ResNet on image data, XGBoost on tabular data, and RoBERTa on text data is pretty much all you need.

Instead of fiddling with architectures, spend your valuable time perfecting your data prep."
2017,@marktenenholtz,2022-04-14 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1514574206079406090,"2. Start your EDA with the raw data

Plotting and aggregations inherently remove context.

You should spend 10+ hours going through individual data points.

Every minute you spend understanding the data at first is paid back 10-fold in reduced dev time."
2018,@marktenenholtz,2022-04-14 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1514574203315376134,"1. Stakeholders are your best friend

Stakeholders and SMEs save you tons of time.

Picking their brain will give you invaluable data context and solution ideas.

Skimp on them at your own peril."
2019,@marktenenholtz,2022-04-14 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1514574200576512000,"Your boss probably values your yearly contributions at 3x your salary.

So, if you make the median machine learning engineer salary and cut 1/3 of your dev time on each project, that's a 6 figure value.

That's my goal with these tips."
2020,@marktenenholtz,2022-04-14 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1514574197875302400,"5 machine learning tips worth $100,000+/DS/year:"
2021,@marktenenholtz,2022-04-13 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1514211841827696644,"A junior data scientist is learning how to make more complex models.

A senior data scientist is learning how to make simpler models."
2022,@marktenenholtz,2022-04-12 16:05:17+00:00,https://twitter.com/marktenenholtz/status/1513911139419230218,@simonsarris The madman actually did it!
2023,@marktenenholtz,2022-04-12 15:52:26+00:00,https://twitter.com/marktenenholtz/status/1513907905711509510,@simonsarris https://t.co/BGefbEGAa0
2024,@marktenenholtz,2022-04-12 15:34:40+00:00,https://twitter.com/marktenenholtz/status/1513903436093431811,@tomjacquesson Buy a Herman Miller Aeron off of eBay. They last forever and are incredibly nice and ergonomic
2025,@marktenenholtz,2022-04-12 13:30:57+00:00,https://twitter.com/marktenenholtz/status/1513872300524908547,@Richard_thaler1 Using courses is still great as long as you go and build something with what you learned. Paying for them is also great as long as you spend your money wisely
2026,@marktenenholtz,2022-04-12 12:03:04+00:00,https://twitter.com/marktenenholtz/status/1513850186006675458,@LBacaj @GergelyOrosz It’s the only way to experience dysfunction *at scale*
2027,@marktenenholtz,2022-04-12 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1513849466515718145,"7 steps to solving ML problems:

1. Problem formulation
2. Data collection
3. Data preparation
4. Model training
5. Model evaluation
6. Model deployment
7. Model monitoring

Rely on courses, and you'll learn maybe 2 or 3 of those.

Rely on building, and you'll learn it all."
2028,@marktenenholtz,2022-04-11 23:42:43+00:00,https://twitter.com/marktenenholtz/status/1513663869566210059,@prthgo I’m not the type of psycho that would share a TF-only course
2029,@marktenenholtz,2022-04-11 14:17:33+00:00,https://twitter.com/marktenenholtz/status/1513521640264585216,@dickiebush The worst part of writing on my desktop compared to my laptop/iPad is that I haven't found a Windows bullet point shortcut as easy as cmd+8
2030,@marktenenholtz,2022-04-11 14:01:58+00:00,https://twitter.com/marktenenholtz/status/1513517720171646982,"@JustinSaaS The wildest part of this isn't the strategies.

It's how incredible exponential growth is."
2031,@marktenenholtz,2022-04-11 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1513487072635490306,"YouTube link: https://t.co/3riJEfBFFS

If you're a UMich alum like me, you can view an even more up-to-date version here: https://t.co/ILP3onLlPt"
2032,@marktenenholtz,2022-04-11 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1513487069145751557,"Computer vision is one of the most exciting areas of machine learning.

No matter your skill level, here’s my favorite computer vision course.

(And, of course, it’s 100% free!)

University of Michigan posted this full course for your enjoyment on YouTube: https://t.co/gH7v3rKQxR"
2033,@marktenenholtz,2022-04-10 23:59:09+00:00,https://twitter.com/marktenenholtz/status/1513305617427050504,"@OneJKMolina What’s one piece of advice you would give to Twitter creators in very technical fields (machine learning, programming, etc.)?"
2034,@marktenenholtz,2022-04-10 12:32:55+00:00,https://twitter.com/marktenenholtz/status/1513132919237365760,@dvassallo Same thing with their EIN tool. It’s ridiculous.
2035,@marktenenholtz,2022-04-10 12:28:25+00:00,https://twitter.com/marktenenholtz/status/1513131788251635715,"@tunguz Same reason that wise, centralized leadership is crucial at institutions.

The problem is, it only works until you have 1 bad leader."
2036,@marktenenholtz,2022-04-10 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1513124645955797001,"If you're struggling to improve your models:

• Take a breath
• Look through individual predictions
• Find particularly bad errors
• Take a walk
• Verbalize what your model is missing
• Find a way to represent that thing

ML is iterative. Perfect your process."
2037,@marktenenholtz,2022-04-09 18:22:04+00:00,https://twitter.com/marktenenholtz/status/1512858398009745410,"@LBacaj Ha, in some ways it's like working a boring job to others. 
Nobody understands it so if you bring it up in conversation to non-technical folks, they tend to just go ""oh, okay."""
2038,@marktenenholtz,2022-04-09 13:22:23+00:00,https://twitter.com/marktenenholtz/status/1512782982419042310,@IttaiSvidler Better yet — no meetings at all!
2039,@marktenenholtz,2022-04-09 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1512762262523564032,"What people think ML is currently useful for:

• Superhuman performance
• Causal relationships
• Solving difficult problems

In reality, it's:

• Superhuman scalability
• Correlative relationships
• Solving constrained problems

We will get there. 

But we aren't there."
2040,@marktenenholtz,2022-04-08 22:00:18+00:00,https://twitter.com/marktenenholtz/status/1512550931140911108,"@fchollet This is a skill that isn't selected for nearly enough in industry.

Companies could massively increase the value of their internal tools if they had product-minded programmers that wrote kick-ass docs."
2041,@marktenenholtz,2022-04-08 18:50:14+00:00,https://twitter.com/marktenenholtz/status/1512503098303991810,"@tunguz ""When trapped, a company will even gnaw off it's own Arm"""
2042,@marktenenholtz,2022-04-08 12:17:52+00:00,https://twitter.com/marktenenholtz/status/1512404357299556353,@tunguz @kaggle Could do something like shinyapps
2043,@marktenenholtz,2022-04-08 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1512399882731941889,"It costs $0.00 to learn machine learning nowadays.

You can:

• Use Kaggle to scrape your data
• Use Kaggle to clean your data
• Use Kaggle to build your model
• Use Kaggle to rerun your model

Kaggle isn't just for competitions.

Take full advantage of it."
2044,@marktenenholtz,2022-04-07 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1512037498171523076,"Don’t rely on work ethic to learn data science.

Instead,

• Learn about what interests you
• Double down, learn more about it
• Teach it so it’s reinforced
• Find the next exciting thing
• Repeat

Always keep your learning fresh and exciting, never dull and repetitive."
2045,@marktenenholtz,2022-04-06 20:46:40+00:00,https://twitter.com/marktenenholtz/status/1511807623917674503,@LBacaj @EngAdvicePod This humble machine learning engineer would be honored
2046,@marktenenholtz,2022-04-06 20:42:31+00:00,https://twitter.com/marktenenholtz/status/1511806581901508618,@LBacaj @EngAdvicePod That was an insta-subscribe
2047,@marktenenholtz,2022-04-06 17:13:35+00:00,https://twitter.com/marktenenholtz/status/1511754001821360136,"@WrongsToWrite @OneJKMolina Yeah you just have to do it like once or twice, right?"
2048,@marktenenholtz,2022-04-06 15:35:33+00:00,https://twitter.com/marktenenholtz/status/1511729332028325891,@tunguz https://t.co/GFIAcquzYr
2049,@marktenenholtz,2022-04-06 15:33:41+00:00,https://twitter.com/marktenenholtz/status/1511728862060855297,@tunguz space cowboy.
2050,@marktenenholtz,2022-04-06 13:30:38+00:00,https://twitter.com/marktenenholtz/status/1511697894176436229,@TheZachMueller 10’s are brutal. Did 5/3/1 for a while with the 5 sets of 10 @ 60% after your heavy sets for a while and genuinely wanted to die
2051,@marktenenholtz,2022-04-06 13:21:41+00:00,https://twitter.com/marktenenholtz/status/1511695642355191810,@TheZachMueller Just took a week off and now 245x5 squats are kicking my ass 😢
2052,@marktenenholtz,2022-04-06 12:12:51+00:00,https://twitter.com/marktenenholtz/status/1511678317983903751,@0xbnomial Do I have to send a resume and cover letter
2053,@marktenenholtz,2022-04-06 12:10:32+00:00,https://twitter.com/marktenenholtz/status/1511677736150052864,"@ykilcher Cmon guys, April fools was 5 days ago…"
2054,@marktenenholtz,2022-04-06 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1511675141142433800,"How not to think about feature engineering:

Descriptive stats on every feature, every interaction term possible, hope the model figures it out

How you should think about it:

Acting as a translator to represent the data in a language your model speaks well."
2055,@marktenenholtz,2022-04-05 22:13:14+00:00,https://twitter.com/marktenenholtz/status/1511467022131122178,@0xbnomial Too much heat in the kitchen
2056,@marktenenholtz,2022-04-05 19:21:38+00:00,https://twitter.com/marktenenholtz/status/1511423837132537864,"@svpino f-strings are love.

f-strings are life."
2057,@marktenenholtz,2022-04-05 15:51:06+00:00,https://twitter.com/marktenenholtz/status/1511370853962993664,@svpino @AlejandroPiad Y’all can’t have one without me
2058,@marktenenholtz,2022-04-05 14:37:40+00:00,https://twitter.com/marktenenholtz/status/1511352373754699785,@dickiebush Member of the lefty club or am I less observant than I think I am?
2059,@marktenenholtz,2022-04-05 12:19:08+00:00,https://twitter.com/marktenenholtz/status/1511317512570523651,"@prthgo Were you using joblib or pickle? 

A lot of it can come down to the compression settings and min_samples_split since you can make a shitload of leaves if min_samples_split is small."
2060,@marktenenholtz,2022-04-05 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1511312746687594499,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
2061,@marktenenholtz,2022-04-05 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1511312743835398144,"TLDR:

1. Deep-dive the data
2. Evaluation first
3. Get a baseline
4. Rapid iteration
5. Test scaling

Follow me @marktenenholtz for more high-signal ML content!"
2062,@marktenenholtz,2022-04-05 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1511312738068312067,"Start small and work quickly

The best models are built by the data scientist that explored the most possibilities. 

Rapid iteration is key.

Exhaust everything you can think of in a way that gives you rapid feedback (&lt;10-15 min)."
2063,@marktenenholtz,2022-04-05 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1511312735274889216,"Get a baseline

Your error metrics are meaningless in isolation.

You should know two things for context:

• How accurate is a naive model?
• How accurate is accurate enough?

If you can't beat these goals, then you're wasting your time.

So you should know them ahead of time."
2064,@marktenenholtz,2022-04-05 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1511312732523356167,"You can't train until you can evaluate

If you did great EDA, you should have an idea of how to set up your evaluation.

Again, don't build models until you do this.

Evaluation gives you rapid feedback on your models, and rapid feedback is the most valuable info you can have."
2065,@marktenenholtz,2022-04-05 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1511312729755119616,"Start by deep-diving the data

Any time spent building a model before thorough EDA is a complete waste.

Every minute spent doing EDA is paid back 100-fold in every future step.

Make the investment and you'll save time not only building your model, but also productionalizing it."
2066,@marktenenholtz,2022-04-05 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1511312726923964421,"I've spent 1000's of hours building ML models over the last couple years.

Truth is, 100's of those hours were wasted because I was working inefficiently.

Here are 5 actionable tips to produce better results more efficiently."
2067,@marktenenholtz,2022-04-05 00:51:52+00:00,https://twitter.com/marktenenholtz/status/1511144556305297410,@Johns_Analytics There’s a ton of gated knowledge in the betting world. Most folks are much more naive to it than they realize.
2068,@marktenenholtz,2022-04-04 20:26:15+00:00,https://twitter.com/marktenenholtz/status/1511077711913144334,@Giba1 @bhutanisanyam1 I think the sklearn model to use now is the HistGradientBoosting family of algo's. It's more comparable to XGB/LGB/CB
2069,@marktenenholtz,2022-04-04 18:06:28+00:00,https://twitter.com/marktenenholtz/status/1511042534067290126,@Jen4SenNC Human-in-the-loop is criminally underused too
2070,@marktenenholtz,2022-04-04 18:04:51+00:00,https://twitter.com/marktenenholtz/status/1511042128230686727,"@abhi1thakur It’s even surprisingly competitive on medium+ sized datasets. Not enough to beat GBDTs, but great for an ensemble"
2071,@marktenenholtz,2022-04-04 17:47:28+00:00,https://twitter.com/marktenenholtz/status/1511037752506257410,@SashaMTL Keep fighting the good fight! We need you!
2072,@marktenenholtz,2022-04-04 17:21:56+00:00,https://twitter.com/marktenenholtz/status/1511031324903100420,"@RoxanaDaneshjou We need a lot more of it in every field. It may be there in medicine, but it’s very absent in most other fields."
2073,@marktenenholtz,2022-04-04 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1510950320536363008,"Data scientists: ""I'd like to start deploying big models""

Academics: ""No, that's dangerous, you haven't properly evaluated your model and it could have terrible consequences""

Data scientists: ""Can you teach me how to evaluate my models?""

Academics: ""Teach you how to what?"""
2074,@marktenenholtz,2022-04-04 11:39:06+00:00,https://twitter.com/marktenenholtz/status/1510945051756670979,@abhi1thakur I tried it as a boosting model on top of CNN embeddings in the Petfinder competition and it worked better than GBDTs (the RAPIDS version of course)
2075,@marktenenholtz,2022-04-03 17:56:46+00:00,https://twitter.com/marktenenholtz/status/1510677706010152969,"Thinking of running an ML modeling cohort class to show the world all the things you can only learn on Kaggle.

The knowledge is so locked away that I'm actually seeing people say that Kaggle experience isn't too useful. Crazy."
2076,@marktenenholtz,2022-04-03 15:40:10+00:00,https://twitter.com/marktenenholtz/status/1510643326856445954,@AlejandroPiad I actually lol’d
2077,@marktenenholtz,2022-04-03 15:39:30+00:00,https://twitter.com/marktenenholtz/status/1510643159998648321,"@dvassallo I’m slowly learning that expressing any consistent worldview on this platform will be controversial to some group.

Ignoring their negativity is the only way to provide the value you hope to provide."
2078,@marktenenholtz,2022-04-03 15:01:51+00:00,https://twitter.com/marktenenholtz/status/1510633684210176007,@mervenoyann @ali_safaya @ardofski @emirhan_krtls Nice! I’ve wanted to make one of these for Hebrew
2079,@marktenenholtz,2022-04-03 14:54:59+00:00,https://twitter.com/marktenenholtz/status/1510631958786691077,@parker_brydon @tunguz Those things are required in nearly every competition. That’s why we think these arguments that say kaggle isn’t that useful are so silly.
2080,@marktenenholtz,2022-04-03 14:49:00+00:00,https://twitter.com/marktenenholtz/status/1510630449986551808,@willmcgugan This library is a blessing
2081,@marktenenholtz,2022-04-03 14:46:30+00:00,https://twitter.com/marktenenholtz/status/1510629822040580108,@tunguz I’ve seen it happen
2082,@marktenenholtz,2022-04-03 14:39:14+00:00,https://twitter.com/marktenenholtz/status/1510627994905288712,@tunguz Yeah but eking out 1e-5 more MSE doesn’t translate to real life
2083,@marktenenholtz,2022-04-03 13:54:33+00:00,https://twitter.com/marktenenholtz/status/1510616747380035592,@parker_brydon Model evaluation is a small part of the puzzle?
2084,@marktenenholtz,2022-04-03 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1510587930666213377,"If a company tells you Kaggle competitions aren't good experience, run for your life.

Then, watch their data science function crumble from afar."
2085,@marktenenholtz,2022-04-03 00:20:26+00:00,https://twitter.com/marktenenholtz/status/1510411868808044548,@bnomialml @tunguz Who is behind the madness that is this account
2086,@marktenenholtz,2022-04-02 22:49:44+00:00,https://twitter.com/marktenenholtz/status/1510389044051652619,@svpino @bnomialml https://t.co/81znaheQan
2087,@marktenenholtz,2022-04-02 22:43:55+00:00,https://twitter.com/marktenenholtz/status/1510387580231114762,@bnomialml 😎
2088,@marktenenholtz,2022-04-02 15:39:43+00:00,https://twitter.com/marktenenholtz/status/1510280826549948417,"@parker_brydon I have not, but I plan to"
2089,@marktenenholtz,2022-04-02 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1510225559485181960,"Want to be a better data scientist?

Surround yourself with them.

The best ways:

• Meetups
• Kaggle discussions
• University/work groups
• Paper sharing groups

The field moves too quickly to keep up by yourself, so don't be a lone wolf. Do it with friends."
2090,@marktenenholtz,2022-04-02 01:26:06+00:00,https://twitter.com/marktenenholtz/status/1510066005736927232,@bhutanisanyam1 @tunguz Man you had me for a second
2091,@marktenenholtz,2022-04-01 18:18:17+00:00,https://twitter.com/marktenenholtz/status/1509958344794152968,@LBacaj You’re an inspiration to us all Louie! Keep it up!
2092,@marktenenholtz,2022-04-01 13:27:33+00:00,https://twitter.com/marktenenholtz/status/1509885177627095048,@svpino This is awesome! Well done!
2093,@marktenenholtz,2022-04-01 12:21:17+00:00,https://twitter.com/marktenenholtz/status/1509868503792242689,"@LBacaj A little change of environment, a little sun, and a little movement is crazy powerful"
2094,@marktenenholtz,2022-04-01 12:20:30+00:00,https://twitter.com/marktenenholtz/status/1509868305091289092,@svpino Talk about a way to reach a flow state every day. That’s awesome!
2095,@marktenenholtz,2022-04-01 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1509863157786767363,"Your options for solving hard ML problems:

• Hours in front of a monitor

or

• Minutes on a walk

When you're stuck, change your location, not your Chrome tab."
2096,@marktenenholtz,2022-03-31 22:14:13+00:00,https://twitter.com/marktenenholtz/status/1509655329562406915,@rasbt @unfoldds @Richard_thaler1 You’re my one exception as far as quality of teaching model evaluation
2097,@marktenenholtz,2022-03-31 20:59:42+00:00,https://twitter.com/marktenenholtz/status/1509636578217537538,@ykilcher @giffmana @daidailoh @PiotrPadlewski If you can beat me at Mario Tennis I’ll include you in all of my related work sections
2098,@marktenenholtz,2022-03-31 14:35:23+00:00,https://twitter.com/marktenenholtz/status/1509539863124750347,@tunguz More like “way to mo” amirite?
2099,@marktenenholtz,2022-03-31 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1509500817602015232,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
2100,@marktenenholtz,2022-03-31 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1509500814783512576,"TL;DR:

1. Seasonality
2. Trends
3. Reactiveness
4. Holidays
5. Bias
6. Accuracy by volume
7. Drift

Follow me @marktenenholtz for more high-signal ML content!"
2101,@marktenenholtz,2022-03-31 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1509500812053008389,"7. Drift

Be sure to keep updated metrics on how your model performs in production!

If you're finding that it starts to drift too quickly, you might consider increasing the length of your roll-forward evaluation windows."
2102,@marktenenholtz,2022-03-31 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1509500809251237892,"6. Accuracy by volume

If you have multiple time series, make sure to separate your evaluation.

Keep the series that are high-volume and regular in one group, and highly intermittent, low-volume series in another."
2103,@marktenenholtz,2022-03-31 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1509500806445166597,"5. Bias

Check to see if your model is consistently over-/under-forecasting.

Modern ML models don't usually struggle with this, but if they do, you may just need more data.

A common issue that can cause this is poor data quality, though, so be sure to check that."
2104,@marktenenholtz,2022-03-31 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1509500803639250956,"4. Holidays

Don't just check the effect of holidays on the day of the holiday.

Make sure that you're checking the period around them.

If your model struggles just before and after holidays, try adding features that tell your model that's it's close to a holiday."
2105,@marktenenholtz,2022-03-31 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1509500798136320001,"3. Reactiveness

This is the ability of your model to quickly react to changes not caused by a trend or cycle.

If your model is slow to react to sudden changes, try adding short-term rolling/lag features.

If it overreacts, try adding longer-term rolling/lag features."
2106,@marktenenholtz,2022-03-31 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1509500792683728904,"2. Trends

Make sure your model can follow general rising/falling trends!

Even better -- make sure your model doesn't perform better/worse while either sort of trend is happening.

EVEN better -- check all of the other characteristics during rising/falling trends."
2107,@marktenenholtz,2022-03-31 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1509500789953208320,"1. Seasonality

Many time series behave differently based on time, whether it's weekly, monthly, or any recurring cycle.

Break your error out by each of these cycles.

If you struggle to capture seasonality, try adding more time-based features."
2108,@marktenenholtz,2022-03-31 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1509500787189190658,"Error analysis, 101: Time-series data

The BEST way to improve ML models is with error analysis -- seeing how your model screws up.

The problem?

Error analysis is really hard, and good info on it is hard to find.

So here are the 7 powerful, actionable methods I discovered:"
2109,@marktenenholtz,2022-03-30 21:57:59+00:00,https://twitter.com/marktenenholtz/status/1509288856562057218,"@_cgustavo @gbayomi @svpino Only been at it for ~4 months, but thanks!"
2110,@marktenenholtz,2022-03-30 21:08:10+00:00,https://twitter.com/marktenenholtz/status/1509276319720353794,"@michaelaye @mervenoyann Usually,

Structured == tabular

Unstructured == images, text, etc"
2111,@marktenenholtz,2022-03-30 16:55:24+00:00,https://twitter.com/marktenenholtz/status/1509212709446893570,"@mervenoyann Agree with your points 100%, and I'd also add that most companies start with something more like data analysis before diving into ML.

Analysts usually require structured data, so companies build their whole strategy around that and it's hard to change that once they start."
2112,@marktenenholtz,2022-03-30 14:39:40+00:00,https://twitter.com/marktenenholtz/status/1509178552054341632,"@Nils_Reimers Your points are useful corollaries, but I’m not sure you really disagree with me here. 

Sure, you can slice and dice by industry all you want, but I’m looking at the problems data scientists face on a whole. Most don’t work in a field with a heavy research focus."
2113,@marktenenholtz,2022-03-30 12:15:08+00:00,https://twitter.com/marktenenholtz/status/1509142179641503750,@tunguz Yeah… you’re probably right…
2114,@marktenenholtz,2022-03-30 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1509138378024898560,"Hard truth:

ML researchers focus on:

• 90% unstructured data
• 10% structured data

Problems in industry are:

• 10% unstructured data
• 90% structured data

It's exciting that industry is moving more towards unstructured data.

But remember: tabular data is still king."
2115,@marktenenholtz,2022-03-30 01:47:17+00:00,https://twitter.com/marktenenholtz/status/1508984175809990656,"@TheZachMueller Stop this ma… wait, I’m intrigued…?"
2116,@marktenenholtz,2022-03-29 16:03:51+00:00,https://twitter.com/marktenenholtz/status/1508837350591315972,"@andrehaykaljr Exactly.

Freedom is achieved through constraints.

Development is achieved by experimenting with the rest.

Look for contradictions, and experiment with both sides!"
2117,@marktenenholtz,2022-03-29 12:00:16+00:00,https://twitter.com/marktenenholtz/status/1508776049802240002,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
2118,@marktenenholtz,2022-03-29 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1508776046958571523,"Learning ML is about starting small and reaching a point of exponential growth.

Project-based learning is how the best data scientists fly past the rest of the field.

Start doing it. Now.

Follow me @marktenenholtz for more high-signal ML content!"
2119,@marktenenholtz,2022-03-29 12:00:15+00:00,https://twitter.com/marktenenholtz/status/1508776044139929602,"Work on what you struggled with in your last project

You should never feel lost about what to learn next.

This is why reflecting on your projects is key!

If you remember what you struggled with and prioritize that in your future projects, you'll never run out of ideas."
2120,@marktenenholtz,2022-03-29 12:00:14+00:00,https://twitter.com/marktenenholtz/status/1508776041401094148,"Write about what you learned

Building a project requires getting deep into the details.

To get the most out of a project, you can't lose the forest in the trees.

instead, compress what you've learned by reflecting deeply at the end, and the best way to do this is by writing."
2121,@marktenenholtz,2022-03-29 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1508776038628614144,"Aim for a challenge

Don't pick a project that looks easy.

Pick one that will challenge you enough to keep you interested, but not too much that it turns you off.

Keep yourself in the zone of proximal development."
2122,@marktenenholtz,2022-03-29 12:00:13+00:00,https://twitter.com/marktenenholtz/status/1508776035856228354,"Focus on depth before breadth

You should learn in a T-shape.

Focus on building strong skills in a subset of tasks, rather than becoming decent at everything."
2123,@marktenenholtz,2022-03-29 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1508776033008308225,"Have a clear end in sight

Much like a painting, it's unclear when a project is done.

This isn't just true in personal projects, though. It's also true in industry.

Set some clear goals as acceptance criteria. It's great real-life project practice."
2124,@marktenenholtz,2022-03-29 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1508776030185484294,"Always finish what you start

80% of the learning is in the last 20% of the project.

It's not the boilerplate that teaches you, it's the rough ends that are entirely project-specific.

If you don't think you'll want to see a specific project through, don't start it."
2125,@marktenenholtz,2022-03-29 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1508776027220324354,"Build something you find useful

The best way to build a motivating project is by solving a need.

If you're not selling something, then you have a customer base of 1:

You.

You'll learn about building products which, in the end, is what keeps data scientists employed."
2126,@marktenenholtz,2022-03-29 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1508776021356531713,"The best way to 10x your data science skills is by building.

Building is irreplaceable.

Courses/bootcamps can never teach you what building does.

The problem? It's only useful if you do it, and most have no idea how to.

8 tips for building incredible ML projects:"
2127,@marktenenholtz,2022-03-28 22:48:39+00:00,https://twitter.com/marktenenholtz/status/1508576834170064899,"@giffmana The biggest regret of my Twitter career is missing 6969. 
Thanks for carrying the torch... I'm living vicariously through you"
2128,@marktenenholtz,2022-03-28 22:44:58+00:00,https://twitter.com/marktenenholtz/status/1508575905710579714,@giffmana 🥲 https://t.co/jsPiZsSrIW
2129,@marktenenholtz,2022-03-28 13:50:03+00:00,https://twitter.com/marktenenholtz/status/1508441290987511812,"@__mharrison__ Unfortunately, there's little to no good material out there on it. It's so problem-specific that it's hard to write good general material on it.

The best way to build the skill is to solve problems and try to dig into what's going wrong with your model (i.e. kaggle competitions)"
2130,@marktenenholtz,2022-03-28 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1508413629216874496,"Error analysis is to data scientists

As test-driven development is to devs"
2131,@marktenenholtz,2022-03-27 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1508051222409842691,"I used to think model evaluation and error analysis were boring.

Now I don’t understand how anyone can approach an ML problem without obsessing over it.

Don’t fly blind."
2132,@marktenenholtz,2022-03-26 23:43:33+00:00,https://twitter.com/marktenenholtz/status/1507865874199465989,@PathikGhugare I’m a Kaggler 🤷‍♂️
2133,@marktenenholtz,2022-03-26 13:40:13+00:00,https://twitter.com/marktenenholtz/status/1507714037798346756,@ThisIsBernardo Can’t use nor sell a model if you have no idea how good it is!
2134,@marktenenholtz,2022-03-26 13:26:23+00:00,https://twitter.com/marktenenholtz/status/1507710557243203600,@TheZachMueller Lol whoever is saying either of those things is off their rocker
2135,@marktenenholtz,2022-03-26 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1507688839149617163,"The WRONG way to significantly improve your models:

• Different architecture
• Hyperparameter tuning
• Bigger ensemble

The RIGHT way:

• Thorough EDA
• Meticulous error analysis
• Robust model evaluation

Spend your time on the 100%+ gains, not the 1% gains."
2136,@marktenenholtz,2022-03-26 01:00:12+00:00,https://twitter.com/marktenenholtz/status/1507522772410507265,@rishabh16_ We need PyCaret-like deployment ease in deep learning libraries
2137,@marktenenholtz,2022-03-25 19:13:47+00:00,https://twitter.com/marktenenholtz/status/1507435596406530049,@0xpookie AutoML doesn't really handle it at all
2138,@marktenenholtz,2022-03-25 12:44:33+00:00,https://twitter.com/marktenenholtz/status/1507337642513518609,@rasbt One of my favorite papers of all time!
2139,@marktenenholtz,2022-03-25 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1507326481784983555,Why don’t college machine learning degrees focus more on model evaluation?
2140,@marktenenholtz,2022-03-24 23:49:03+00:00,https://twitter.com/marktenenholtz/status/1507142482060054531,"@midickinson @mdancho84 Most that claim that are full of crap, but keep in mind they may only have a 1% edge on the market.

It takes a LOT of capital to be able to leverage that small of an edge."
2141,@marktenenholtz,2022-03-24 17:53:10+00:00,https://twitter.com/marktenenholtz/status/1507052920604438536,@prthgo Congrats!
2142,@marktenenholtz,2022-03-24 14:53:31+00:00,https://twitter.com/marktenenholtz/status/1507007708024639491,"@TheZachMueller If YOU failed, I'd say there's no disagreeing with this thread!

Someone send @jeremyphoward after your professor!"
2143,@marktenenholtz,2022-03-24 14:52:57+00:00,https://twitter.com/marktenenholtz/status/1507007565724454925,"@prthgo I did it for the credentials, basically same as you"
2144,@marktenenholtz,2022-03-24 14:52:16+00:00,https://twitter.com/marktenenholtz/status/1507007394915622935,"@tng_konrad Ha, a great point. I would say that falls under ""actually learn how to do EDA and appreciate data lineage"""
2145,@marktenenholtz,2022-03-24 14:51:07+00:00,https://twitter.com/marktenenholtz/status/1507007106242678791,@GuzmanOjero That's a great addition!
2146,@marktenenholtz,2022-03-24 14:49:54+00:00,https://twitter.com/marktenenholtz/status/1507006798867271681,"@Richard_thaler1 I said teaching it, not researching it"
2147,@marktenenholtz,2022-03-24 13:56:44+00:00,https://twitter.com/marktenenholtz/status/1506993419373142028,@OneJKMolina Check my banner
2148,@marktenenholtz,2022-03-24 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1506964077855186952,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
2149,@marktenenholtz,2022-03-24 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1506964074990497799,"TL;DR:

1. Don't frontload the math
2. ML is useless in a vacuum
3. Make your model convincing to use
4. Model evaluation &gt;&gt;&gt; model building
5. Never stop learning

I WISH I knew these things when I started learning ML.

Follow me @marktenenholtz for more high-signal ML content!"
2150,@marktenenholtz,2022-03-24 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1506964072209690624,"Staying up-to-date in ML is hard

Your college courses are out-of-date because the field moves so quickly.

Want to stay relevant in industry? 

Commit yourself to continual learning.

Seek out new models, MLOps tools, libraries, etc., and always be one step ahead."
2151,@marktenenholtz,2022-03-24 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1506964066769637376,"Your model is worthless if nobody uses it

If you can't communicate the value of your model, nobody will use it.

If nobody uses your model, then its development was an expensive waste of time.

Stakeholder management is crucial. Learning a bit of sales goes a long way."
2152,@marktenenholtz,2022-03-24 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1506964064022327296,"ML is only as useful as it creates value

Nobody cares if you apply an LSTM to a problem that could be solved by a heuristic. 

It's actually a bad thing.

Data scientists are hired for their problem-solving creativity first, and the ability to use ML to do it second."
2153,@marktenenholtz,2022-03-24 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1506964061195448326,"Learn the math as you go, not all at once

If you try to calculate the derivative of a convolution before you've applied them to a few problems, you're doing it wrong.

If you try to calculate XGBoost's boosting algorithm by hand before applying it, you're doing it wrong."
2154,@marktenenholtz,2022-03-24 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1506964058372640769,"Universities do a terrible job teaching machine learning.

Not only do they give you critically out-of-date information, but they focus most of their time on the least important aspects.

Here 5 things everyone in industry WISHES your professor taught you:"
2155,@marktenenholtz,2022-03-23 22:50:42+00:00,https://twitter.com/marktenenholtz/status/1506765410372407308,@tunguz I almost mentally corrected “Facebook” to “Meta” until I realized that you are 100% right
2156,@marktenenholtz,2022-03-23 19:34:20+00:00,https://twitter.com/marktenenholtz/status/1506715993212608517,@abhi1thakur @TheZachMueller Haha I tried it and it didn't work for me either -- what I do for that is highlight the text and then use cmd+/ (or ctrl+/)
2157,@marktenenholtz,2022-03-23 16:39:17+00:00,https://twitter.com/marktenenholtz/status/1506671939728904193,"Courses/textbooks/boot camps have value as informational tools, but true learning only comes through application.

Also — to those saying learning by itself is valuable:

It’s only as valuable as it helps you build better outcomes and products."
2158,@marktenenholtz,2022-03-23 14:15:12+00:00,https://twitter.com/marktenenholtz/status/1506635680235016194,"@LBacaj Any smart podcast host that wants to know about how successful engineering teams operate and how to chart your own path in business would be stupid to not want to learn from you.

Keep crushing it Louie!"
2159,@marktenenholtz,2022-03-23 12:45:01+00:00,https://twitter.com/marktenenholtz/status/1506612982477955073,"@TheZachMueller Imo, watching those videos can inform you, but to actually learn you need to find a way to apply that information.

I realized that the only reason I learned a ton from the fastai course was that I was constantly building while going through the course, not passively listening"
2160,@marktenenholtz,2022-03-23 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1506601675376660480,"Data scientists can only provide value 1 way:

Building.

You don't learn by reading textbooks, watching videos, taking courses, etc.

You learn by building."
2161,@marktenenholtz,2022-03-22 14:03:46+00:00,https://twitter.com/marktenenholtz/status/1506270413227233283,"@JustinSaaS Insane accomplishment Justin, well done! You’ve been a huge inspiration for me."
2162,@marktenenholtz,2022-03-22 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1506239315931631617,"If you like this content, I go into much more detail on this topic and similar ones in my newsletter.

Check it out: https://t.co/34sBfoLpMx"
2163,@marktenenholtz,2022-03-22 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1506239313205334016,"I hope you found value in these ramblings!

Follow me @marktenenholtz for more high-signal content on how to learn ML.

TL;DR:
• Become a builder
• Specialize
• Do the dirty work
• Learn the only way to make effective models
• Reflect, compress, and explain your learnings"
2164,@marktenenholtz,2022-03-22 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1506239310273527810,"Teach what you've learned to others

If you can't explain it simply, you don't understand it.

Nothing is too complicated to give a jargon-free explanation.

Step back, zoom out, and remember the purpose of everything that goes into the method."
2165,@marktenenholtz,2022-03-22 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1506239307488501760,"To help shape your mental map, ask yourself the following questions about each new method:

• What is it similar to? 
• How is it unique? 
• What are the limitations? 
• When is it the best solution? 

Mental context is important. Build it constantly over time."
2166,@marktenenholtz,2022-03-22 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1506239304586055681,"Build a mental map of your learnings

Most innovations in data science are slight changes from other methods.

Don't try to learn everything in isolation.

When you learn something new, spend some time comparing it to what you've already learned."
2167,@marktenenholtz,2022-03-22 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1506239301700497416,"Spend a ton of time on model evaluation and error analysis

Your models are practically useless if you can't get a reliable measure of their accuracy and you don't know how to improve them.

These 2 skills will improve your modeling skills astronomically."
2168,@marktenenholtz,2022-03-22 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1506239298143617025,"Collect your own data

Nothing gives you an appreciation for EDA like collecting your own data.

I know it sucks, but for as many projects as possible, collect your own raw data.

You'll learn more about EDA by doing that than any course or guide could ever teach you."
2169,@marktenenholtz,2022-03-22 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1506239295333433344,"Learn like a specialist

The field is expanding far too quickly to be a generalist.

If you try to be one, you will drown and take forever to build a skill that can actually drive value.

Instead, pick some topics that you love to specialize in.

You'll be marketable in no time."
2170,@marktenenholtz,2022-03-22 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1506239292472930306,"The next thing you should learn is the last thing you struggled with

If you focus on building, you'll never question what you should be learning next.

If your model sucked, find a better technique.
If your data was a mess, improve your ETL skills.

The cycle never stops."
2171,@marktenenholtz,2022-03-22 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1506239289704660994,"Building projects &gt; all else

Every ML technique is a means to an end.

If it isn't going to be used to build something that someone can use, it's useless.

So, if you aren't learning how to build projects, you're not really learning anything useful."
2172,@marktenenholtz,2022-03-22 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1506239286948995084,"Any online course is 2-3+ years behind the field

It takes long enough for the teacher to learn the material, put together a curriculum, validate it, and teach it that most courses you find are fairly out of date.

Another reason not to become reliant on bootcamps and courses."
2173,@marktenenholtz,2022-03-22 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1506239284147212288,"Bootcamps and courses are nothing more than introductions

There's no way to effectively teach a course on a technique/subject that captures all of the nuances in applying that thing.

The only way to figure them out is by applying your knowledge by building something."
2174,@marktenenholtz,2022-03-22 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1506239281412509696,9 guiding principles for learning data science:
2175,@marktenenholtz,2022-03-21 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1505876887112880130,"Wrong way to learn:

1. Learn the basics for everything
2. Learn the intermediates for everything
3. Become an expert in everything

Right way to learn:

1. Learn the basics
2. Find what you love
3. Become an expert in that
4. Repeat steps 2 &amp; 3

Specialization is a superpower."
2176,@marktenenholtz,2022-03-21 06:50:02+00:00,https://twitter.com/marktenenholtz/status/1505798873805201412,@LBacaj Yeah it’s served me well for learning anything I’ve applied it to
2177,@marktenenholtz,2022-03-20 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1505514499855462402,"Your 5-step guide to learning anything in data science:

1. Learn enough to start
2. Build
3. Build more
4. Build some more
5. Teach what you learned to someone else

The best data scientists are builders with deep knowledge. This is how to become that."
2178,@marktenenholtz,2022-03-19 15:27:49+00:00,https://twitter.com/marktenenholtz/status/1505204404240146435,@prthgo Just remember https://t.co/ZzNMygYtVQ
2179,@marktenenholtz,2022-03-19 13:36:17+00:00,https://twitter.com/marktenenholtz/status/1505176334607306759,"@svpino Hey, I’m Mark!

My missing is to unlock the gated knowledge of applied ML and bring it to the world, one thread at a time.

I want everyone to build better models, faster and easier."
2180,@marktenenholtz,2022-03-19 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1505152110496690181,"Machine learning superpower:

Deep knowledge of powerful tools.

Experience with the best frameworks lets you:

• Set up SOTA methods in hours
• Focus more on value and less on boilerplate
• Quickly go from R&amp;D to production

Go crush your problems, quickly."
2181,@marktenenholtz,2022-03-19 02:44:37+00:00,https://twitter.com/marktenenholtz/status/1505012335840309254,"@MeganRisdal @chrisalbon Oh you’ll be leaning into it. 

Every time you sit down!"
2182,@marktenenholtz,2022-03-18 21:55:00+00:00,https://twitter.com/marktenenholtz/status/1504939452447268868,@TobiOlabode3 When you’re running out of ideas or time
2183,@marktenenholtz,2022-03-18 19:03:52+00:00,https://twitter.com/marktenenholtz/status/1504896385686155265,https://t.co/lTM9SJ61Fw
2184,@marktenenholtz,2022-03-18 15:29:58+00:00,https://twitter.com/marktenenholtz/status/1504842553731989525,@KiroSasuke Sorry you didn’t like it
2185,@marktenenholtz,2022-03-18 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1504789728062627846,"The explore/exploit principle for data science:

Explore as many new ideas as you can.

Then, exploit the best one as much as possible.

Don't get lost exploring, and don't get stuck exploiting."
2186,@marktenenholtz,2022-03-17 15:27:15+00:00,https://twitter.com/marktenenholtz/status/1504479482031214606,"@rushworth_a While I understand what you're saying, prediction intervals are useless if your point forecast isn't accurate enough.

Uncertainty can be useful in some applications, point forecast accuracy is always essential."
2187,@marktenenholtz,2022-03-17 15:22:13+00:00,https://twitter.com/marktenenholtz/status/1504478219285655560,@rasbt @hardmaru I have not. Would love to see a well-formulated benchmark between that and the GBT method.
2188,@marktenenholtz,2022-03-17 15:13:21+00:00,https://twitter.com/marktenenholtz/status/1504475986624016390,"@rasbt @hardmaru In my experience (along with every other Kaggler) it's the only one that consistently produces top-tier results.

Sometimes, a really well-tuned LSTM/GRU/Transformer can outperform it.

The best overall in my experience though is ensembling a DL technique with XGBoost/LightGBM."
2189,@marktenenholtz,2022-03-17 13:31:18+00:00,https://twitter.com/marktenenholtz/status/1504450305731469314,"@JakeGearon @__mharrison__ Not a problem, you're not the first to make a mistake on the internet :)"
2190,@marktenenholtz,2022-03-17 13:19:46+00:00,https://twitter.com/marktenenholtz/status/1504447401733636100,"@__mharrison__ Yeah he hasn’t actually triggered the computation, has he?"
2191,@marktenenholtz,2022-03-17 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1504427379590017030,"Twitter threads only allow me to go into so much detail on complex, information-rich topics such as this.

I write about this kind of topic in my newsletter, so if you want more, subscribe! https://t.co/34sBfoLpMx"
2192,@marktenenholtz,2022-03-17 12:00:12+00:00,https://twitter.com/marktenenholtz/status/1504427376792387585,"TL;DR:

1. Empathize with the task
2. Anticipate future data
3. Copious error analysis
4. Don't overfit on your validation set
5. Verify your validation strategy

Follow me @marktenenholtz for more high-signal ML content."
2193,@marktenenholtz,2022-03-17 12:00:11+00:00,https://twitter.com/marktenenholtz/status/1504427373953163265,"5. Verify your validation strategy 

Before deploying your model, simulate its performance in production to check the robustness of your evaluation setup.

Test sets don't fully accomplish this since you may not have accounted for data differences from step number 2!"
2194,@marktenenholtz,2022-03-17 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1504427371297841160,This also goes for things like ReduceLROnPlateau in Keras/torch. Use a consistent scheduler instead!
2195,@marktenenholtz,2022-03-17 12:00:10+00:00,https://twitter.com/marktenenholtz/status/1504427368575758341,"An easy way of doing that:

1. Train your model on each fold
2. See how many training steps you took on each fold
3. Take the average number of training steps
4. Train your final model(s) for that average number of steps"
2196,@marktenenholtz,2022-03-17 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1504427365828505607,"To avoid this problem, don't use methods like early stopping that are contingent on model performance on a single validation fold.

Instead, in the case of early stopping, tune your training runs so that, on average, your last iteration is your best iteration."
2197,@marktenenholtz,2022-03-17 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1504427363072831493,"4. Don't overfit on your validation set

Keep in mind that any individual validation set is a minority of your total dataset.

If you use methods like early stopping, you risk optimizing a model on that minority of your dataset, rather than what works on the larger dataset."
2198,@marktenenholtz,2022-03-17 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1504427360325541900,"3. Do copious error analysis

No matter how robust your evaluation setup is, a top-line error metric is only so helpful.

You should focus more on identifying the ways that your model is failing."
2199,@marktenenholtz,2022-03-17 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1504427357532155904,"Here are some examples of differences you might see in production data: 

• Image resolution
• New products
• Changes in market conditions
• Changes in how users use your product"
2200,@marktenenholtz,2022-03-17 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1504427354742923265,"2. Anticipate future data

It's important to remember that the data you'll see in production can differ from your training data.

This goes for the usual things like not letting a time series model see future data, but there's so much more."
2201,@marktenenholtz,2022-03-17 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1504427348850053121,"Model evaluation is the worst taught skill in machine learning.

You can spend all the time you want learning about new models, data preprocessing, etc., but it's all a waste of time if your evaluation sucks.

Here are 5 principles to follow to create a solid evaluation setup:"
2202,@marktenenholtz,2022-03-16 20:45:49+00:00,https://twitter.com/marktenenholtz/status/1504197266269417473,@Sidsharmaa22 I'm not saying it has to be skipped. I'm just saying you have to do a lot less now than before.
2203,@marktenenholtz,2022-03-16 12:08:39+00:00,https://twitter.com/marktenenholtz/status/1504067114977669126,@prthgo @codingwithlucy @svpino @alfcnz @eddiejaoude @DThompsonDev Glad I could help!
2204,@marktenenholtz,2022-03-16 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1504064959944167428,"How to spend time building most ML models:

Old way:

- Tons of literature review
- Bespoke statistical representation
- Intense assumption checking

New way:

- Off-the-shelf model
- Data curation, evaluation, EDA
- Stakeholder engagement

Spend your time wisely."
2205,@marktenenholtz,2022-03-16 04:10:07+00:00,https://twitter.com/marktenenholtz/status/1503946688028979202,@Prathkum Take breaks and leave the night mode setting on all the time so you get less blue light
2206,@marktenenholtz,2022-03-16 03:16:22+00:00,https://twitter.com/marktenenholtz/status/1503933164590358528,"@GaryMarcus That's almost certainly going to be the case, but sometimes you have to figure out a way to do something stupidly before you can figure out how to do it well. Nothing wrong with that.

For all we know, it's very worth it to ""perfect"" the DL side before we broach symbolics."
2207,@marktenenholtz,2022-03-16 03:11:00+00:00,https://twitter.com/marktenenholtz/status/1503931812229353473,"@GaryMarcus It's fine and all to speculate that symbolic learning will help, I don't disagree.

But your point is just not correct -- we've had fundamentally new ideas and I don't understand why you feel the need to be so harsh to people that are exploring the DL side rather than symbolics"
2208,@marktenenholtz,2022-03-16 03:05:52+00:00,https://twitter.com/marktenenholtz/status/1503930520454971392,"@GaryMarcus Not a great take imo. Nothing has changed as far as how to solve MNIST because it's an incredibly easy problem that was solved all that time ago. Even SVMs get like 95% accuracy.

The field has solved many problems now it couldn't have fathomed of solving 33 years ago."
2209,@marktenenholtz,2022-03-16 00:57:20+00:00,https://twitter.com/marktenenholtz/status/1503898173529542662,@ideapharma @TheEdwinHung @marktenenhoItz @TheZachMueller It’s a capital i instead of an l
2210,@marktenenholtz,2022-03-15 21:31:40+00:00,https://twitter.com/marktenenholtz/status/1503846418418749446,"Everyone please report this fake account! @marktenenhoItz

(h/t to @TheZachMueller for tipping me off!)"
2211,@marktenenholtz,2022-03-15 15:43:14+00:00,https://twitter.com/marktenenholtz/status/1503758732479578130,@osanseviero The only true way to do this is adopt both names and use them interchangeably for maximum confusion
2212,@marktenenholtz,2022-03-15 15:23:47+00:00,https://twitter.com/marktenenholtz/status/1503753834635612163,@simonw Ensure that your models don't train on a sample and then validate on an earlier-dated sample from the same series
2213,@marktenenholtz,2022-03-15 14:40:27+00:00,https://twitter.com/marktenenholtz/status/1503742930170925073,@prthgo My advice -- look at an implementation of an LSTM cell. Makes so much more sense than the complicated theoretical explanation.
2214,@marktenenholtz,2022-03-15 14:37:18+00:00,https://twitter.com/marktenenholtz/status/1503742136113680393,"@edkesuma Great question. Imagine you have 100 stores that you're forecasting for. Each store has many items.

A grouped model could either be:

• A model for each item
• A model for each store

I generally find that either approach produces worse results than a model for everything."
2215,@marktenenholtz,2022-03-15 14:13:59+00:00,https://twitter.com/marktenenholtz/status/1503736270173966345,@PrasoonPratham Build something
2216,@marktenenholtz,2022-03-15 14:10:03+00:00,https://twitter.com/marktenenholtz/status/1503735280674787331,"@TheZachMueller I've used pybars3 before, which is basically Handlebars.js implemented in Python.

It's not incredibly well-maintained, but it gets the job done for sure."
2217,@marktenenholtz,2022-03-15 13:57:31+00:00,https://twitter.com/marktenenholtz/status/1503732124687511558,"@svpino Oh, yes! Sorry for the confusing wording."
2218,@marktenenholtz,2022-03-15 13:52:04+00:00,https://twitter.com/marktenenholtz/status/1503730753850712064,@jmmagana I didn't forget :)
2219,@marktenenholtz,2022-03-15 13:51:32+00:00,https://twitter.com/marktenenholtz/status/1503730620547149825,"@svpino 3. This question actually isn't specific to the others, it really is meant to test how well they can test the ""skill"" of their model, i.e. can they benchmark the model effectively? How could they justify its value to a stakeholder?"
2220,@marktenenholtz,2022-03-15 13:50:41+00:00,https://twitter.com/marktenenholtz/status/1503730405144465415,"@svpino 1. Some form of roll-forward or expanding-window cross-validation
2. There are so many, but I'd like to see RMSE, MAE, MAPE (and explain how this is bad for low-volume time series), WAPE, and whatever else they can justify."
2221,@marktenenholtz,2022-03-15 13:27:54+00:00,https://twitter.com/marktenenholtz/status/1503724674508804107,"@TheBrotherUno Impossible to tell without trying it out. You need to set a level accuracy that would be necessary for your model to drive value, then see if you can hit it.

You also need to have enough data to feel confident that your evaluation is proper."
2222,@marktenenholtz,2022-03-15 13:25:21+00:00,https://twitter.com/marktenenholtz/status/1503724032000155655,@TheBrotherUno Depends on the problem
2223,@marktenenholtz,2022-03-15 13:24:52+00:00,https://twitter.com/marktenenholtz/status/1503723907370606602,"@svpino I would give them a case study with a sample of a grouped time series with some high volume items and some low volume items. I'd ask:

1. How would you set up model evaluation?
2. What metrics would you use?
3. Say your metric ended up being &lt;x&gt;. Is that good?"
2224,@marktenenholtz,2022-03-15 12:00:09+00:00,https://twitter.com/marktenenholtz/status/1503702589883379712,"I hope you learned something!

Time series modeling is ridiculously important, and knowledge on how to supercharge it is sorely lacking out there in the wild.

Follow me @marktenenholtz for more high-signal ML content."
2225,@marktenenholtz,2022-03-15 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1503702587098378241,"Topic TL;DR

1. Domain-specific EDA
2. Domain-specific benchmarking
3. Stakeholder relationships for anomalies
4. Model evaluation
5. Use XGBoost/LightGBM/RNNs/Transformers
6. Monitoring in production"
2226,@marktenenholtz,2022-03-15 12:00:08+00:00,https://twitter.com/marktenenholtz/status/1503702584367869954,"6. Production

The main concern you'll have is data drift. Make sure you have a monitoring system that you check in with on a regular basis.

Personally, I love adversarial validation to identify data drift. It's one of the most powerful and underused techniques in ML."
2227,@marktenenholtz,2022-03-15 12:00:07+00:00,https://twitter.com/marktenenholtz/status/1503702581637353472,"LSTMs/GRUs/Transformers

If you have enough data and spend enough time tuning the model, you can beat LightGBM/XGBoost sometimes.

The same feature engineering techniques work, but keep in mind that these models are much more finicky about the scale of the data."
2228,@marktenenholtz,2022-03-15 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1503702578860724227,"LightGBM/XGBoost

You're probably not going to do better than this.

For these models, rolling/lag features on the target variable work fantastically.

Other great features include grouped aggregations -- this is how you help the model distinguish different groups in the data."
2229,@marktenenholtz,2022-03-15 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1503702576121847820,"5. Now, we're gonna get into the fun stuff: modeling techniques!

With the modeling approaches I'll talk about, the following are almost always true:

• Don't create group-level models. Make the group a feature"
2230,@marktenenholtz,2022-03-15 12:00:05+00:00,https://twitter.com/marktenenholtz/status/1503702573387182080,"4. Model evaluation

Any answer other than roll-forward/expanding window cross-validation is wrong for 99.999% of problems.

Do NOT do any sort of random split on your samples that doesn’t ensure future values aren’t included.

That’s a death sentence for a model."
2231,@marktenenholtz,2022-03-15 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1503702570614677508,"3. Domain expertise

If available, ALWAYS engage with someone that understands how the data is collected and dynamics at play from the business side.

There can be huge spikes caused by collection issues, strikes, out of stocks, you name it.

These conversations are invaluable."
2232,@marktenenholtz,2022-03-15 12:00:04+00:00,https://twitter.com/marktenenholtz/status/1503702567863271429,"2. Benchmarking

Predicting the mean is decent, but keep in mind that even a naive forecaster could outperform that usually.

Try things that better consider the domain, such as a rolling mean, predicting the value seen in the last day of available data, etc."
2233,@marktenenholtz,2022-03-15 12:00:03+00:00,https://twitter.com/marktenenholtz/status/1503702565128597506,"1. EDA

The best way to start any ML problem.

Here are some things to look at in time series data:

• Seasonality (monthly, daily, etc.)
• Trends (big for proper evaluation)
• Autocorrelation
• Diff. between older and newer series
• Tons of raw samples"
2234,@marktenenholtz,2022-03-15 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1503702562171629570,"Modern ML research loves to ignore time series forecasting.

Sure, language sequence modeling is huge, but forecasting isn’t.

Yet, for every company in the US’s $5,000,000,000,000 retail industry, forecasting is a huge problem.

Here’s your state-of-the-art forecasting toolbox:"
2235,@marktenenholtz,2022-03-14 18:01:04+00:00,https://twitter.com/marktenenholtz/status/1503431029197049862,"@svpino It's totally self-defeating. 

You learn the most about algorithms by applying them, not from studying them in excruciating detail before using them."
2236,@marktenenholtz,2022-03-14 12:00:06+00:00,https://twitter.com/marktenenholtz/status/1503340187232788485,"You should write about ML while you learn it.

If you can't explain what you're learning to someone else simply, concisely, and without jargon, then you don't understand it.

Find what you can't explain simply, and learn more about it until you can."
2237,@marktenenholtz,2022-03-14 00:29:17+00:00,https://twitter.com/marktenenholtz/status/1503166339803930625,@CardinalMason And a higher income tax rate
2238,@marktenenholtz,2022-03-13 12:00:02+00:00,https://twitter.com/marktenenholtz/status/1502977785416978432,The best models come from the data scientists that evaluate well and try the most things.
2239,@marktenenholtz,2022-03-12 18:43:43+00:00,https://twitter.com/marktenenholtz/status/1502716988250075144,"@muralijnu1 Read books on marketing/sales that encourage uncovering the true needs/desires of customers.

Then, translate that into crafting/selling models that perfectly meet stakeholder needs."
2240,@marktenenholtz,2022-03-12 13:18:25+00:00,https://twitter.com/marktenenholtz/status/1502635122935353354,@JFPuget @kaggle The gap I’ve seen between kagglers and most non-kagglers in model evaluation is staggering
2241,@marktenenholtz,2022-03-12 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1502630494927695877,"Every top-tier data scientist is fantastic at:

• Evaluating their models
• Engaging stakeholders
• Building minimally-complex solutions
• Iterating rapidly

Doesn't matter what company/field you're in.

All of them have these qualities.

If you don't, work on them!"
2242,@marktenenholtz,2022-03-11 16:25:01+00:00,https://twitter.com/marktenenholtz/status/1502319692379308043,"@tunguz XGBoost, because it’s all you need.

In all seriousness, I guess you could technically do 95% of the non-plotting workflows you need if you just built a bit of your own code around PyTorch"
2243,@marktenenholtz,2022-03-11 16:07:43+00:00,https://twitter.com/marktenenholtz/status/1502315339715067905,"@LBacaj I love this, Louie. I feel like 50-75% of the time preparing to give feedback to someone is figuring out how to effectively manage the emotions of the situation, and this does a really good job of abstracting that away to keep it constructive."
2244,@marktenenholtz,2022-03-11 14:30:06+00:00,https://twitter.com/marktenenholtz/status/1502290774691569667,"@prthgo Sorry, gonna have to report you to @tunguz"
2245,@marktenenholtz,2022-03-11 13:55:07+00:00,https://twitter.com/marktenenholtz/status/1502281971585724419,@LeopolisDream Winner
2246,@marktenenholtz,2022-03-11 13:17:17+00:00,https://twitter.com/marktenenholtz/status/1502272451488665602,@TheZachMueller I’ll be the lightning rod for everyone else today
2247,@marktenenholtz,2022-03-11 13:13:05+00:00,https://twitter.com/marktenenholtz/status/1502271393806884872,@LucimrioCustdi1 That meme template never fails. Well done
2248,@marktenenholtz,2022-03-11 13:11:29+00:00,https://twitter.com/marktenenholtz/status/1502270988716761090,"@svpino I’ve talked to hiring managers that required a Master’s in their job description but would still interview qualified applicant that didn’t have one.

Never be afraid to apply for a job even if you don’t have an advanced degree!"
2249,@marktenenholtz,2022-03-11 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1502268127916167175,I’ll start: “R isn’t capable of running in production”
2250,@marktenenholtz,2022-03-11 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1502268125156306952,Start a machine learning fight in one tweet:
2251,@marktenenholtz,2022-03-10 23:30:13+00:00,https://twitter.com/marktenenholtz/status/1502064310469767171,"@tunguz Ah I know you’re just yanking my chain, we all know XGBoost is all you need"
2252,@marktenenholtz,2022-03-10 22:55:59+00:00,https://twitter.com/marktenenholtz/status/1502055696363368448,"By popular demand, I’ll add one more.

Object detection: Yolov5, EfficientDet"
2253,@marktenenholtz,2022-03-10 18:16:01+00:00,https://twitter.com/marktenenholtz/status/1501985240952233985,@GergelyOrosz I’m really hoping small towns begin to thrive with less pressure to be in a tech hub
2254,@marktenenholtz,2022-03-10 17:46:52+00:00,https://twitter.com/marktenenholtz/status/1501977904586104834,@StraightLineAd This is why paying for courses from reputable creators is such a worthwhile investment
2255,@marktenenholtz,2022-03-10 16:34:11+00:00,https://twitter.com/marktenenholtz/status/1501959613146664960,@JFPuget We Kagglers all think alike
2256,@marktenenholtz,2022-03-10 16:22:55+00:00,https://twitter.com/marktenenholtz/status/1501956776828325896,"@simonsarris @dvassallo I come from a different niche (machine learning), but I've seen some examples of e-books that have accompanying Jupyter notebooks.

For JS, you could do an analogous thing with slightly different tools.

Feel free to DM me if you want some inspiration!"
2257,@marktenenholtz,2022-03-10 16:18:25+00:00,https://twitter.com/marktenenholtz/status/1501955645758652416,@deepa0299 Appreciate the kind words!
2258,@marktenenholtz,2022-03-10 15:08:58+00:00,https://twitter.com/marktenenholtz/status/1501938168051867651,@GergelyOrosz The creator economy wins
2259,@marktenenholtz,2022-03-10 14:55:04+00:00,https://twitter.com/marktenenholtz/status/1501934671457783815,@DataScienceHarp Addressed that here: https://t.co/VHSaL1s5qc
2260,@marktenenholtz,2022-03-10 13:26:03+00:00,https://twitter.com/marktenenholtz/status/1501912269751468038,"@KevinKaichuang @SwetaBioX Those are a bit more tricky, although custom transformers and graph NNs seem to be the way to go"
2261,@marktenenholtz,2022-03-10 13:19:20+00:00,https://twitter.com/marktenenholtz/status/1501910577786064898,"@tunguz @tobias_sterbak Good point. 

I’ll admit to both of ya, I usually throw those linear models in to debug my pipeline when I’m starting out, so I’m probably subconsciously doing the same thing"
2262,@marktenenholtz,2022-03-10 13:16:24+00:00,https://twitter.com/marktenenholtz/status/1501909839945076739,@LBacaj This thing looks seriously awesome. No more Google docs!
2263,@marktenenholtz,2022-03-10 13:13:16+00:00,https://twitter.com/marktenenholtz/status/1501909050509910016,"@tobias_sterbak XGBoost on a GPU or LightGBM on a CPU are almost always fast enough that it just doesn’t matter for me, but I hear you"
2264,@marktenenholtz,2022-03-10 13:08:58+00:00,https://twitter.com/marktenenholtz/status/1501907969029689348,"@Cogitoe79321738 Regression works the same as classification as far as model architecture.

However what you’re describing (sentiment analysis) is usually treated as a classification problem (1 is happy, 0 is sad)"
2265,@marktenenholtz,2022-03-10 13:02:49+00:00,https://twitter.com/marktenenholtz/status/1501906421461438470,"@svpino Make sure to bold, capitalize, and underline “validation”!"
2266,@marktenenholtz,2022-03-10 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1501905760678150146,"I hope you learned something.

Strong baselines are a quality of highly successful modelers, and developing this skill is a force multiplier for solving ML problems.

Follow me @marktenenholtz for more high-signal ML content!"
2267,@marktenenholtz,2022-03-10 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1501905757842731014,"TL;DR:

Tabular: XGBoost/LightGBM/RF
Time series: XGBoost/LightGBM/RF
Image: ResNet/EffNet
Text: RoBERTa
Audio: ResNet/EffNet

Your best bet is usually to start with these and then experiment from there. 

Nothing in ML is an end-all-be-all!"
2268,@marktenenholtz,2022-03-10 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1501905754994839552,"Audio data: ResNet/EffNet

That's right -- image models for audio data.

My favorite way of starting audio problems is converting the audio to a spectrogram and throwing an image model at it.

Give it a shot!"
2269,@marktenenholtz,2022-03-10 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1501905752125935618,"Text data: RoBERTa

I love starting text problems with a DistilRoBERTa model.

Sometimes, the combination of speed and accuracy is exactly what you need.

If not, it's the same as above -- scale up to the bigger versions and you'll get that juice accuracy you're looking for."
2270,@marktenenholtz,2022-03-10 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1501905749328293888,"Image data: ResNet/EffNet

ResNet18 and EffNet-B0 are small, quick models that are effective for nearly any type of image data.

The best part? 

Once you've squeezed all the juice out of those, you can scale up to their bigger versions and almost always get better accuracy."
2271,@marktenenholtz,2022-03-10 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1501905746501386245,"Time series data: XGBoost/LightGBM/RF

That's right -- the best time series models are ones not even built for it.

Instead, set a prediction horizon and create time series features for your model (i.e. rolling/lagged features).

Then, treat it like any other tabular data!"
2272,@marktenenholtz,2022-03-10 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1501905743653457924,"Tabular data: XGBoost/LightGBM/RF

To most, no surprise here.

To everyone else, ensemble tree-based models are by far the best plug-and-play tabular models.

Neural networks can beat them sometimes, but if GBT models are easy-mode, NNs are hard-mode."
2273,@marktenenholtz,2022-03-10 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1501905740813848582,"Over the last couple of years, I've spent 1000's of hours building ML models.

Truth is, after using dozens of models/architectures, 99% of them are a waste of time.

I start most problems with 1 of 6 architectures.

Here are the best models for a strong baseline 🧵"
2274,@marktenenholtz,2022-03-10 00:14:49+00:00,https://twitter.com/marktenenholtz/status/1501713147790434305,"@jeremyphoward I would argue this is how the best products are made!

If you’re building something because you’re lazy, it inherently implies there’s an itch that isn’t being scratched."
2275,@marktenenholtz,2022-03-09 15:13:54+00:00,https://twitter.com/marktenenholtz/status/1501577020391759873,@tunguz Only thornier take is pissing off the R community
2276,@marktenenholtz,2022-03-09 14:21:32+00:00,https://twitter.com/marktenenholtz/status/1501563842706477058,"@GergelyOrosz One thing I've seen in my area (not a big tech city) is that some companies are adjusting salaries to grow into the remote work age, and some are not.

There's a chasm between the TC's."
2277,@marktenenholtz,2022-03-09 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1501543332614340609,"Your goal should not just be decreasing your error metric.

You goal should be eliminating ways that your model fails."
2278,@marktenenholtz,2022-03-08 18:36:57+00:00,https://twitter.com/marktenenholtz/status/1501265734755880966,@pi_emoji Where's @tunguz when you need him?
2279,@marktenenholtz,2022-03-08 16:38:55+00:00,https://twitter.com/marktenenholtz/status/1501236028715245570,@PrasoonPratham Folks saying you don't need this clearly haven't written PyTorch code 😬
2280,@marktenenholtz,2022-03-08 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1501180980480004098,"I hope you learned something!

These are skills I've built through 1000's of hours of building ML models.

Follow me @marktenenholtz to get more high-signal ML content!"
2281,@marktenenholtz,2022-03-08 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1501180976218529795,"TL;DR:

1. Raw data observation
2. Model evaluation
3. Smart baselines
4. Strong starting points
5. Rapid iteration
6. Error analysis"
2282,@marktenenholtz,2022-03-08 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1501180973156732932,"6. Error analysis

The only reliable way of improving your models is by thoroughly analyzing their predictions.

The average modeler stares at their error metric and hopes it gets better.

The best know hope is not a strategy, and they tailor solutions to their model's errors."
2283,@marktenenholtz,2022-03-08 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1501180970086453248,"5. Rapid iteration

If we could know what techniques would work before attacking a problem, then we wouldn't need data scientists.

After pruning the bad ideas out, the best modelers most efficiently search the remaining possibilities to land on the best solution."
2284,@marktenenholtz,2022-03-08 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1501180967272083459,"4. Starting strong

Think of the potential solutions to an ML problem as a nearly-infinite tree.

The most successful modelers can prune all but 0.1% of that tree before they even start."
2285,@marktenenholtz,2022-03-08 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1501180964076081155,"3. Smart baselines

Predicting the mean works, but is it the best baseline?

The best know to use better benchmarks, such as human performance and intelligent heuristics.

The best model is the simplest, so always start there."
2286,@marktenenholtz,2022-03-08 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1501180960531877890,"2. Ingenious model evaluation

The best modelers are the best model evaluators.

If there was a Venn Diagram of the two groups, it would be a circle.

This is a skill that anyone can learn, but it takes several hundred hours of modeling experience to become an expert."
2287,@marktenenholtz,2022-03-08 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1501180957218332677,"1. Observation of raw data

EDA by plotting is great.

EDA by looking at thousands of samples is the best.

EDA is for becoming one with your data, with all of its bumps and sharp edges. This is how to do it.

This should be done over many hours, not minutes."
2288,@marktenenholtz,2022-03-08 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1501180954404024324,6 habits of highly successful machine learning modelers:
2289,@marktenenholtz,2022-03-07 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1500818570124562433,"The 10x ML Engineer playbook:

- Latest/greatest MLOps tools
- Great model evaluation
- Rapid iteration
- Clear, concise code

Automate every best practice you can. For everything else, become an expert."
2290,@marktenenholtz,2022-03-06 17:04:06+00:00,https://twitter.com/marktenenholtz/status/1500517590824890375,"@prthgo @edkesuma @the_oreolorun Twitter is so great for that, completely agree"
2291,@marktenenholtz,2022-03-06 15:11:44+00:00,https://twitter.com/marktenenholtz/status/1500489313523482625,"@haltakov This is mine, too"
2292,@marktenenholtz,2022-03-06 14:00:02+00:00,https://twitter.com/marktenenholtz/status/1500471266918076422,What’s your number 1 tip for learning machine learning?
2293,@marktenenholtz,2022-03-06 13:23:57+00:00,https://twitter.com/marktenenholtz/status/1500462189819346946,@chpolyzo Not advocating us to stop developing at all. We just need to be more specific about where the potential issues lie.
2294,@marktenenholtz,2022-03-06 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1500456171584442368,"Right now, we need to worry less about AI outsmarting us.

We need to worry more about it outscaling us.

Most of what we perceive as AI being smarter than us is actually it overwhelming us."
2295,@marktenenholtz,2022-03-05 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1500093785182593028,"The best way to do EDA is to look at thousands of samples, individually.

Spend hours, not minutes."
2296,@marktenenholtz,2022-03-04 20:51:11+00:00,https://twitter.com/marktenenholtz/status/1499849961449660426,"@svpino There’s a lot of research that shows how little humans understand why they make decisions, FWIW"
2297,@marktenenholtz,2022-03-04 17:34:56+00:00,https://twitter.com/marktenenholtz/status/1499800574388224003,@ph_singer @JFPuget @MeganRisdal They also don’t benchmark the 1D-CNN model that’s been floating around on Kaggle since the MoA competition. I’ve had a lot of success with that.
2298,@marktenenholtz,2022-03-04 13:35:15+00:00,https://twitter.com/marktenenholtz/status/1499740255817285632,@JFPuget The best solution of all: ensemble the two
2299,@marktenenholtz,2022-03-04 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1499731395920285699,"Brainstormed a solution to an machine learning problem I was stuck on for days while walking with my girlfriend.

- Completely simplified my approach
- 2x'd model performance
- Completed a POC and launch in a few weeks

Taking walks is an ML cheat code."
2300,@marktenenholtz,2022-03-03 21:22:06+00:00,https://twitter.com/marktenenholtz/status/1499495356165406723,@BasantSingh1000 @ykilcher You should never feel bad about specializing. You can catch up in other fields by reading ~10-15 of the most influential new papers whenever you need to.
2301,@marktenenholtz,2022-03-03 18:57:37+00:00,https://twitter.com/marktenenholtz/status/1499458992942333956,@marksaroufim @bhutanisanyam1 @rasbt @sudomaze @ykilcher @AICoffeeBreak I’ll send you some vitamin drips
2302,@marktenenholtz,2022-03-03 18:54:41+00:00,https://twitter.com/marktenenholtz/status/1499458256598716419,@bhutanisanyam1 @marksaroufim @rasbt @sudomaze @ykilcher @AICoffeeBreak Heck I’m moving apartments this weekend but I’ll join if I can!
2303,@marktenenholtz,2022-03-03 18:39:30+00:00,https://twitter.com/marktenenholtz/status/1499454433184423944,@svpino Making moves! I love it!
2304,@marktenenholtz,2022-03-03 15:36:14+00:00,https://twitter.com/marktenenholtz/status/1499408314723143682,@bhutanisanyam1 @ykilcher @AICoffeeBreak I haven’t checked out @AICoffeeBreak  yet — thanks for the tip!
2305,@marktenenholtz,2022-03-03 15:32:00+00:00,https://twitter.com/marktenenholtz/status/1499407247578910720,@rasbt @ykilcher Couldn’t agree more!
2306,@marktenenholtz,2022-03-03 14:16:29+00:00,https://twitter.com/marktenenholtz/status/1499388243284054019,"@dickiebush I grew from 100 followers to 12.9k in less than 3 months using this general strategy.

This works, no question."
2307,@marktenenholtz,2022-03-03 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1499369021321318402,"Yannic keeps up to take with the most relevant research and has a library of videos going back 4+ years with clear and concise rundowns of influential ML research.

I highly recommend that you check him out to keep up to date!"
2308,@marktenenholtz,2022-03-03 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1499369017877823488,"Machine learning research is hard to keep up with.

You can read multiple papers a week and still fall behind.

To stay on top of the latest, I take full advantage of the wealth of knowledge on the internet, and one of my favorite resources is @ykilcher's YouTube channel. https://t.co/YaQNAdQbHm"
2309,@marktenenholtz,2022-03-03 03:03:58+00:00,https://twitter.com/marktenenholtz/status/1499219000202571777,"@FeedCompu I usually recommend that people read 2 papers max per day, but ideally only 1. It’s a very mentally taxing exercise and it’s more important that you actually retain what you read."
2310,@marktenenholtz,2022-03-02 15:36:39+00:00,https://twitter.com/marktenenholtz/status/1499046031434162181,@tunguz 🗣Drop a brand name!
2311,@marktenenholtz,2022-03-02 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1499006636731428866,"In his words:

""If someone cannot explain something in plain English, then we should question whether they really do themselves understand what they profess.""

Apply this to yourself. You'll find that you quickly identify knowledge gaps and develop a deeper understanding."
2312,@marktenenholtz,2022-03-02 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1499006633564803073,"Underrated tip for learning ML:

Write about what you've just learned.

• Exposes holes in your knowledge
• Helps with long-term comprehension
• Improves your communication skills

Why should you do this?

Richard Feynman proposed the following test:"
2313,@marktenenholtz,2022-03-01 19:09:43+00:00,https://twitter.com/marktenenholtz/status/1498737263102070796,"@JustinSaaS I grew so much more quickly when I started thinking of other creators like colleagues instead of ""figures with authority.""

There are some folks with more influence/trust than others, but there's really no ""authority"" hierarchy -- we're all in it together."
2314,@marktenenholtz,2022-03-01 18:56:22+00:00,https://twitter.com/marktenenholtz/status/1498733905440038921,"@MishraAmogh @paperswithcode Your method is pretty good for new architectures, but what about concept papers? Some of the papers that had the biggest influence on how I train/interpret models were conceptual papers with no code."
2315,@marktenenholtz,2022-03-01 17:57:55+00:00,https://twitter.com/marktenenholtz/status/1498719194011029509,"@JustinSaaS I really like this framework.

I usually hear ""trust building"" referred to as something more like ""building authority."" I think trust is a bit more accurate since it doesn't make things sound like you're in a zero-sum competition with other creators."
2316,@marktenenholtz,2022-03-01 14:55:08+00:00,https://twitter.com/marktenenholtz/status/1498673197721567243,@Jeande_d Those are the best papers!
2317,@marktenenholtz,2022-03-01 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1498644256726020097,"If you follow these 4 steps, you should be able to catch up to any field, new or mature, in machine learning.

Follow me @marktenenholtz and we'll discuss more about how to learn in ML!"
2318,@marktenenholtz,2022-03-01 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1498644253903179778,"Why skip what you don't get?

Keep in mind that research is at the cutting edge of the field. This means that nobody fully understands the content, not even the authors.

Also, sometimes older papers reference archaic topics that are no longer discussed, so they aren't relevant."
2319,@marktenenholtz,2022-03-01 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1498644251076227073,"4th pass: the whole thing, but skip what doesn't make sense

Now it's time to go back over the paper to reinforce what you gathered in the previous pass.

You also are better equipped with the context to understand the specifics of the math."
2320,@marktenenholtz,2022-03-01 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1498644248316370945,"3rd pass: the whole thing, but skip/skim math

Now that you have all the context you need, it's time to see how the authors actually did it.

However, it's really easy to get bogged down in the specifics of the math. Try to focus on building a conceptual map of the paper here."
2321,@marktenenholtz,2022-03-01 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1498644245527228416,"At this point, you can skip/skim the related work section.

If you're new to the paper's field, it's going to be tough to understand.

Sometimes, it's also where authors try to promote the work of their colleagues or promote the work of a potential reviewer to get brownie points."
2322,@marktenenholtz,2022-03-01 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1498644239890087939,"1st pass: title, abstract, and pictures

The goal here is to be able to quickly gather a fairly uneducated ""elevator pitch"" for the paper so that you have good context on what it talks about.

Some papers (esp. DL) are almost completely explained in one of the figures."
2323,@marktenenholtz,2022-03-01 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1498644237071441924,"The goal is to understand papers by getting the following information, in order:

1. Context
2. Motivation and results
3. Implementation
4. Nitty-gritty details

Here's how it's done:"
2324,@marktenenholtz,2022-03-01 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1498644234240348160,"Do you ever feel overwhelmed when reading papers?

It's probably because you're not approaching them correctly.

Andrew Ng (@AndrewYNg), arguably the GOAT of ML education, recommends readers go over a paper 4 times.

In each pass, you incrementally absorb the paper."
2325,@marktenenholtz,2022-03-01 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1498644231149142025,How to efficiently read and understand machine learning research papers:
2326,@marktenenholtz,2022-02-28 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1498281860752838660,"I hope you learned something!

Follow me @marktenenholtz for more high-signal ML content and tips on how to grow and learn in the field."
2327,@marktenenholtz,2022-02-28 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1498281857984577537,"Ideally, you should spend as little time as possible *not* using the wonderful libraries out there.

Once you’ve done the dirty work once, it’s only worth going back to from-scratch if you feel like you’ve completely forgotten, but this is what the mental map helps to prevent."
2328,@marktenenholtz,2022-02-28 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1498281855166021634,"5. Incrementally replace w/ library code

Once you have a barely-working solution, I rarely (if ever) implement from scratch ever again unless I forget how it works.

If you have a solid library, it saves you so much time and effort."
2329,@marktenenholtz,2022-02-28 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1498281852334837762,"4. Mental map

Now that you can explain it and code it, try to fit this solution into your overall picture of ML.

Answer these questions:

• What is it similar to?
• How is it unique?
• What are the limitations?
• When is it the best solution?

Mental context is important."
2330,@marktenenholtz,2022-02-28 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1498281849482715142,"3. Quick-and-dirty implementation

I like to implement some of the core functionality in extremely basic code.

Focus on the most important parts to your use of the library.

Remember: ML tools are leaky abstractions. This is what helps you avoid the sharp edges."
2331,@marktenenholtz,2022-02-28 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1498281846680920065,"2. ELI5

Try to explain the topic as if you were explaining it to someone with absolutely no background.

Keep this jargon-free and as concise and clear as possible.

Take note of what you can’t explain well. Go back and learn more about it until you can."
2332,@marktenenholtz,2022-02-28 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1498281843832938496,"1. Use a course/blog/article

The first thing to do is to understand what the library accomplishes.

Take advantage of the wealth of explanations out there.

The goal of this step is to build a base of knowledge — don’t feel like you need to become an expert yet."
2333,@marktenenholtz,2022-02-28 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1498281841001844740,"You should embrace MLOps tooling.

It’s a blessing that easy, packaged solutions exist for nearly all common use-cases.

But, some rightfully worry about using “out-of-the-box solutions” and not understanding the underlying code.

4 steps I take before using library code🧵"
2334,@marktenenholtz,2022-02-27 21:31:16+00:00,https://twitter.com/marktenenholtz/status/1498048110580359175,"@svpino I didn’t want to spoil it!

Again, technicality, but collections of vectors and matrices can have varying dimensions along the first axis, whereas tensors enforce that every object in the collection has the same dimensions."
2335,@marktenenholtz,2022-02-27 21:27:51+00:00,https://twitter.com/marktenenholtz/status/1498047248512520195,@prthgo So accurate AND so fast
2336,@marktenenholtz,2022-02-27 21:19:29+00:00,https://twitter.com/marktenenholtz/status/1498045146339942403,"@svpino I think you’re looking for 1, but I think it’s technically 4."
2337,@marktenenholtz,2022-02-27 13:42:11+00:00,https://twitter.com/marktenenholtz/status/1497930059532877825,@ai_for_humans Nothing wrong with good ol text files
2338,@marktenenholtz,2022-02-27 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1497919453857673217,"Every machine learning project should keep the following notes:

• Tech debt log
• ETL pipeline description
• Future work ideas
• Experiment logs
• List of assumptions
• Potential risks
• Changelog

Even on solo projects, it’s hard to remember everything. Write it down."
2339,@marktenenholtz,2022-02-26 23:10:29+00:00,https://twitter.com/marktenenholtz/status/1497710690978738176,"@fchollet A big, golden star for a marvel of MLOps tooling"
2340,@marktenenholtz,2022-02-26 23:05:37+00:00,https://twitter.com/marktenenholtz/status/1497709465935695873,"@svpino @haltakov “Here, we don’t have work-life balance.

Just work.”"
2341,@marktenenholtz,2022-02-26 22:52:04+00:00,https://twitter.com/marktenenholtz/status/1497706054246973441,"@Xvayzet3 Depth = deep knowledge of a specific topic

Breath = knowledge of multiple topics"
2342,@marktenenholtz,2022-02-26 22:51:05+00:00,https://twitter.com/marktenenholtz/status/1497705810520158210,"@eShaneRobinson It’s equally useful for everyone. If you have absolutely no idea, just Google what the basics are for what you’re interested in. Then start there."
2343,@marktenenholtz,2022-02-26 19:02:02+00:00,https://twitter.com/marktenenholtz/status/1497648165767548934,"@svpino MLOps tooling is one of the greatest force multipliers for business efficiency out there.

Love seeing more accessible services because data science allows us to do them at a lower cost!"
2344,@marktenenholtz,2022-02-26 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1497557071704715267,"Bad system for learning:

“Now that I know the basics, I’m going to start learning a little bit more about everything.”

Good system for learning:

“Now that I know the basics, I’m going to dive heavily into one or two specific areas.”

Learn like this: https://t.co/ShNvwDwwxc"
2345,@marktenenholtz,2022-02-25 21:40:27+00:00,https://twitter.com/marktenenholtz/status/1497325645726113795,@TheZachMueller On vacation right now and I’m SO HYPED for when I get back
2346,@marktenenholtz,2022-02-25 20:36:30+00:00,https://twitter.com/marktenenholtz/status/1497309553226129412,@tunguz @svpino ayy girl ur DGX be lookin fineeee today
2347,@marktenenholtz,2022-02-25 17:42:23+00:00,https://twitter.com/marktenenholtz/status/1497265735470620681,@rasbt Looks great Sebastian! Well done!
2348,@marktenenholtz,2022-02-25 13:07:44+00:00,https://twitter.com/marktenenholtz/status/1497196617354649619,"@svpino It’s a classic case of the pendulum swinging too far.

Years ago people complained MLOps was crucially, sorely missing from the space.

Now people complain that deep learning is too leaky of an abstraction for how easy we’ve made it.

Both points have some truth."
2349,@marktenenholtz,2022-02-25 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1497194681335091200,"You need to time-box your machine learning projects.

ML models can impact nearly every part of your business.

It’s far more valuable to get a serviceable model in place everywhere rather than an extremely heavily-engineered model in a single place."
2350,@marktenenholtz,2022-02-25 02:04:10+00:00,https://twitter.com/marktenenholtz/status/1497029623745761282,"@LBacaj Thank you for sharing that, Louie. We all need that wisdom right about now."
2351,@marktenenholtz,2022-02-24 21:37:01+00:00,https://twitter.com/marktenenholtz/status/1496962392974106625,@svpino @haltakov @funny_tintin I’d say you do an admirable job considering how many you write without a hitch!
2352,@marktenenholtz,2022-02-24 21:19:41+00:00,https://twitter.com/marktenenholtz/status/1496958030558240777,"@jeremyphoward @robjhyndman Ever since learning CUDA C, I’ve wanted to use numba.cuda.

Have you played around with this for fastai or at all in general?

Super interested in the utility of custom CUDA kernels ever since you talked about sparse convs in the fastai course!"
2353,@marktenenholtz,2022-02-24 15:19:59+00:00,https://twitter.com/marktenenholtz/status/1496867511387578372,"@radekosmulski Great thread, Radek!"
2354,@marktenenholtz,2022-02-24 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1496832293125513218,"Bad system for learning:

""I want to learn X, Y and Z, so I'm going to go follow a bootcamp on all of them.""

Good system for learning:

""I want to learn X, Y, and Z, so I'm going to come up with a project that requires them and reference the bootcamp when I need to."""
2355,@marktenenholtz,2022-02-24 00:48:36+00:00,https://twitter.com/marktenenholtz/status/1496648220893663233,@jordanbpeterson Couldn’t have said it better myself!
2356,@marktenenholtz,2022-02-23 14:38:42+00:00,https://twitter.com/marktenenholtz/status/1496494732037525508,"@davidshor Yes! I didn’t see enough about it to mention it, but I believe they compounded their errors there."
2357,@marktenenholtz,2022-02-23 14:36:19+00:00,https://twitter.com/marktenenholtz/status/1496494134370226186,"@7ForwardGears @kearneymw Appreciate the criticism (not the insults, though), but that’s not why I said they failed. I said they failed because of poor decision analysis that was exposed by a market that was no longer exploding.

I’m sure their accuracy study is biased, but it’s still not why they failed."
2358,@marktenenholtz,2022-02-23 13:29:26+00:00,https://twitter.com/marktenenholtz/status/1496477300006821889,@tunguz Thanks Bojan! 🙏
2359,@marktenenholtz,2022-02-23 13:21:27+00:00,https://twitter.com/marktenenholtz/status/1496475290566111236,@TheZachMueller @AndrewYNg I’ll have to show you one of the visuals I have in my course :)
2360,@marktenenholtz,2022-02-23 13:20:33+00:00,https://twitter.com/marktenenholtz/status/1496475066435051522,"@kearneymw There’s plenty out there if you Google it. I also did my own research.

I did not work there."
2361,@marktenenholtz,2022-02-23 13:17:04+00:00,https://twitter.com/marktenenholtz/status/1496474191025123333,"@kearneymw You want me to cite my sources? On Twitter? 

Okay, here’s one for the accuracy numbers (not that it has any impact on the point of the thread): https://t.co/ZzWYXoz0fl"
2362,@marktenenholtz,2022-02-23 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1496469927787597826,"I’m releasing a course soon on the best way to build your skills as a data scientist.

If you wanna figure out how to learn for yourself and become less reliant on courses and boot camps, I think it’ll be perfect for you.

Follow me @marktenenholtz so you don’t miss it!"
2363,@marktenenholtz,2022-02-23 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1496469925057089543,"5. Broaden your scope
 
At this point, you’re probably ready for something different.

Now it’s time to revisit any other skills you enjoyed in step 2 and do it all over again!

Before you know it, you’ll have multiple skills that employers will be desperate to hire you for."
2364,@marktenenholtz,2022-02-23 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1496469922129403907,"4. Learn by doing

The most effective way of doing step 3 is by applying the skill in a project.

Project-based learning is irreplaceable.

There are an infinite amount of nuances that go into solving data science problems, and this is the only way you can be introduced to them."
2365,@marktenenholtz,2022-02-23 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1496469919273160707,"3. Go all-in

For at least a month, dive as deep into your favorite skill as you can.

Do projects, take more advanced courses, etc.

You’ll find that this act of immersion 100x’s your competency.

Now you’re ahead of 99% of those who started at the same time as you."
2366,@marktenenholtz,2022-02-23 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1496469916513234945,"2. Find what you love

Once you have a solid base, find the things you enjoyed the most.

Maybe it’s data viz, maybe it’s ML.

Revisit those and pick one of them to start focusing on more. 

Most data scientists have more than one, but just pick one for now."
2367,@marktenenholtz,2022-02-23 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1496469913640181763,"1. Don’t be a generalist

It’s smart to get the basics in every major part of the field.

But don’t try to be an expert in every part of the field. 

This is a terrible idea. The field is expanding quickly, and nobody wants to hire someone that is “just okay” at everything."
2368,@marktenenholtz,2022-02-23 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1496469910809038854,"2022 data science superpower:

Specialization.

Data science is an ill-defined field. It’s as deep and as wide as an ocean.

The thing is — if you can’t reach parts of that ocean that few others can, your value diminishes, big time.

5 tips for maximizing your value 🧵"
2369,@marktenenholtz,2022-02-23 02:31:14+00:00,https://twitter.com/marktenenholtz/status/1496311658880421891,@RossRutledge3 Those are reported MAPEs. I don’t have access to their internal numbers
2370,@marktenenholtz,2022-02-23 02:01:59+00:00,https://twitter.com/marktenenholtz/status/1496304298510196737,@mElantkowski This is the critical flaw they had in back testing
2371,@marktenenholtz,2022-02-23 02:01:33+00:00,https://twitter.com/marktenenholtz/status/1496304190997635073,@mElantkowski This is assuming your residuals are IID. Their residuals are not. They were always the worst case scenario because the bid was only accepted if they were overpaying.
2372,@marktenenholtz,2022-02-23 01:21:14+00:00,https://twitter.com/marktenenholtz/status/1496294043130769408,@Danai_data The regression model was totally fine. Their decision analysis was not.
2373,@marktenenholtz,2022-02-23 01:20:44+00:00,https://twitter.com/marktenenholtz/status/1496293917310001153,@CenHu People are greedy for trying to get the best offer for their homes?
2374,@marktenenholtz,2022-02-23 01:20:27+00:00,https://twitter.com/marktenenholtz/status/1496293848284286978,@DhirajSIngla15 https://t.co/2YuusRtdhX
2375,@marktenenholtz,2022-02-22 21:25:07+00:00,https://twitter.com/marktenenholtz/status/1496234623286013952,@gusthema https://t.co/L1unnWI6mO
2376,@marktenenholtz,2022-02-22 21:24:41+00:00,https://twitter.com/marktenenholtz/status/1496234515798364160,@gusthema Thanks Luis!
2377,@marktenenholtz,2022-02-22 20:47:34+00:00,https://twitter.com/marktenenholtz/status/1496225171547598859,@D_v_z_v I mean... I guess it's both. They would have known how wrong it was if they backtested properly.
2378,@marktenenholtz,2022-02-22 20:45:41+00:00,https://twitter.com/marktenenholtz/status/1496224699826724872,@DataGeek15 https://t.co/aAmeg42HL0
2379,@marktenenholtz,2022-02-22 20:43:23+00:00,https://twitter.com/marktenenholtz/status/1496224122166259715,"Okay folks, we need to talk.

If you think the problem here is that they needed a ""human"" factor, or that they did poor regression analysis, you're wrong.

The problem is their *decision analysis*. 

THIS is what failed in backtesting, NOT the regression."
2380,@marktenenholtz,2022-02-22 20:39:08+00:00,https://twitter.com/marktenenholtz/status/1496223052337758212,@D_v_z_v Their regression analysis was incredibly accurate. Their decision analysis was not.
2381,@marktenenholtz,2022-02-22 20:38:24+00:00,https://twitter.com/marktenenholtz/status/1496222864932024323,@D_v_z_v Not really. Their models were just as accurate in falling markets. It’s just that their decision analysis was simply “buy if zestimate &gt;&gt; asking price” and that killed them
2382,@marktenenholtz,2022-02-22 20:36:23+00:00,https://twitter.com/marktenenholtz/status/1496222358549446662,@eaglyl No. They just misidentified the role of the model.
2383,@marktenenholtz,2022-02-22 14:04:00+00:00,https://twitter.com/marktenenholtz/status/1496123614214692868,@LBacaj Thanks Louie!
2384,@marktenenholtz,2022-02-22 14:01:08+00:00,https://twitter.com/marktenenholtz/status/1496122890831511572,"@LBacaj That’s the issue with average accuracy metrics, though — some big misses get masked by many “easy” guesses (like large suburban areas, probably)"
2385,@marktenenholtz,2022-02-22 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1496107554606247936,"I hope you learned something!

Follow me @marktenenholtz for more high-signal ML content.

Let’s build more robust ML models together."
2386,@marktenenholtz,2022-02-22 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1496107551791874048,"This is why model evaluation is so difficult, and yet so incredibly important to get right.

As a field, we’re still in the early phases of understanding how to account for adversarial conditions.

I hope this thread drives home just how important they are to consider!"
2387,@marktenenholtz,2022-02-22 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1496107548943867908,"However, once the market cooled off, they were exposed.

A successful house-flipping operation can still succeed in a cool market.

But in Zillow’s case, it just uncovered the deficiencies that were otherwise masked by a rising market."
2388,@marktenenholtz,2022-02-22 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1496107546100510723,"Finally the death blow — all of their simulations took place during a market where housing prices were significantly rising.

This meant that if they screwed up a bid, they were probably still going to survive since their portfolio was constantly growing in value."
2389,@marktenenholtz,2022-02-22 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1496107543101280257,"So, if Zillow put in a bid that wasn’t high enough, the homeowner would reject it.

But if Zillow put in a bid that was way too high, the homeowner would definitely accept it.

Basically, Zillow was getting the worst case scenario on almost all of their purchases."
2390,@marktenenholtz,2022-02-22 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1496107539812937730,"The second part of their disadvantage was an adversarial market.

Remember how I mentioned average accuracy metrics don’t capture the big misses?

Well, the big misses likely come in situations when the homeowner has a key piece of info that Zillow is missing."
2391,@marktenenholtz,2022-02-22 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1496107536394686464,"How does this happen in the housing market?

Well, the home owner and their real estate agent inevitably have more information on the home than Zillow.

What happens, for instance, if the house has a strong odor or big plumbing issues?

In the long run, this hurts Zillow a lot."
2392,@marktenenholtz,2022-02-22 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1496107533395582979,"Sure, on average, they’re going to be very accurate.

But this is the problem with average accuracy metrics — they mask big errors.

It’s inevitable that even the Zestimate score with up to 99% will miss big on some homes."
2393,@marktenenholtz,2022-02-22 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1496107530207985670,"The first part of their failure was a massive information disadvantage.

I know what you’re thinking:

“But Mark, you just said they have a huge information advantage and a super accurate price estimate for homes!”"
2394,@marktenenholtz,2022-02-22 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1496107526932205568,"However, they didn’t just thrust themselves into the market.

Over the course of ~3 years, they simulated their strategy.

Inspired by successful simulations, they began to purchase tens of thousands of homes.

If their simulation was so successful, though, how’d they fail?"
2395,@marktenenholtz,2022-02-22 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1496107523144753158,"Zillow is really good at pricing homes.

I mean *really* good.

Their Zestimate score reportedly has an average accuracy of 96%, and closer to 99% on homes up for sale.

With all this data available to them, they could carefully back-test through all sorts of market conditions."
2396,@marktenenholtz,2022-02-22 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1496107519877414918,"Anyone who has even wanted to buy or sell a home before knows how arduous of a process it is.

It’s a difficult process with tons of back-and-forth, and usually takes months.

So what if someone buy from impatient sellers and sell to impatient buyers?

Enter, Zillow:"
2397,@marktenenholtz,2022-02-22 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1496107516324835331,"Zillow’s home buying business lost them $500,000,000, 25% of their stock value, and 25% of their workforce.

How did this happen to a company with so much data on housing prices?

Bad model evaluation.

Here’s the fatal error they made that you must avoid when deploying models🧵"
2398,@marktenenholtz,2022-02-22 01:57:23+00:00,https://twitter.com/marktenenholtz/status/1495940752488243201,"@LBacaj @balajis In my mind I always thought the ideal society was a 70% as opposed to a 100% democracy.

A pretty strong and unified majority but still plenty of people advocating for improvements."
2399,@marktenenholtz,2022-02-22 01:55:38+00:00,https://twitter.com/marktenenholtz/status/1495940312660905985,@prthgo Lol once I hit it I was just sad I didn’t have 10k 😡
2400,@marktenenholtz,2022-02-22 01:37:55+00:00,https://twitter.com/marktenenholtz/status/1495935853088153601,"@bhutanisanyam1 Okay, looks like I’m adding “biking on mountains in India” to my bucket list!"
2401,@marktenenholtz,2022-02-21 23:27:04+00:00,https://twitter.com/marktenenholtz/status/1495902924488257540,@twelvespot @3blue1brown I love that way of thinking about it 😂
2402,@marktenenholtz,2022-02-21 18:39:35+00:00,https://twitter.com/marktenenholtz/status/1495830577743900683,@willjsteele @python_engineer @_brohrer_ @vboykis Thanks for the shoutout!
2403,@marktenenholtz,2022-02-21 14:02:01+00:00,https://twitter.com/marktenenholtz/status/1495760725607948290,@_Perceptron_ @3blue1brown Agreed!
2404,@marktenenholtz,2022-02-21 13:06:37+00:00,https://twitter.com/marktenenholtz/status/1495746785658081280,"Check out the playlist here: https://t.co/pCyuZlOMT3

While you're at it, I'd recommend checking out the rest of his channel. It's a gold mine!

Follow me @marktenenholtz for more intuitive and high-signal ML content!"
2405,@marktenenholtz,2022-02-21 13:06:37+00:00,https://twitter.com/marktenenholtz/status/1495746782319431680,"Neural networks can be hard to understand until you develop an intuition for them.

I found @3blue1brown's series on them to be incredibly useful for this. 

Why? Beautiful visualizations.

Understanding the math is helpful, but pairing math with intuition is a game changer. https://t.co/OXTn2qpqV2"
2406,@marktenenholtz,2022-02-20 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1495382744481746946,"When you learn ML, there’s no replacement for project-based learning.

The internet is bursting at the seams with courses, blogs, and bootcamps, and you should take advantage of them.

But if you ever want knowledge to stick, you have to apply it."
2407,@marktenenholtz,2022-02-19 21:09:36+00:00,https://twitter.com/marktenenholtz/status/1495143553672134659,"@TheZachMueller Think about what you were learning 2 years ago and try to create content around that.

I struggle to write at times, but for the most part the content flows a lot easier since I took that point of view"
2408,@marktenenholtz,2022-02-19 20:24:12+00:00,https://twitter.com/marktenenholtz/status/1495132129994416133,@TheZachMueller @abhi1thakur Welcome to the world of buzzwords — I would have pegged you as an MLOps Engineer
2409,@marktenenholtz,2022-02-19 16:51:13+00:00,https://twitter.com/marktenenholtz/status/1495078528337027075,@inf800 The most clear and the most concise :)
2410,@marktenenholtz,2022-02-19 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1495020358247739399,"Bad solutions are clever and complex.

Good solutions are clear and concise."
2411,@marktenenholtz,2022-02-19 01:40:02+00:00,https://twitter.com/marktenenholtz/status/1494849224940433409,@wightmanr Updated CUDA and drivers yesterday after a long and arduous process… then I realized I forgot to install cudNN
2412,@marktenenholtz,2022-02-18 13:08:08+00:00,https://twitter.com/marktenenholtz/status/1494660003730542594,@TheZachMueller @the_antlr_guy @jeremyphoward I always forget the link 😅
2413,@marktenenholtz,2022-02-18 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1494657990128857089,"To be clear, you don't need most of this to get started.

As they say in the intro, this is for folks that have gotten started learning deep learning but are looking to solidify their understanding of the math.

Follow me @marktenenholtz for more high-signal content like this!"
2414,@marktenenholtz,2022-02-18 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1494657986643456004,"The Matrix Calculus You Need For Deep Learning

Many folks want to get started with deep learning, but don't feel they have the math skills.

Written by @the_antlr_guy and @jeremyphoward, this paper assumes no more knowledge than high-school math. https://t.co/YSP2nb7tin"
2415,@marktenenholtz,2022-02-18 12:51:18+00:00,https://twitter.com/marktenenholtz/status/1494655764669083650,@ChristophMolnar Thanks Christoph! Glad you found value in it.
2416,@marktenenholtz,2022-02-18 12:39:39+00:00,https://twitter.com/marktenenholtz/status/1494652834591162372,@prthgo Congrats!
2417,@marktenenholtz,2022-02-17 18:02:35+00:00,https://twitter.com/marktenenholtz/status/1494371712963690498,"@svpino Keras, @fastdotai, and @weights_biases are all great examples"
2418,@marktenenholtz,2022-02-17 18:00:05+00:00,https://twitter.com/marktenenholtz/status/1494371087320158208,"Qualities of a great MLOps library:

• Best practices are automated
• Good default settings
• No domain-specific languages
• Progressive disclosure of complexity
• User-centric API

Most libraries don’t meet these standards.

But the ones that do will 10x your productivity."
2419,@marktenenholtz,2022-02-17 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1494295601206349826,"Do yourself a favor: learn to sell your models.

Good sales leads stakeholders to:

• Buy-in quickly
• Trust ML solutions in the future
• Anticipate future ML use-cases

Bad sales lead to:

• Over-scrutinized results
• Wasted time in back-and-forths
• Distrust and siloing"
2420,@marktenenholtz,2022-02-16 23:54:20+00:00,https://twitter.com/marktenenholtz/status/1494097847947599872,"@charles_irl Great point. I find them to be most useful when they significantly reduce complexity and/or significantly improve results.

Otherwise, it leads to the problems you’re talking about."
2421,@marktenenholtz,2022-02-16 21:08:31+00:00,https://twitter.com/marktenenholtz/status/1494056118028537858,@LBacaj @tunguz Already going in that direction!
2422,@marktenenholtz,2022-02-16 13:07:47+00:00,https://twitter.com/marktenenholtz/status/1493935138702561287,@bce3227 Try this for NLP
2423,@marktenenholtz,2022-02-16 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1493933235205672963,"7. Repeat steps 5 + 6

Your goal now should be to keep trying to overfit and then reduce that overfitting.

You can stop when you run out of time or when you're no longer seeing improvements in validation accuracy when you regularize."
2424,@marktenenholtz,2022-02-16 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1493933232412266502,"There are some tricks you can use, but they're more situational.

Here are some of them:

• Crop out as much background as possible
• Decrease batch size
• Add a learning rate scheduler (I do this very often)

These are helpful, but usually not as much as the previous ones."
2425,@marktenenholtz,2022-02-16 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1493933229593616397,"6. Reduce overfitting

Now that you've started to overfit, it's time to figure out how to make your model generalize better.

I usually follow these 4 steps, in order:

1. Get more data
2. Augmentation
3. Regularization (i.e. dropout, weight decay)
4. Smaller architecture"
2426,@marktenenholtz,2022-02-16 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1493933226640822282,"5. Overfit

In this context, I mean try to get your training error as low as possible.

Try and pull different levers to accomplish this, such as:

• Increases to model size
• Increased image size
• Bigger output head

Make sure you only try one new thing at a time!"
2427,@marktenenholtz,2022-02-16 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1493933222597578765,"This is also where you should set up some QA steps.

Here are some I use (mostly borrowed from @karpathy):

• Set inputs to all zeros and compare loss to normal run
• Visualize some samples right before they enter your model
• Ensure training loss is decreasing"
2428,@marktenenholtz,2022-02-16 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1493933219766362117,"4. Set up your pipeline

I start off with an extremely minimal pipeline. Here's my checklist:

• Fixed random seed
• No augmentation
• Small pretrained model (resnet18, efficientnet-b0)
• Adam optimizer with no scheduler
• Implement logging
• Sanity check your metric"
2429,@marktenenholtz,2022-02-16 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1493933216956248068,"2. Create a human baseline

While you're scrolling through images, try to get a gauge of your own accuracy.

Kaggle is nice in the sense that you get a leaderboard as a benchmark, but if you don't have this, a human benchmark is much better than a naive one, like the target mean."
2430,@marktenenholtz,2022-02-16 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1493933214108319745,"You can use Jupyter widget to make this faster, also.

While you're scrolling, ask yourself questions like: 

• Does spatial position matter?
• Are there any data issues (i.e. duplicates)
• How noisy is the target?
• Is the target ever occluded?"
2431,@marktenenholtz,2022-02-16 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1493933211335811076,"1. Immerse yourself in the data

The great part about image data is that it's as visual as it gets.

You should scroll through as many images as possible and try to find patterns.

The best models come from those who have spent hours on this, not minutes."
2432,@marktenenholtz,2022-02-16 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1493933208525631488,"I spent 400+ hours training computer vision models last year on my road to Kaggle Master.

I recently revisited my code and notes from those competitions and distilled them into a repeatable process that anyone can follow.

7 steps to train any computer vision model 🧵"
2433,@marktenenholtz,2022-02-16 02:41:18+00:00,https://twitter.com/marktenenholtz/status/1493777477109432320,@dvassallo @Austen Do you still think it’s speculation if you’re payed market salary/benefits and you get equity as a sweetener?
2434,@marktenenholtz,2022-02-16 01:13:38+00:00,https://twitter.com/marktenenholtz/status/1493755414747525120,@mhajabri Like running a model in production
2435,@marktenenholtz,2022-02-16 01:06:02+00:00,https://twitter.com/marktenenholtz/status/1493753504040513542,@Cogitoe79321738 I’d start with a T4/multiple T4’s and move up to an A100 if that isn’t enough
2436,@marktenenholtz,2022-02-16 01:05:11+00:00,https://twitter.com/marktenenholtz/status/1493753289216860166,@rasbt Well that’s one of the biggest compliments I’ve ever gotten on here! Thanks again 😊
2437,@marktenenholtz,2022-02-16 00:53:19+00:00,https://twitter.com/marktenenholtz/status/1493750303421722627,@rasbt Thanks Sebastian! Glad you liked it
2438,@marktenenholtz,2022-02-15 23:38:12+00:00,https://twitter.com/marktenenholtz/status/1493731399970418688,@jmart_ds Thanks Jorge!
2439,@marktenenholtz,2022-02-15 21:55:12+00:00,https://twitter.com/marktenenholtz/status/1493705477267591176,"@vad13irt If you’re asking the question, you probably should stick to Kaggle/Colab"
2440,@marktenenholtz,2022-02-15 18:16:49+00:00,https://twitter.com/marktenenholtz/status/1493650519193182209,"@EMostaque My focus here was on data center GPUs, but A6000s are definitely viable for consumer rigs"
2441,@marktenenholtz,2022-02-15 17:16:05+00:00,https://twitter.com/marktenenholtz/status/1493635237431107585,@svpino No problem! Glad you learned something.
2442,@marktenenholtz,2022-02-15 15:34:32+00:00,https://twitter.com/marktenenholtz/status/1493609678982549512,@HegdeIshan No. They're very expensive.
2443,@marktenenholtz,2022-02-15 15:11:38+00:00,https://twitter.com/marktenenholtz/status/1493603915971051520,@heyblake +1 for a zero-indexed loop
2444,@marktenenholtz,2022-02-15 15:08:16+00:00,https://twitter.com/marktenenholtz/status/1493603069434765314,@SanjogMehta Glad I could help!
2445,@marktenenholtz,2022-02-15 14:57:38+00:00,https://twitter.com/marktenenholtz/status/1493600396425764871,"@rasbt Yeah V100s are quickly becoming inefficiently priced.

There’s not much of a reason to with one over what you’re doing or a single A100."
2446,@marktenenholtz,2022-02-15 14:36:06+00:00,https://twitter.com/marktenenholtz/status/1493594973966639107,"@TheZachMueller @JacopoAttolini It's a shame considering how user-friendly it's become to calculate shap values, but I tried explaining them a couple of times and noped the heck outta that"
2447,@marktenenholtz,2022-02-15 13:06:33+00:00,https://twitter.com/marktenenholtz/status/1493572438898192384,@prthgo No problem!
2448,@marktenenholtz,2022-02-15 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1493570834828431360,"I hope you learned something and feel less intimidated looking at GPU offerings!

Follow me @marktenenholtz and you'll learn a lot more about how to train big models with those fancy GPUs you've rented."
2449,@marktenenholtz,2022-02-15 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1493570831993106432,"NVIDIA has been blowing it away with recent advancements, so usually, the newest GPUs are the most cost-efficient.

My general recommendations:

• A100 for model training
• T4 for inference workloads"
2450,@marktenenholtz,2022-02-15 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1493570829136760838,"6. A100

The A100 is the newest data center GPU. 

It brings upgraded tensor cores to the table and most benchmarks show 3x+ faster training compared to the V100.

It also comes with up to 80GB VRAM!

The price tag might be big, but it's usually worth it over the V100."
2451,@marktenenholtz,2022-02-15 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1493570826347577344,"5. V100

Until recently, the V100 was by far the most powerful model training GPU and a huge upgrade over the P100.

While it has the same amount of VRAM, it has many more CUDA cores and introduces Tensor Cores.

These upgrades generally make it more cost-efficient than the P100."
2452,@marktenenholtz,2022-02-15 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1493570823516532741,"4. P100

The P100 was a big improvement for model training workloads over the K80 when released.

While it has less RAM (16GB), the P100 packs way more compute and even can see memory savings from using mixed-precision training (although no tensor cores!)"
2453,@marktenenholtz,2022-02-15 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1493570820584599559,"3. T4

The T4 was released two years after the P4 and is a significant upgrade for inference workloads.

Why? Extremely low power consumption, tensor cores, and plenty (16GB) of VRAM.

They're nice and cheap, so if you have an inference workload, I'd strongly consider a T4."
2454,@marktenenholtz,2022-02-15 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1493570817711509511,"2. P4

The P4 was the earliest released GPU for inference workloads in this list.

At the time of its release (2016), its main value proposition was its low power consumption.

Nowadays, you may even find it priced higher than its upgraded version (the T4), so I'd avoid it."
2455,@marktenenholtz,2022-02-15 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1493570814960013314,"1. K80

Released in 2015, the K80 contained a lot of VRAM for the time (24 GB) and was considered the go-to data center GPU for model training.

However, the GPU came before tensor cores and is relatively weak by today's standards, so I'd avoid these unless you're just learning."
2456,@marktenenholtz,2022-02-15 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1493570812154048513,"NVIDIA has released a lot of GPUs over the years, so I decided to focus on the most common ones available on cloud services.

You may ask yourself why there are no RTX 20-series or 30-series GPUs, and it's because they're forbidden from inclusion in data centers.

Here we go!"
2457,@marktenenholtz,2022-02-15 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1493570809339658243,"If you want to get serious about training big deep learning models, you'll need some serious compute.

However, knowing what GPU to rent can be very intimidating and hard to research.

So, I decided to provide you with a buyer's guide!

Here's your guide to data center GPUs 🧵"
2458,@marktenenholtz,2022-02-15 12:56:09+00:00,https://twitter.com/marktenenholtz/status/1493569820440371207,"@YAML92 If I can verify that there are no preprocessing errors, I either transform the target (e.g. log transform) or I pick a different model"
2459,@marktenenholtz,2022-02-15 12:54:52+00:00,https://twitter.com/marktenenholtz/status/1493569499660005379,@JacopoAttolini Shap is good but I usually use permutation feature importance since it’s easier to explain to stakeholders
2460,@marktenenholtz,2022-02-15 12:54:04+00:00,https://twitter.com/marktenenholtz/status/1493569297209344004,@S_Esteban_ Yes! Proper validation is essential for doing error analysis
2461,@marktenenholtz,2022-02-15 12:53:17+00:00,https://twitter.com/marktenenholtz/status/1493569102253899777,@kunsv Logging tools like Tensorboard and @weights_biases can do that for you
2462,@marktenenholtz,2022-02-14 13:14:31+00:00,https://twitter.com/marktenenholtz/status/1493212057872867334,"@prthgo It takes practice, but it’s extremely useful"
2463,@marktenenholtz,2022-02-14 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1493208435789164547,"I hope you learned something about how to analyze your models.

Follow me @marktenenholtz and we'll create the best ones together!"
2464,@marktenenholtz,2022-02-14 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1493208432991608834,"5. Follow the breadcrumbs

Once you see some patterns in the previous steps, try to follow the trail back to the model.

In other words -- what are you giving to your model, and why is it having trouble turning it into good results?

This is an iterative process! Repeat steps 3+4"
2465,@marktenenholtz,2022-02-14 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1493208430047215622,"4. Slice your data

Now that you've seen a bunch of examples, try to confirm your observations on a global level.

Slice your dataset by all the individual categories (i.e. separate cats and dogs) and see what your model's error is on each category."
2466,@marktenenholtz,2022-02-14 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1493208427262136321,"3. Visualize individual predictions

I try to scroll through as many predictions from my model as possible, maybe even hundreds.

Not only can this surface obvious bugs, but it can give you ideas of what to try next since you'll see first-hand where your model is tripping up."
2467,@marktenenholtz,2022-02-14 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1493208424389259270,"2. Be your own model

This mostly applies to image and NLP tasks, but I usually try to create my own human benchmark.

Any error metric in a vacuum is essentially useless, so this gives you a good idea of what kind of irreducible error you're dealing with."
2468,@marktenenholtz,2022-02-14 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1493208421293641728,"1. Visualize the prediction distribution

I usually start here.

A complex-enough model should *generally* match its prediction distribution to the target distribution.

All this tells you, usually, is that it's at least predicting something that approximates that target."
2469,@marktenenholtz,2022-02-14 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1493208418512822275,"Error Analysis, 101

The best way to improve your models is to see what they're doing wrong.

This is called Error Analysis.

Done correctly, this gives you a clear path towards improving your models.

Ignore it, and you'll just bump around in the dark.

Here are 5 principles🧵"
2470,@marktenenholtz,2022-02-13 13:43:43+00:00,https://twitter.com/marktenenholtz/status/1492857019228991490,"@Crowningsuns @jeremyphoward @GuggerSylvain Either one will serve you just fine. I think folks generally prefer the “feel” of PyTorch to TF when choosing between the two, though."
2471,@marktenenholtz,2022-02-13 13:42:28+00:00,https://twitter.com/marktenenholtz/status/1492856701703405569,@jleplat @jeremyphoward @GuggerSylvain Couldn’t agree more
2472,@marktenenholtz,2022-02-13 13:26:08+00:00,https://twitter.com/marktenenholtz/status/1492852590698188800,"@svpino We need good MLOps engineers in the space, so anyone doing this is very valuable!"
2473,@marktenenholtz,2022-02-13 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1492846034904485890,"If you want to build great models, you should try collecting your own data for a project.

If you don't understand what's going into your model, you'll never understand what's coming out.

It's sneakily the best way to develop your EDA skills, since you learn what to look for!"
2474,@marktenenholtz,2022-02-13 12:55:54+00:00,https://twitter.com/marktenenholtz/status/1492844983132565506,@enjaxy @jeremyphoward @GuggerSylvain It’s meant for beginners
2475,@marktenenholtz,2022-02-13 03:20:03+00:00,https://twitter.com/marktenenholtz/status/1492700066754220037,"@paulg In that case, do you read income statements?"
2476,@marktenenholtz,2022-02-13 03:08:28+00:00,https://twitter.com/marktenenholtz/status/1492697152300142596,@bhutanisanyam1 @truehonesti @jeremyphoward @GuggerSylvain @weights_biases 😜
2477,@marktenenholtz,2022-02-13 01:58:59+00:00,https://twitter.com/marktenenholtz/status/1492679665630298112,@prthgo gm :(
2478,@marktenenholtz,2022-02-12 21:13:00+00:00,https://twitter.com/marktenenholtz/status/1492607695328890885,@SilverDeGeneral @jeremyphoward @GuggerSylvain You’re probably fine. Just familiarity with the language generally
2479,@marktenenholtz,2022-02-12 20:57:28+00:00,https://twitter.com/marktenenholtz/status/1492603785843249155,"@pk_dash @jeremyphoward @GuggerSylvain As long as you have high school level math, they’ll teach you what you need in that book"
2480,@marktenenholtz,2022-02-12 20:02:02+00:00,https://twitter.com/marktenenholtz/status/1492589836859068417,@kunsv MLOps is great!
2481,@marktenenholtz,2022-02-12 20:01:45+00:00,https://twitter.com/marktenenholtz/status/1492589762825396227,@BrianMugo_ @jeremyphoward @GuggerSylvain I can't say without knowing what data you're trying to build the model with
2482,@marktenenholtz,2022-02-12 18:27:26+00:00,https://twitter.com/marktenenholtz/status/1492566026894118918,"@GergelyOrosz The same thing is happening in machine learning.

Surprised how few understand just how much of their value is derived from having top talent!"
2483,@marktenenholtz,2022-02-12 15:57:40+00:00,https://twitter.com/marktenenholtz/status/1492528337704607761,"@JFPuget I'm not saying it is, just that it's perceived as being. I have no clue why.

It's funny, in my Master's program, it was easier to get into the reinforcement learning and deep learning courses if you were in the CS program rather than the analytics program."
2484,@marktenenholtz,2022-02-12 15:56:18+00:00,https://twitter.com/marktenenholtz/status/1492527993742364678,"I hope you gained something from this!

For more high-signal machine learning content, follow me @marktenenholtz 

I also just started a newsletter! Go ahead and subscribe here: https://t.co/34sBfotOUZ

First issue drops tomorrow!"
2485,@marktenenholtz,2022-02-12 15:41:53+00:00,https://twitter.com/marktenenholtz/status/1492524367204528128,@JFPuget It’s an interesting question. I’ve always assumed it has something to do with ML seeming more CS-adjacent which therefore creates a better MLOps toolset by bringing in that audience.
2486,@marktenenholtz,2022-02-12 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1492483667716804611,"I've always advocated a hands-on approach to learning, and this book definitely delivers on that.

They also teach you from the basics, so you'll develop a strong foundational understanding.

You can buy it on Amazon, but it's also free here: https://t.co/Z8zRlszd81"
2487,@marktenenholtz,2022-02-12 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1492483664352985088,"You don't need a Ph.D. to learn deep learning.

All you need is a little Python experience and to read Deep Learning for Coders with fastai and PyTorch!

@jeremyphoward and @GuggerSylvain are who I learned DL from, so I can't recommend it highly enough! https://t.co/hDRl7174PB"
2488,@marktenenholtz,2022-02-11 14:04:07+00:00,https://twitter.com/marktenenholtz/status/1492137377137147910,"@tunguz Forget my apartment, I'll just rent a DGX station and live inside that"
2489,@marktenenholtz,2022-02-11 13:44:45+00:00,https://twitter.com/marktenenholtz/status/1492132502537920514,@LukasPlatinsky Agreed! We’re lucky to have such incredible devs in the community
2490,@marktenenholtz,2022-02-11 13:35:25+00:00,https://twitter.com/marktenenholtz/status/1492130152557559811,@prthgo @fchollet Thanks Parth!
2491,@marktenenholtz,2022-02-11 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1492121300453650436,"I hope you learned something about why these frameworks are so successful and that you'll apply these principles in your own work. If you did, please do drop me a follow @marktenenholtz.

We need more user-centric MLOps tools in the world, and this is how they are built."
2492,@marktenenholtz,2022-02-11 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1492121297412771841,"To sum up the philosophy:

1. Quick experimentation
2. Infinite hackability
3. Low cognitive load
4. Documentation &gt; features
5. Anticipate common mistakes

For more of Francois' thoughts, check this out: https://t.co/gRa79vCpKX

And for Jeremy Howard's: https://t.co/ZA5YTVUqPX"
2493,@marktenenholtz,2022-02-11 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1492121294376112133,"Principle 5: Anticipate common mistakes

Pay attention to how your API is commonly misused.

Sometimes it makes sense to rewrite that API, but usually it's much easier to just provide helpful error messages.

Don't drive users to this level of frustration:
https://t.co/YCp89iuifc"
2494,@marktenenholtz,2022-02-11 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1492121291368792065,"Principle 4: Documentation &gt; features

Users spend more time reading documentation than writing code.

If a user can't understand what a feature does from the documentation, they will not use it. 

That means you completely wasted your time implementing it!"
2495,@marktenenholtz,2022-02-11 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1492121288575373318,"Principle 3: Low cognitive load

Workflows should be simple for the user, and everything that can be automated should be (including defaults).

Also, after performing a task once or twice, users should be able to do it again without looking up a tutorial or documentation."
2496,@marktenenholtz,2022-02-11 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1492121285811310592,"Principle 2: Infinite hackability

If a user wants to customize a specific step in the process, they should be able to dive in as deep as they want without knowing how the rest of the library functions.

Rigid abstractions die.

User-centric APIs thrive."
2497,@marktenenholtz,2022-02-11 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1492121283001085952,"Principle 1: Quick experimentation

To fit a simple model, there is almost no code to write.

Do a little googling and you'll find models trained in less than 20 or 30 lines of code.

Once users are satisfied with that, they can dive in a little further."
2498,@marktenenholtz,2022-02-11 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1492121277439504386,"It's no secret that writing code to train deep learning models is complex.

To many newcomers, it's simply too overwhelming... until they come across Keras.

It's all because the library is built in a way that prioritizes user experience."
2499,@marktenenholtz,2022-02-11 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1492121274562220033,"Francois Chollet is the main author of arguably the most influential ML framework, Keras.

What makes his work so successful? Designing user-centric APIs.

How does @fchollet accomplish this? 

Here are the 5 principles🧵"
2500,@marktenenholtz,2022-02-11 00:53:38+00:00,https://twitter.com/marktenenholtz/status/1491938443940749318,@rasbt I am a member of the church of Adam+cosine annealing
2501,@marktenenholtz,2022-02-11 00:39:00+00:00,https://twitter.com/marktenenholtz/status/1491934761694248974,"@rasbt Yeah, tuning SGD is one of those optimizations I may do at the very end to squeeze out the last bit of performance. Other than that, it's Adam all the way.

I've been seeing more people talk about utilizing schedulers with cooldown though, so I should give that I shot with SGD."
2502,@marktenenholtz,2022-02-11 00:27:47+00:00,https://twitter.com/marktenenholtz/status/1491931940198531099,"@rasbt Well-tuned SGD always beats Adam, but the “well-tuned” part is what makes me still default to Adam"
2503,@marktenenholtz,2022-02-11 00:26:10+00:00,https://twitter.com/marktenenholtz/status/1491931530134011920,@tunguz torch xla pls :(
2504,@marktenenholtz,2022-02-11 00:19:14+00:00,https://twitter.com/marktenenholtz/status/1491929788592037889,"@ykilcher @hardmaru Yeah I think the answer that it gives is something like the answer to the closest thing to the actual math problem that it has seen before.

It could be doing some slightly more fancy interpolation, though."
2505,@marktenenholtz,2022-02-10 16:55:01+00:00,https://twitter.com/marktenenholtz/status/1491817996645543943,@__mharrison__ Absolutely!
2506,@marktenenholtz,2022-02-10 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1491758887057330182,"Matt is a wonderful Python instructor and this book is no exception.

Pandas is a library with an incredible depth of features, and you should definitely learn from the best if you want to use it to its fullest.

For more high-signal resources for ML, follow me @marktenenholtz"
2507,@marktenenholtz,2022-02-10 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1491758883693416448,"If you want to build effective machine learning models, you need to master data manipulation.

The better you are with libraries like Pandas, the better your pipelines will be and the faster you'll be able to experiment.

Enter Effective Pandas by @__mharrison__ https://t.co/aOszaTwfrf"
2508,@marktenenholtz,2022-02-09 23:56:05+00:00,https://twitter.com/marktenenholtz/status/1491561574372458502,@TheZachMueller If it's not then I genuinely feel bad for anyone outside the States
2509,@marktenenholtz,2022-02-09 21:17:03+00:00,https://twitter.com/marktenenholtz/status/1491521551409233922,@cgarciae88 @giffmana Did you mean @_clashluke ?
2510,@marktenenholtz,2022-02-09 18:13:07+00:00,https://twitter.com/marktenenholtz/status/1491475262336348162,@curiovana Kaggle competitions are fantastic for this
2511,@marktenenholtz,2022-02-09 16:43:31+00:00,https://twitter.com/marktenenholtz/status/1491452712126652418,@PythonWithRune Agreed! Perseverance is key.
2512,@marktenenholtz,2022-02-09 16:39:14+00:00,https://twitter.com/marktenenholtz/status/1491451636392157187,@sGx_tweets More than a little googling involved for me too!
2513,@marktenenholtz,2022-02-09 15:43:58+00:00,https://twitter.com/marktenenholtz/status/1491437726272737281,@sGx_tweets I built a golf betting model following a methodology I found online because it involved mixed effects modeling and I wanted to learn more about statistical modeling
2514,@marktenenholtz,2022-02-09 15:06:27+00:00,https://twitter.com/marktenenholtz/status/1491428287587184644,@tunguz @Onalytica Nice Bojan! Loved reading it
2515,@marktenenholtz,2022-02-09 14:02:38+00:00,https://twitter.com/marktenenholtz/status/1491412225944600580,@lu19000 @Al_Grigor That’s good! Applying knowledge right after you learn it is an effective method.
2516,@marktenenholtz,2022-02-09 13:54:38+00:00,https://twitter.com/marktenenholtz/status/1491410210988048387,@aakashg0 Haha that was definitely a reply I didn’t expect to get 😅
2517,@marktenenholtz,2022-02-09 13:51:29+00:00,https://twitter.com/marktenenholtz/status/1491409419225092097,"@SahilBloom Great timing, Sahil — I just wrote about this specifically for folks who are learning machine learning.

Love your take on it!"
2518,@marktenenholtz,2022-02-09 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1491396501846724611,"Before you know it, project-based learning will become a natural part of your process.

You'll find that you develop much deeper, more nuanced understanding of everything you learn.

If this helped you out, follow me @marktenenholtz for more high-signal content!"
2519,@marktenenholtz,2022-02-09 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1491396498986209280,"5. Repeat steps 3+4 until completion

The most important part of the project is to complete it.

80% of the learning usually happens in the last 20%.

It's easy to get lost in your work along the way, and this forces you to contextualize everything you've accomplished."
2520,@marktenenholtz,2022-02-09 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1491396496083750915,"4. Seek out help

This is the moment where it is perfect to dip into the well of the internet courses we spoke of before.

Everything you learn will now be reinforced 100x more because you have the gift of context.

Use the internet as a means to achieve your end, not as the end."
2521,@marktenenholtz,2022-02-09 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1491396493239939072,"3. Work until you get stuck

Inevitably, you will get stuck. That's a part of the learning process.

If you don't get stuck anywhere along the way or at least hit a bumpy part, you didn't challenge yourself enough.

The ideal project should make you think, but not stress you out."
2522,@marktenenholtz,2022-02-09 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1491396490241052672,"2. Come up with a project to test those skills

Just pick something, anything! I promise, you aren't curing cancer.

Find something that will require you to apply the skills you want to learn and will capture your interest"
2523,@marktenenholtz,2022-02-09 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1491396487275634688,"1. Pick a couple of skills to develop

The key here is to keep it manageable. Don't bite off more than you can chew.

These can be anything, but try to be as specific as possible.

""Machine learning"" is not a specific skill.

""Regression with linear models"" is much better."
2524,@marktenenholtz,2022-02-09 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1491396484389949446,"That contract dictates that the creator is distilling their knowledge for their consumption.

However, to appreciate their knowledge, you'll need to follow the same steps they took to acquire it:

Application.

So, how do we approach application?"
2525,@marktenenholtz,2022-02-09 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1491396481412046852,"You will never truly understand the material until you apply it.

It's incredible how much access we have to ML learning material, but it comes with a *huge* responsibility that is implicit in the contract between the creator and the learner."
2526,@marktenenholtz,2022-02-09 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1491396478501154820,"The internet is bursting at the seams with introductory content on machine learning.

Bootcamps, blogs, online courses, you name it. These are great sources of exposure. 

However, that's all they are -- exposure."
2527,@marktenenholtz,2022-02-09 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1491396475628048386,"When you learn data science, there’s no replacement for project-based learning.

Courses and blogs are fantastic resources, and you should take advantage of them.

But if you ever want knowledge to stick, you have to apply it.

Here are the 5 steps to project-based learning 🧵"
2528,@marktenenholtz,2022-02-09 12:30:46+00:00,https://twitter.com/marktenenholtz/status/1491389105678483457,@Sinn1717 Definitely!
2529,@marktenenholtz,2022-02-09 02:56:08+00:00,https://twitter.com/marktenenholtz/status/1491244496574504961,"@tunguz Excuses, excuses… if you were in a bomb shelter, I bet you had a lot of time on your hands!"
2530,@marktenenholtz,2022-02-09 02:52:25+00:00,https://twitter.com/marktenenholtz/status/1491243561614217217,@analokmaus Just had to Google what that was. Had no idea that even existed. Pretty cool!
2531,@marktenenholtz,2022-02-08 14:08:26+00:00,https://twitter.com/marktenenholtz/status/1491051299940761601,"@JFPuget @danofer It cannot be overstated how much the GPU helps.

When I ran random forests, SVMs, and k-NNs my face was like 😱"
2532,@marktenenholtz,2022-02-08 13:00:21+00:00,https://twitter.com/marktenenholtz/status/1491034163847184389,"Link here: https://t.co/ktABkF2VxC

Follow me @marktenenholtz for more high-signal content!"
2533,@marktenenholtz,2022-02-08 13:00:20+00:00,https://twitter.com/marktenenholtz/status/1491034160844050432,"It’s a great intro to how transformers work and it teaches you everything you need to know about using their package.

I’ve used the library everywhere from Kaggle to serverless deployments. High-powered and flexible!"
2534,@marktenenholtz,2022-02-08 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1491034156800770051,"Transformers have taken over NLP and are coming for computer vision.

Hugging Face 🤗 is at the forefront of their development. So, if you want to learn about transformers, who better to learn from than the leaders themselves?

Check out the free Hugging Face Course! https://t.co/Do6A35PVrz"
2535,@marktenenholtz,2022-02-07 21:58:03+00:00,https://twitter.com/marktenenholtz/status/1490807094966210561,@tunguz DGX go brrrr
2536,@marktenenholtz,2022-02-07 21:56:28+00:00,https://twitter.com/marktenenholtz/status/1490806693332144137,@tunguz Alright now how the heck did ya make XGBoost work here
2537,@marktenenholtz,2022-02-07 15:01:32+00:00,https://twitter.com/marktenenholtz/status/1490702273097437186,"@JFPuget Yeah I’ve been having so many issues with GBM implementations on my GPUs that I haven’t benchmarked them.

Maybe I should revisit it."
2538,@marktenenholtz,2022-02-07 15:00:46+00:00,https://twitter.com/marktenenholtz/status/1490702081992413192,@JFPuget @danofer I’ll admit RF is making a comeback for me after I started using the cuML implementation
2539,@marktenenholtz,2022-02-07 14:18:52+00:00,https://twitter.com/marktenenholtz/status/1490691534974500870,"@atchimolajuwon Most organizations already collect tabular data, so why not use it!"
2540,@marktenenholtz,2022-02-07 13:50:44+00:00,https://twitter.com/marktenenholtz/status/1490684457413656577,"@GaelVaroquaux @scikit_learn I have, and it’s probably my favorite sklearn model. But I’ve never (in my experience) seen it beat LightGBM/XGBoost, maybe CatBoost"
2541,@marktenenholtz,2022-02-07 13:24:32+00:00,https://twitter.com/marktenenholtz/status/1490677863665840131,@TheZachMueller Definitely one of my favorite parts about the library
2542,@marktenenholtz,2022-02-07 13:23:50+00:00,https://twitter.com/marktenenholtz/status/1490677686993309696,@bhutanisanyam1 @weights_biases I’ll check it out! Thanks!
2543,@marktenenholtz,2022-02-07 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1490671722541957120,"I'm gonna put one of these threads out there for all the common supervised learning tasks.

Follow me @marktenenholtz so you don't miss it!"
2544,@marktenenholtz,2022-02-07 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1490671719526264832,"These are the models that I immediately jump to when I'm presented with a tabular data problem.

It's by no means comprehensive! Tell me if you think I missed something."
2545,@marktenenholtz,2022-02-07 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1490671716636393473,"5. @nvidia cuML SVMs

This one is a bit out there. However, I'm *specifically* advocating for the cuML implementation.

SVMs were always a pretty solid model type, but were SUPER slow to train.

Turns out, they're quick on the GPU, though!

Check it out: https://t.co/IRzo2E4HSL"
2546,@marktenenholtz,2022-02-07 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1490671713708752901,"4. TabNet

TabNet basically uses a sequence of attention modules to create step-wise ""reasoning"". It's a tiny bit like what GBTs do.

Here's the paper: https://t.co/WoB5LUe8Fy

And here's a good implementation: https://t.co/mnTFf7njUt"
2547,@marktenenholtz,2022-02-07 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1490671710772752384,"3. 1D-CNNs

This is one you probably haven't heard of outside of Kaggle.

The basic idea is to expand your features wide (e.g. up to 2048+) with a linear layer and then slice them into small chunks to run 1D convs.

Here's the original implementation: https://t.co/VPbpCW19xe"
2548,@marktenenholtz,2022-02-07 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1490671707769692162,"2. MLPs

Usually, MLPs are just slightly worse than the above trinity, but they are still top performers (tip: try removing batch norm).

The best part about MLPs, though? They ensemble extremely well with GBT models -- like peanut butter and jelly."
2549,@marktenenholtz,2022-02-07 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1490671704800071682,"1. LightGBM/XGBoost/Catboost

The holy trinity of tabular data.

These models are as close as you can get to set-it-and-forget-it.

Personally, LightGBM is my initial go-to since it is so fast to train and memory-efficient, but all three are amazing models."
2550,@marktenenholtz,2022-02-07 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1490671701884952576,"Modern research loves to ignore tabular data.

Despite that, tabular data is involved in the most business use-cases of all supervised methods.

Personally, I have 5 go-to tabular data models.

Your state-of-the-art tabular data toolkit 🧵"
2551,@marktenenholtz,2022-02-06 20:00:51+00:00,https://twitter.com/marktenenholtz/status/1490415208820576267,"@AtharvaIngle7 Appreciate the shoutout, but I’m not the only one!"
2552,@marktenenholtz,2022-02-06 18:46:54+00:00,https://twitter.com/marktenenholtz/status/1490396600073662468,@alfcnz @AllesistKode @comsianusman @ylecun Wise words 😊
2553,@marktenenholtz,2022-02-06 18:02:32+00:00,https://twitter.com/marktenenholtz/status/1490385435570446342,@alfcnz @comsianusman @ylecun It’s so good that it’s worth plagiarizing 😜
2554,@marktenenholtz,2022-02-06 18:01:41+00:00,https://twitter.com/marktenenholtz/status/1490385221308530695,@comsianusman @alfcnz @ylecun Lmao… get your own content
2555,@marktenenholtz,2022-02-06 17:10:00+00:00,https://twitter.com/marktenenholtz/status/1490372213194825732,@ashvanth_s1 Thank you so much Ashvanth!
2556,@marktenenholtz,2022-02-06 14:54:45+00:00,https://twitter.com/marktenenholtz/status/1490338177659375621,@ytvtsp @joshuastarmer StatQuest is great! I’ve learned so much from it
2557,@marktenenholtz,2022-02-06 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1490309319962677248,"Honestly, I'd recommend blocking off as much time as possible and reading everything you can from them. It's a great resource.

For more high-signal content and tutorials, follow me @marktenenholtz!

Link: https://t.co/85X6TsbajJ"
2558,@marktenenholtz,2022-02-06 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1490309316347248646,"When you learn new concepts in machine learning, try to understand them on an intuitive level.

This blog from Distill is what made Momentum in SGD *click* for me. 

It's an interactive demo that allows you to see the effect of different hyperparameters on the outcome! https://t.co/Mau6h83AT1"
2559,@marktenenholtz,2022-02-06 02:40:07+00:00,https://twitter.com/marktenenholtz/status/1490153300787879936,@LBacaj Just spent today making the first draft. Gotta edit it and voice it over… call it a week or two?
2560,@marktenenholtz,2022-02-06 02:38:28+00:00,https://twitter.com/marktenenholtz/status/1490152886415806464,@LBacaj 👀 https://t.co/645AtT0VUV
2561,@marktenenholtz,2022-02-05 19:34:36+00:00,https://twitter.com/marktenenholtz/status/1490046215697973253,"@GaryMarcus @rasbt @swarat Ah sure that work is fascinating.

Do you think this is the path towards more reliable/factual results from AI?"
2562,@marktenenholtz,2022-02-05 19:31:42+00:00,https://twitter.com/marktenenholtz/status/1490045486866341899,"@GaryMarcus @rasbt Are some of the recent advances that use language models to map word predictions to RL action spaces the kind of thing that you're talking about?

Or are you talking about a whole new paradigm?"
2563,@marktenenholtz,2022-02-05 19:03:07+00:00,https://twitter.com/marktenenholtz/status/1490038294922637322,"@LBacaj I’ve had phone calls with several folks who are referred to me through friends that are interested in entering data science.

It’s definitely doable, but it’s made a 100x better if they give information on their background/interests ahead of time. Saves a lot of time on the call."
2564,@marktenenholtz,2022-02-05 15:56:21+00:00,https://twitter.com/marktenenholtz/status/1489991294260817928,@alfcnz Thanks for teaching me so much!
2565,@marktenenholtz,2022-02-05 15:44:34+00:00,https://twitter.com/marktenenholtz/status/1489988326081941506,@svpino And it’s just in its infancy :)
2566,@marktenenholtz,2022-02-05 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1489946944839438339,"The best part of the course are the visualizations. 

Alfredo has put so much work into refining them and they strike the perfect balance between intuition and math.

For more high-signal course recommendations, follow me @marktenenholtz!

Link here: https://t.co/7RfOC9vok6"
2567,@marktenenholtz,2022-02-05 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1489946941068767237,"If you’re going to learn deep learning, learn it from the best.

@alfcnz and @ylecun teach arguably the most comprehensive, from the fundamentals course out there and post it for free on YouTube every semester!

CNNs, RNNs, Transformers, GCNs… you name it! https://t.co/fcbJSDkmc1"
2568,@marktenenholtz,2022-02-04 18:15:14+00:00,https://twitter.com/marktenenholtz/status/1489663854812577800,@karpathy Oh yeah baby https://t.co/Qt8VXQIHXm
2569,@marktenenholtz,2022-02-04 17:00:42+00:00,https://twitter.com/marktenenholtz/status/1489645099424309248,@rasbt As it should be :)
2570,@marktenenholtz,2022-02-04 14:56:45+00:00,https://twitter.com/marktenenholtz/status/1489613903931879432,@rasbt I need to start using that more often. The name makes me laugh every time
2571,@marktenenholtz,2022-02-04 14:34:07+00:00,https://twitter.com/marktenenholtz/status/1489608208549396482,@julien_c We'll call it: commit hash mining!
2572,@marktenenholtz,2022-02-04 14:14:12+00:00,https://twitter.com/marktenenholtz/status/1489603199833190403,"@rasbt In my master’s program, we learned train/validation/holdout and simple k-fold… and that was it"
2573,@marktenenholtz,2022-02-04 13:00:15+00:00,https://twitter.com/marktenenholtz/status/1489584586686230529,"Read it. Study it. Eat, sleep, and breathe it.

Model evaluation is a crucial skill to master, and this is a great base of knowledge for it.

Link to the paper: https://t.co/UQVllT1UKD"
2574,@marktenenholtz,2022-02-04 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1489584583242702850,"You should always work on improving your model evaluation skills.

Model evaluation is the worst taught skill in machine learning, and I believe the best way of improving is through practice.

But, this paper from @rasbt is by far my favorite single written resource: https://t.co/XZMgkTYMCz"
2575,@marktenenholtz,2022-02-03 23:43:35+00:00,https://twitter.com/marktenenholtz/status/1489384100335075329,"@LBacaj YES Louie, YES! It was so helpful when you went through a Google doc with me and this looks a lot more streamlined.

Let me know if you need testers!"
2576,@marktenenholtz,2022-02-03 22:29:09+00:00,https://twitter.com/marktenenholtz/status/1489365368292450308,"@svpino People love to shit on advances like that because of the obvious flaws, but I have a hard time understanding why they can't see just how amazing it is."
2577,@marktenenholtz,2022-02-03 20:55:04+00:00,https://twitter.com/marktenenholtz/status/1489341693254156291,"@giffmana Ah yes. My default is cosine annealing (usually without warmup) but I've always wanted to experiment with this more. 

It's amazing to me that there aren't more implementations of rsqrt in big libraries (unless I'm missing them)."
2578,@marktenenholtz,2022-02-03 20:43:40+00:00,https://twitter.com/marktenenholtz/status/1489338822433329154,@giffmana Isn't this where a scheduler that incorporates restarts comes most in handy?
2579,@marktenenholtz,2022-02-03 19:58:10+00:00,https://twitter.com/marktenenholtz/status/1489327370356109318,"@chrisaballard Thanks, Chris! Glad you found value."
2580,@marktenenholtz,2022-02-03 19:16:39+00:00,https://twitter.com/marktenenholtz/status/1489316925419229198,@therealfelix5 Glad you found value!
2581,@marktenenholtz,2022-02-03 18:56:45+00:00,https://twitter.com/marktenenholtz/status/1489311916002582530,@radio_K5 I’m working on some things behind the scenes :)
2582,@marktenenholtz,2022-02-03 18:34:50+00:00,https://twitter.com/marktenenholtz/status/1489306398630555655,@jeffbarr Thanks Jeff!
2583,@marktenenholtz,2022-02-03 16:53:59+00:00,https://twitter.com/marktenenholtz/status/1489281020910350341,@Articuano19 Maybe start with the NVIDIA whitepaper on the Ampere architecture https://t.co/Gas7r02wkS
2584,@marktenenholtz,2022-02-03 15:59:44+00:00,https://twitter.com/marktenenholtz/status/1489267366315925504,"@mervenoyann @TheZachMueller The best way I've heard that describes the difference is how you build your energy.

Extroverts build their energy by being around others and are drained by being alone.

Introverts build their energy by having alone time and spend it while around others"
2585,@marktenenholtz,2022-02-03 15:19:03+00:00,https://twitter.com/marktenenholtz/status/1489257131710709760,@_krr12 @prthgo Here’s the NVIDIA white paper on the Ampere (RTX 30xx series) architecture. Doesn’t get more detailed than this https://t.co/Gas7r0k7cq
2586,@marktenenholtz,2022-02-03 14:25:37+00:00,https://twitter.com/marktenenholtz/status/1489243682398162944,@LBacaj Thanks Louie! This one was a lot of fun to write
2587,@marktenenholtz,2022-02-03 13:58:31+00:00,https://twitter.com/marktenenholtz/status/1489236862912942082,@RealYink Thanks! Glad you found value!
2588,@marktenenholtz,2022-02-03 13:35:15+00:00,https://twitter.com/marktenenholtz/status/1489231009904316417,@prthgo Thanks Parth! There’s not a ton of good resources out there on the internals of GPUs unfortunately
2589,@marktenenholtz,2022-02-03 13:00:20+00:00,https://twitter.com/marktenenholtz/status/1489222219494408197,"Hope you learned something about how GPUs work!

If you liked this, follow me @marktenenholtz for more."
2590,@marktenenholtz,2022-02-03 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1489222216675840003,"The whole idea with this hardware design is that you can break these big graphics/deep learning tasks into a bunch of tiny operations.

For instance, a matrix multiplication (a GPUs most common operation), is incredibly parallelizable and can be broken out all these many cores."
2591,@marktenenholtz,2022-02-03 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1489222213828222979,"Tensor Cores pack some serious juice, and this is what powers some extremely exciting new applications, such as the CNNs that can upsample video game/movie graphics."
2592,@marktenenholtz,2022-02-03 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1489222211026128900,"So, while CUDA cores need a cycle for adding and multiplying individual scalars, Tensor Cores only need one cycle to add and multiply whole matrices!

Remember when I said the RTX 3090 does 35.6 TFLOPs? Well, if you give it FP16 matrices, it can do 142 TFLOPs on the Tensor Cores!"
2593,@marktenenholtz,2022-02-03 13:00:17+00:00,https://twitter.com/marktenenholtz/status/1489222208194957312,"However, the fun does not stop there!

Next to those, you can see the Tensor Cores, which is what make these extra valuable for deep learning.

You see, while CUDA cores operate on scalars, Tensor Cores actually operate on little FP16 matrices."
2594,@marktenenholtz,2022-02-03 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1489222205363798021,"Each SM has FP32 cores (32-bit precision floating point) and cores for both FP32 and INT32s.

These are what are commonly known as CUDA cores -- they're made for operating on scalars.

In one clock cycle, each of them can do 1 addition operation and 1 multiplication operation."
2595,@marktenenholtz,2022-02-03 13:00:16+00:00,https://twitter.com/marktenenholtz/status/1489222202146766851,"To make the RTX 3090 so powerful, NVIDIA basically adopted this philosophy: https://t.co/YpAdJeEx9t"
2596,@marktenenholtz,2022-02-03 13:00:14+00:00,https://twitter.com/marktenenholtz/status/1489222194395639809,"What you can see are 82 of these fellas, called Streaming Multiprocessors (abbreviated SMs)

SMs are the core element of the GPU.

Each one is like its own multi-core CPU system, having its own cache, registers, and computational elements. https://t.co/bC4aX4MvUu"
2597,@marktenenholtz,2022-02-03 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1489222184815919111,"Smack dab in the middle (with the NVIDIA logo) is the processor, and the black squares to either side are all of the memory modules (12 2GB modules, in the case of a 3090).

Let's zoom in to that RTX 3090 processor and see what's going on inside 👇 https://t.co/1E7RG4iQWm"
2598,@marktenenholtz,2022-02-03 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1489222175051563010,"So, how do they pack all that compute in there?

Let's take a look at the board that a 3090 is built on 👇 https://t.co/qh19sIBGqg"
2599,@marktenenholtz,2022-02-03 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1489222164679041026,"For comparison, a $2000 (at release) Ryzen Threadripper 3970X CPU has 64 cores that run at 3.7 GHz (6.9 trillion floating-point operations/sec (TFLOPs)).

A $1500 (if you're lucky) RTX 3090 GPU has 10496 cores that run at 1.4 GHz (35.6 TFLOPs).

That's a lot of freaking cores!"
2600,@marktenenholtz,2022-02-03 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1489222161692717059,"In this analogy, the Ferrari is the CPU and the truck is the GPU.

CPUs are made up of fewer cores, but each core is very capable. It's great at sequential tasks.

GPUs, however, have a ton of kinda dumb cores, and are great for parallel tasks."
2601,@marktenenholtz,2022-02-03 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1489222156013555713,"Your current car is a small, but speedy Ferrari (business has been kind to you). It can only carry 1 package at a time.

On days where you are delivering 1 package, it's not a problem -- your deliveries are incredibly speedy."
2602,@marktenenholtz,2022-02-03 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1489222153186598915,"Let's start with a simple analogy to ground us.

Imagine you deliver packages to a single location every day. For your business, delivery speed is incredibly important.

Some days you deliver 1 package, other days you deliver 1000."
2603,@marktenenholtz,2022-02-03 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1489222150384848900,"GPUs, 101

GPUs have powered the deep learning revolution.

However, most practitioners have very little idea how they work. 

If you want to get the most out of them, this is important knowledge to have.

As it turns out, the basics are really easy to grasp.

🧵 👇"
2604,@marktenenholtz,2022-02-02 19:15:46+00:00,https://twitter.com/marktenenholtz/status/1488954312067883010,@tunguz You're not gonna get me with another joke. I bet you're in your yard making a snowman!
2605,@marktenenholtz,2022-02-02 19:07:42+00:00,https://twitter.com/marktenenholtz/status/1488952285808009224,@sama can u do my homework pls
2606,@marktenenholtz,2022-02-02 18:58:12+00:00,https://twitter.com/marktenenholtz/status/1488949891598635010,@suryakiran10 Best of luck! It's worth learning!
2607,@marktenenholtz,2022-02-02 16:41:45+00:00,https://twitter.com/marktenenholtz/status/1488915553137971200,"@tunguz Damnit Bojan, I knew I was getting baited the moment I saw something serious come from you 😂"
2608,@marktenenholtz,2022-02-02 16:39:24+00:00,https://twitter.com/marktenenholtz/status/1488914962303102984,@tunguz The points in that thread are pretty much pulled directly from what I sucked at
2609,@marktenenholtz,2022-02-02 16:38:33+00:00,https://twitter.com/marktenenholtz/status/1488914746841735177,"@tunguz This was tough to write, but I have to say it feels good to write about it in the end because it helps me move on and learn the important lessons."
2610,@marktenenholtz,2022-02-02 14:32:39+00:00,https://twitter.com/marktenenholtz/status/1488883066475102208,@TnSabrina No problem!
2611,@marktenenholtz,2022-02-02 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1488859779225165825,"One of my favorites in here is OpenImages, since it gives you the data to tackle detection, segmentation, and classification tasks.

Follow me @marktenenholtz and I'll help you learn methods you can apply to these datasets!"
2612,@marktenenholtz,2022-02-02 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1488859775722917888,"The best way to grow your machine learning skills is to build more models.

But, to do that, you need some interesting datasets.

Google Research has released 106 datasets spanning from text to images to time series data.

Want to try a new method? Use one of these as a basis! https://t.co/Q4mkY7SPky"
2613,@marktenenholtz,2022-02-01 20:02:51+00:00,https://twitter.com/marktenenholtz/status/1488603773371097094,"@TheZachMueller That’s interesting, I use VSCode on Ubuntu every day for even heavy workloads and I’ve never had a single issue.

Sorry you went through that!"
2614,@marktenenholtz,2022-02-01 15:28:26+00:00,https://twitter.com/marktenenholtz/status/1488534716433186816,"@gusthema Zip is one of the most underrated built-ins!

My favorite way of using it is simultaneous looping over multiple lists."
2615,@marktenenholtz,2022-02-01 14:12:33+00:00,https://twitter.com/marktenenholtz/status/1488515617829306370,@tunguz @3blue1brown Shh… don’t tell them!
2616,@marktenenholtz,2022-02-01 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1488497386963959808,"If you want to master deep learning, you first need to know linear algebra.

My favorite introductory resource for that is Essence of Linear Algebra from @3blue1brown.

In my opinion, the only way to learn linear algebra is visually, and this course executes it perfectly. https://t.co/h1VzdnNDCX"
2617,@marktenenholtz,2022-02-01 03:31:01+00:00,https://twitter.com/marktenenholtz/status/1488354173443878917,@aureliengeron NFNet. Important to understand what’s being done to get around batch norm.
2618,@marktenenholtz,2022-02-01 02:34:08+00:00,https://twitter.com/marktenenholtz/status/1488339855302082560,@karpathy 3e-4 is the one I’m most curious about 😂 https://t.co/wUDhyviHlX
2619,@marktenenholtz,2022-02-01 02:21:51+00:00,https://twitter.com/marktenenholtz/status/1488336766516056068,"@karpathy That blog post was a formative resource when I started learning DL. Thanks for writing it!

Have you changed your methodology at all since you wrote it?"
2620,@marktenenholtz,2022-02-01 00:13:18+00:00,https://twitter.com/marktenenholtz/status/1488304416864280582,"@LBacaj Been working on some way-too-early plans for something like this and you’re a huge inspiration. 

Maybe we should chat about it sometime."
2621,@marktenenholtz,2022-02-01 00:06:06+00:00,https://twitter.com/marktenenholtz/status/1488302602278391815,"@LBacaj That is crazy, Louie. I’m so freaking happy for you!"
2622,@marktenenholtz,2022-01-31 23:16:15+00:00,https://twitter.com/marktenenholtz/status/1488290055768662016,@gho__sted Same as my Twitter handle
2623,@marktenenholtz,2022-01-31 22:19:09+00:00,https://twitter.com/marktenenholtz/status/1488275688427646977,@seodtitruh The only time I use synthetic data creation is testing database deployments. I've never had success with tricks like SMOTE.
2624,@marktenenholtz,2022-01-31 22:18:31+00:00,https://twitter.com/marktenenholtz/status/1488275528276914176,@JorgeTorresAI @camilooob @MindsDB Sorry! I just opened it up. Try now.
2625,@marktenenholtz,2022-01-31 21:45:39+00:00,https://twitter.com/marktenenholtz/status/1488267256832290816,@JorgeTorresAI @camilooob @MindsDB DM me
2626,@marktenenholtz,2022-01-31 18:50:47+00:00,https://twitter.com/marktenenholtz/status/1488223250232619020,@DieterCastel Always.
2627,@marktenenholtz,2022-01-31 16:38:25+00:00,https://twitter.com/marktenenholtz/status/1488189938386542599,"@gusthema You’re absolutely right — these aren’t just kaggling tips, they’re tips for building any model."
2628,@marktenenholtz,2022-01-31 15:35:24+00:00,https://twitter.com/marktenenholtz/status/1488174082235551758,@DieterCastel I do this at my day job and it’s incredibly successful. The key is model validation.
2629,@marktenenholtz,2022-01-31 14:25:08+00:00,https://twitter.com/marktenenholtz/status/1488156396596174854,@thedataprof @kaggle Thanks for the shoutout!
2630,@marktenenholtz,2022-01-31 13:42:34+00:00,https://twitter.com/marktenenholtz/status/1488145684863852552,"@tunguz I write bash scripts to do it for most competitions since I use ColabCode from @abhi1thakur.

I just use the “mount_drive=True” param, clone my GitHub repo, and extract the data from a zip file in my drive."
2631,@marktenenholtz,2022-01-31 13:27:50+00:00,https://twitter.com/marktenenholtz/status/1488141976113459203,"@JFPuget @J_P_Raymond True, but I ran into issues multiple times in university where measurements failed due to using a new sensor. Even these factors cause a time element to become a part of the equation."
2632,@marktenenholtz,2022-01-31 13:18:16+00:00,https://twitter.com/marktenenholtz/status/1488139568754962432,"@vivhernandez I should write a whole thread on it at some point.

But on your line of thought, you should always build 100% reproducible pipelines (wherever possible) and document exactly how you ran them."
2633,@marktenenholtz,2022-01-31 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1488135002315386886,"The only thing to do from here?

Go do it. A lot.

Nothing can replace practical experience. There are tons of nuances that you will only pick up from trying over and over.

Follow me @marktenenholtz and I'll go over as many of the nuances that I've picked up in the future!"
2634,@marktenenholtz,2022-01-31 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1488134999534571522,"6. Find what scales

Finally, it's time to scale up.

This may mean including more data or grabbing the ""large"" variant of whatever model you're training.

You also can toy around with ensembling at this stage."
2635,@marktenenholtz,2022-01-31 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1488134996699201536,"5. Tweak your model and iterate

Error analysis should surface some new improvements, so go try them!

At this stage, repeat steps 3-4 until you run out of things to try."
2636,@marktenenholtz,2022-01-31 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1488134993733840898,"4. Analyze your predictions

Now that you've done a bit of tweaking to beat your baseline, it's time to see what your model is missing.

Save out your validation predictions and start slicing your data every way you can think to see which subsets your model struggles with."
2637,@marktenenholtz,2022-01-31 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1488134990873333760,"3. Start small

My rule of thumb is that I start with a model that:

• Can give quick feedback (&lt;10 minutes)
• Can beat my baseline

The most important thing at this stage with rapid iteration.

The best models are created by the data scientists that tried the most things."
2638,@marktenenholtz,2022-01-31 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1488134988042170368,"2. Get a baseline

This is worthy of it's own thread, but start by guessing the mean, yesterday's value for time series, etc.

I like to start by knowing what accuracy I can get with a totally naive prediction.

Also, if you can estimate irreducible error at this stage, do it."
2639,@marktenenholtz,2022-01-31 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1488134984938377218,"1. Implement/test your evaluation metric

Don't fly blind!

Make sure you understand what ""good"" looks like.

It's also a good time to probe it and understand the flaws -- for instance, AUC can be misleading on imbalanced datasets.

Complicated metrics will require more testing."
2640,@marktenenholtz,2022-01-31 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1488134981985583105,"I spent 500+ hours on Kaggle competitions last year and just became a Kaggle Master.

Over those many hours, I learned a systematic process you can use to train any model on any dataset.

6 steps to train any model 🧵"
2641,@marktenenholtz,2022-01-30 21:44:13+00:00,https://twitter.com/marktenenholtz/status/1487904509951152132,@delai50 @bhutanisanyam1 I would be happy to. I’ll do it when I’m back in town
2642,@marktenenholtz,2022-01-30 19:13:08+00:00,https://twitter.com/marktenenholtz/status/1487866488623095818,@sickcodes @mihaimaruseac https://t.co/s884qEedo9
2643,@marktenenholtz,2022-01-30 18:55:43+00:00,https://twitter.com/marktenenholtz/status/1487862105667735556,@kamalbowselvam It’s hard to say what specific problem you’re running into. It could be more of an issue with how you’re validating your newly trained models.
2644,@marktenenholtz,2022-01-30 18:49:17+00:00,https://twitter.com/marktenenholtz/status/1487860486045634565,@kamalbowselvam Yes! Look up statistical process control.
2645,@marktenenholtz,2022-01-30 18:18:32+00:00,https://twitter.com/marktenenholtz/status/1487852747609165824,"Writing maintainable code is a superpower.

You should always err on the side of writing too much documentation.

Your code is only as valuable as others can use it, and it’s only as valuable as it is easy for you to return to."
2646,@marktenenholtz,2022-01-30 15:13:22+00:00,https://twitter.com/marktenenholtz/status/1487806148426227712,"@katjawittfoth It’s not that it’s a certainty that it will change over time, it’s that there’s always a big risk.

You never know when ppl will start using your fish recognition app for something different. You also don’t know the types of cameras people will be using (BIG performance impact)."
2647,@marktenenholtz,2022-01-30 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1487772601653116928,"All data is time series data.

Your data will always vary with time, so there's no such thing as a ""set-and-forget"" production model.

If you don't regularly check on your production models, you're putting yourself at huge risk of suffering performance hits from data drift."
2648,@marktenenholtz,2022-01-30 02:30:29+00:00,https://twitter.com/marktenenholtz/status/1487614160515092484,"@LBacaj @DThompsonDev Better to start everyone out the same way and be willing to quickly make adjustments.

No better way to prove that hard work pays.

If companies are paying asymmetrical entry salaries because of incoming credentials, what does that say about their talent development abilities?"
2649,@marktenenholtz,2022-01-30 02:25:26+00:00,https://twitter.com/marktenenholtz/status/1487612890261729290,"@LBacaj @DThompsonDev Employers don’t realize that employees are becoming more willing to talk about their salaries.

They’re messing with a ticking time bomb."
2650,@marktenenholtz,2022-01-29 18:19:28+00:00,https://twitter.com/marktenenholtz/status/1487490594951860227,@notwhatitwillbe Yup. My point is more that it’s good to keep a note of it just in case.
2651,@marktenenholtz,2022-01-29 18:00:03+00:00,https://twitter.com/marktenenholtz/status/1487485707606056963,"Rule of thumb for productive programming:

Don’t over-optimize, and don’t optimize too quickly.

When writing code that you realize could be optimized, think about it for 5 minutes and no longer.

Keep a refactoring log and come back to it later/after a night of sleep"
2652,@marktenenholtz,2022-01-29 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1487410234339381248,"Machine learning superpower:

An extreme willingness to try new things.

Once you’ve tried a new method, you will forever understand how to use it in the future. 

And, the best models are built by practitioners that can try new things quickly.

So go try something new."
2653,@marktenenholtz,2022-01-28 23:17:49+00:00,https://twitter.com/marktenenholtz/status/1487203289217343493,@jeffistyping It’s more like this https://t.co/zOWpEWpmgX
2654,@marktenenholtz,2022-01-28 22:16:34+00:00,https://twitter.com/marktenenholtz/status/1487187873472389120,@kexp12117 @huggingface Try this
2655,@marktenenholtz,2022-01-28 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1487047834130006016,"Huggingface is by far my favorite NLP library.

I've used it for both training models and productionalized transformers.

It's a library that's packed full of features, and this book does a wonderful job of exposing you to the entirety of its functionality."
2656,@marktenenholtz,2022-01-28 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1487047830439038984,"You should build deep knowledge of important deep learning tools and state-of-the-art methods.

Do both at the same time by reading Natural Language Processing with Transformers!

It's written by folks behind the incredibly @huggingface library and will 10x your NLP effectiveness https://t.co/FNzGNh3XIr"
2657,@marktenenholtz,2022-01-27 18:12:02+00:00,https://twitter.com/marktenenholtz/status/1486763945654431754,@LBacaj Could this be your push over the 10k mark? 👀
2658,@marktenenholtz,2022-01-27 17:16:37+00:00,https://twitter.com/marktenenholtz/status/1486750001539829760,@PranayT17837428 Check these examples out: https://t.co/4p1zPpHiwV
2659,@marktenenholtz,2022-01-27 14:07:16+00:00,https://twitter.com/marktenenholtz/status/1486702348395335685,@edkesuma I always regret not setting up logging immediately when starting a project
2660,@marktenenholtz,2022-01-27 13:00:11+00:00,https://twitter.com/marktenenholtz/status/1486685468204830729,"I’ve talked a lot about model training and validation lately, but I’m looking to write more about MLOps in the near future.

If that sounds interesting, give me a follow @marktenenholtz so you don’t miss it!"
2661,@marktenenholtz,2022-01-27 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1486685465344233473,"There are a bunch of great logging libraries out there.

I mentioned that I tend to use Weights &amp; Biases, but other solutions like https://t.co/DGwjvT0S6i, TensorBoard, and MLFlow are also great options."
2662,@marktenenholtz,2022-01-27 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1486685462378860548,"5. Fault-tolerant

If your logs are streaming over a network, you need to make sure that a brief network interruption won’t kill your training process.

To prepare for this, I usually keep a temporary local copy of my logs."
2663,@marktenenholtz,2022-01-27 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1486685456104243202,"3. Easy to compare runs

Here’s a screenshot from the same dashboard, but this time with two separate runs overlayed.

This way, I can compare two separate runs entirely visually and in one glance.

It’s hard to do this with simple numerical, printed logs! https://t.co/Y8EMwMVcsG"
2664,@marktenenholtz,2022-01-27 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1486685449162612737,"2. Easy to search and filter

Come up with a naming convention for your runs so you can quickly filter comparable runs.

Typically, I break runs down to model type and some name for the modeling strategy so I can easily slice and dice by both factors.

(Side note: learn regex!)"
2665,@marktenenholtz,2022-01-27 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1486685445484257286,"1. Good visuals

I prefer a setup that allows me to see all of my key, top-line metrics in one or two glances.

Here’s a screenshot from one of my @weights_biases dashboards. There’s a ton there, but I don’t have to scroll to any new pages to see any of the key metrics. https://t.co/UAWVWvNRWV"
2666,@marktenenholtz,2022-01-27 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1486685434725875715,"A crucial piece of your machine learning pipelines is automatic experiment logging.

Whether you implement it yourself or use a library, organized logs are the best way to visualize what works and what doesn't.

Here are the qualities I look for 🧵"
2667,@marktenenholtz,2022-01-26 22:11:49+00:00,https://twitter.com/marktenenholtz/status/1486461902964371469,"@aaditsh Incredibly important in tech.

The younger generation always comes in with the latest and greatest advances in your field.

If you can’t learn from them, you become a dinosaur.

Quickly."
2668,@marktenenholtz,2022-01-26 18:17:44+00:00,https://twitter.com/marktenenholtz/status/1486402994824527875,"@_Parag_Das_ Thanks, Parag!"
2669,@marktenenholtz,2022-01-26 17:56:28+00:00,https://twitter.com/marktenenholtz/status/1486397640266952705,"@jim_dowling @bhutanisanyam1 You can, yes, but it shouldn’t replace evaluation that you do on future holdout datasets.

It’s rare that a validation set perfectly matches to the real world, so it should always be revisited."
2670,@marktenenholtz,2022-01-26 16:58:49+00:00,https://twitter.com/marktenenholtz/status/1486383136112058372,@edkesuma Why’d you abandon Ubiquant?
2671,@marktenenholtz,2022-01-26 16:24:13+00:00,https://twitter.com/marktenenholtz/status/1486374425188569090,@jim_dowling @bhutanisanyam1 I would generally say yes
2672,@marktenenholtz,2022-01-26 15:13:01+00:00,https://twitter.com/marktenenholtz/status/1486356506945667072,"@mcros22 @bhutanisanyam1 The more, the better"
2673,@marktenenholtz,2022-01-26 15:02:40+00:00,https://twitter.com/marktenenholtz/status/1486353905822875649,@J_P_Raymond And that one too :)
2674,@marktenenholtz,2022-01-26 15:01:53+00:00,https://twitter.com/marktenenholtz/status/1486353704810946564,"@J_P_Raymond That’s a great point. Like I said here, this is for learning from scratch.

I’ve discussed this before in prior threads, actually:"
2675,@marktenenholtz,2022-01-26 14:05:08+00:00,https://twitter.com/marktenenholtz/status/1486339425030262785,"@prthgo That's great, Parth! Reading other people's code is essential to learning.

Eventually, though, you'll find it more valuable to try it yourself without reading others' code. Then, after you've done that, compare your work with others."
2676,@marktenenholtz,2022-01-26 13:56:47+00:00,https://twitter.com/marktenenholtz/status/1486337324912988165,"@dav_ell Adversarial validation is an incredibly valuable skill, but cross-validation is much more widely applicable, especially when you have small datasets."
2677,@marktenenholtz,2022-01-26 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1486323067458523137,"This is a pretty general outline, but I plan on diving into the specifics on evaluation metrics and CV schemes in the future.

I also discussed them on a podcast with @bhutanisanyam1 here: https://t.co/AiGAe1zBH3

Follow me @marktenenholtz so that you don’t miss it!"
2678,@marktenenholtz,2022-01-26 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1486323064648331269,"5. Go and do it. A lot.

You will only improve at validation if you apply it to a ton of datasets.

If you stop after step 2, your skills will not be good enough. Full stop.

Never rest on your laurels. There is always something new to learn, and some new trick you can use."
2679,@marktenenholtz,2022-01-26 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1486323061812981762,"4. Build simple models and try different CV schemes

Get a dataset and create a random test set. 

Then, build some simple models and switch validation strategies in and out and see how well your models generalize for each scheme.

This will cement the importance of validation."
2680,@marktenenholtz,2022-01-26 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1486323058981834755,"3. Read old Kaggle competition solutions

Every day, or multiple times a week, pick an old Kaggle competition.

Read every solution that is posted and skip to their validation schemes.

There are nuances to every dataset, and this is the best way to see how pros navigate them."
2681,@marktenenholtz,2022-01-26 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1486323056125480962,"2. Learn the common forms of cross validation

Before diving in too deep, make sure you understand the basics.

You can’t become an expert in validation in the classroom, but knowing what is out there (simple k-fold, stratified, grouped, roll forward, etc.) is crucial."
2682,@marktenenholtz,2022-01-26 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1486323053336293380,"1. Learn the essential evaluation metrics

Think accuracy should be your primary metric? You’re sorely mistaken.

Most of the best metrics instead focus on how far your were from the correct answer. Think RMSE and MAE.

Others point to how well calibrated your model is, like F1."
2683,@marktenenholtz,2022-01-26 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1486323050219855874,"The worst taught skill in machine learning is model validation.

If you can’t validate your models well, you have no idea if they will actually work.

Here are 3 steps I’d take if I was relearning model validation from scratch 🧵"
2684,@marktenenholtz,2022-01-25 19:07:34+00:00,https://twitter.com/marktenenholtz/status/1486053146132680711,"@svpino A useful way of thinking of the difference between the two:

RMSE is best when you want to optimize for the mean and create a model sensitive to big jumps.

MAE is best when you want to optimize for the median and create a more conservative model. https://t.co/F4bgwcWqon"
2685,@marktenenholtz,2022-01-25 16:42:04+00:00,https://twitter.com/marktenenholtz/status/1486016529795887107,@MLinProduction You and a few others inspired me :)
2686,@marktenenholtz,2022-01-25 13:08:41+00:00,https://twitter.com/marktenenholtz/status/1485962829521993735,@prthgo It takes a lot of practice to truly understand
2687,@marktenenholtz,2022-01-25 13:00:18+00:00,https://twitter.com/marktenenholtz/status/1485960721439461378,"If you create machine learning models, your time is

1. Incredibly valuable
2. Incredibly scalable

The best way to maximize your value?

Build the simplest models that do the job, but no simpler."
2688,@marktenenholtz,2022-01-24 23:27:41+00:00,https://twitter.com/marktenenholtz/status/1485756219482677248,@LBacaj Now there’s an idea 👀
2689,@marktenenholtz,2022-01-24 23:00:35+00:00,https://twitter.com/marktenenholtz/status/1485749401679286275,"@LBacaj My first time was today!

I’m glad I have a lot of public speaking practice or I wouldn’t have been able to execute when it mattered."
2690,@marktenenholtz,2022-01-24 17:07:20+00:00,https://twitter.com/marktenenholtz/status/1485660501879267329,"@seanjtaylor Writing on Twitter has helped me realize the challenging trade offs between adding nuance and getting your point across in a way that resonates.

It’s so difficult to do it well that it’s indistinguishable from art when you see it done well."
2691,@marktenenholtz,2022-01-24 13:01:20+00:00,https://twitter.com/marktenenholtz/status/1485598594061070337,"If you liked this, I’ll be on the Chai Time Kaggle Talks podcast with @bhutanisanyam1 at 1 pm eastern today.

We’ll be discussing models that I built for the latest Petfinder Kaggle competition in which I achieve Kaggle Master status

Join in and learn more about building models!"
2692,@marktenenholtz,2022-01-24 13:01:19+00:00,https://twitter.com/marktenenholtz/status/1485598591208943617,"Congrats, you have a production model!

Of course, this thread skips the specific model validation and training processes, which are vital.

I’ve tweeted a lot about how to improve those and plan to talk about it a lot more."
2693,@marktenenholtz,2022-01-24 13:01:19+00:00,https://twitter.com/marktenenholtz/status/1485598588365258753,"5. Periodic check-ins

Eventually, you will reach a point where the model performs consistently enough that it doesn’t need constant monitoring.

At this stage (and for the rest of the model’s life), set up regular, but less frequent check ups to evaluate its performance."
2694,@marktenenholtz,2022-01-24 13:01:18+00:00,https://twitter.com/marktenenholtz/status/1485598585609539586,"4. Monitor it closely

The worst thing you can do is throw a model into production and forget about it.

No amount of model validation can 100% prepare you for the real world.

Use human-in-the-loop tactics wherever you can."
2695,@marktenenholtz,2022-01-24 13:01:17+00:00,https://twitter.com/marktenenholtz/status/1485598582845571072,"3. Do the relevant research

You don’t need to reinvent the wheel.

Wherever possible, use the advances researchers/Kagglers have made, pretrained models, etc.

Spend as much time as possible focusing on building a model that performs well and less on creating something fancy."
2696,@marktenenholtz,2022-01-24 13:01:17+00:00,https://twitter.com/marktenenholtz/status/1485598580106620929,"2. Identify where ML helps

Don’t ask ML models to do something they aren’t good at.

Just like an employee should be put in a role they will succeed in, an ML model should be given a task that it can perform well.

Ideally, frame it as a common task, like image classification."
2697,@marktenenholtz,2022-01-24 13:01:15+00:00,https://twitter.com/marktenenholtz/status/1485598574490439683,"Over the last year, I saw 4 models through from R&amp;D to production.

There’s tons of information out there about building models, but I wish I’d known more about structuring projects.

Here are 5 steps you should take to plan and productionalize the perfect model for your problem:"
2698,@marktenenholtz,2022-01-23 19:01:02+00:00,https://twitter.com/marktenenholtz/status/1485326728230809603,"@svpino If the first thing you throw at the problem is a fully-fledged model, you didn’t start by benchmarking enough."
2699,@marktenenholtz,2022-01-23 13:36:17+00:00,https://twitter.com/marktenenholtz/status/1485244999918927875,@bhutanisanyam1 @ThomasViehmann @weights_biases Well I know what I’m binging next!
2700,@marktenenholtz,2022-01-22 18:08:54+00:00,https://twitter.com/marktenenholtz/status/1484951220343169028,@tunguz And just when you get comfortable with that... https://t.co/rCzjpQHnSg
2701,@marktenenholtz,2022-01-22 17:58:07+00:00,https://twitter.com/marktenenholtz/status/1484948507744219141,"@MLinProduction I see what you’re saying.

Yeah, the simplest solution is always the best one."
2702,@marktenenholtz,2022-01-22 17:52:35+00:00,https://twitter.com/marktenenholtz/status/1484947115247906823,"@MLinProduction That would be ideal, but most don’t work at a company where stakeholders know enough about machine learning to seek out a model to solve their problems."
2703,@marktenenholtz,2022-01-22 17:14:05+00:00,https://twitter.com/marktenenholtz/status/1484937425952649217,@LBacaj You only think you suck. I bet you have a higher hit rate than the market at large.
2704,@marktenenholtz,2022-01-22 15:14:21+00:00,https://twitter.com/marktenenholtz/status/1484907291409358854,@ckmeher_ @bhutanisanyam1 Thanks Chinmaya!
2705,@marktenenholtz,2022-01-22 14:14:21+00:00,https://twitter.com/marktenenholtz/status/1484892192028168200,@prthgo Thanks Parth!
2706,@marktenenholtz,2022-01-22 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1484873500833710080,"1. My journey to ML
2. My journey to Kaggle Master
3. The Petfinder computer vision competition

We’ll also take questions from the audience, so feel free to AMA!"
2707,@marktenenholtz,2022-01-22 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1484873497738309632,"After 500+ hours of competing, I finally became a Kaggle Competitions Master.

I’ll discuss it on the Chai Time Kaggle Talks podcast with @bhutanisanyam1! 

Check it out at 1 pm EST on Monday the 24th! 

Livestream here: https://t.co/MwzyfCUBor

On the podcast, we’ll discuss:"
2708,@marktenenholtz,2022-01-21 13:17:49+00:00,https://twitter.com/marktenenholtz/status/1484515580098924544,@primaprashant https://t.co/IVsTO4BlPt
2709,@marktenenholtz,2022-01-21 13:08:04+00:00,https://twitter.com/marktenenholtz/status/1484513125177118720,@bhutanisanyam1 Depends on how much time I have this weekend!
2710,@marktenenholtz,2022-01-21 13:06:56+00:00,https://twitter.com/marktenenholtz/status/1484512838408458242,"@bhutanisanyam1 I’ve been thinking about a newsletter for a while now.

More on that soon 😉"
2711,@marktenenholtz,2022-01-21 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1484511134736547842,"I hope I've helped you avoid some of my past failures.

If you liked this, I plan on writing a lot more about model validation and benchmarking, which are two key skills to help you sell the effectiveness of your models.

Follow me and let's drive more ML adoption together."
2712,@marktenenholtz,2022-01-21 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1484511133629239298,"4. Understand what ML will improve

Sometimes, improved accuracy is not the main goal.

An ML model that is much faster and scalable than a human can often provide a ton of value even if it's a bit less accurate."
2713,@marktenenholtz,2022-01-21 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1484511132530339842,"3. Don't build without a specific goal

Before you build, sit down with your stakeholder. Figure out how accurate is accurate enough.

A good way to start: if the task is handled by a human, evaluate how accurate the human is.

If you can beat that, you build a convincing case."
2714,@marktenenholtz,2022-01-21 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1484511131452383232,"2. Convert your loss function into 💰 

Money is the universal language. Everyone in a business understands what money means.

Don't tell them that your model is X% more accurate or faster, tell them how much money it saves/generates."
2715,@marktenenholtz,2022-01-21 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1484511130328309765,"1. Sell it to the decision makers

You're probably not trying convincing your direct stakeholder.

You're probably trying convincing their boss.

Your pitch should be one that best helps your direct stakeholder sell it to them."
2716,@marktenenholtz,2022-01-21 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1484511129162301446,"Your models are only as useful as the buy-in they generate from your stakeholders.

I've wasted, conservatively, 250+ hours building models that weren't used because I didn't build enough buy-in.

Here are 4 lessons I learned along the way 🧵"
2717,@marktenenholtz,2022-01-21 12:06:07+00:00,https://twitter.com/marktenenholtz/status/1484497535179935746,"@ph_singer @wightmanr This is what I was worried about when BeiT and effnetv2 were left out of the paper.

I can sorta understand why BeiT was left out, but effnetv2 raised an alarm bell or two."
2718,@marktenenholtz,2022-01-20 22:04:56+00:00,https://twitter.com/marktenenholtz/status/1484285844299132928,"@EdemGold1 I would say the exact same advice as in this thread applies, it just also extends to your choice of databases, servers, etc.

Maybe I’ll write another thread on that."
2719,@marktenenholtz,2022-01-20 21:37:21+00:00,https://twitter.com/marktenenholtz/status/1484278903309012996,"@_ScottCondron I’ve seen a couple of nice templates with Lightning and Hydra, and I love that this one includes @weights_biases (which I use in all of my projects now)"
2720,@marktenenholtz,2022-01-20 17:37:48+00:00,https://twitter.com/marktenenholtz/status/1484218619164860419,@bdsaglam I’ll be going over one on the @weights_biases Chai Time Kaggle Talks with @bhutanisanyam1 on Monday!
2721,@marktenenholtz,2022-01-20 15:47:05+00:00,https://twitter.com/marktenenholtz/status/1484190755786838017,@TheZachMueller @bhutanisanyam1 @weights_biases @sschoenholz @cgarciae88 @waydegilliam @gretel_ai @ChrisDeotte @ucucsds He’s a train that cannot be stopped
2722,@marktenenholtz,2022-01-20 15:40:26+00:00,https://twitter.com/marktenenholtz/status/1484189082758569986,@bhutanisanyam1 @TheZachMueller @weights_biases @sschoenholz @cgarciae88 @waydegilliam @gretel_ai @ChrisDeotte @ucucsds How about I come on and we host a live meditation session to make sure your hair doesn’t go gray too young!
2723,@marktenenholtz,2022-01-20 14:55:54+00:00,https://twitter.com/marktenenholtz/status/1484177875712548864,@bhutanisanyam1 @weights_biases @sschoenholz @cgarciae88 @waydegilliam @gretel_ai @ChrisDeotte @ucucsds Couldn’t be more excited to talk to you on Monday!
2724,@marktenenholtz,2022-01-20 14:47:34+00:00,https://twitter.com/marktenenholtz/status/1484175778719969283,"@cgarciae88 Hey I did say it’s a process!

Also I’m stealing that meme"
2725,@marktenenholtz,2022-01-20 13:50:42+00:00,https://twitter.com/marktenenholtz/status/1484161466349633537,"@themodev I generally recommend the FastAI course because it introduces the math as you need it.

Here's the version of it that I took, but there is a more up-to-date one: https://t.co/Gu3ZyQNvfC"
2726,@marktenenholtz,2022-01-20 13:05:30+00:00,https://twitter.com/marktenenholtz/status/1484150092823158784,"@svpino I teach both Python and GitHub trainings where I work.

I do it because we’d rather hire people with good analytical brains and teach them the tools rather than the other way around."
2727,@marktenenholtz,2022-01-20 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1484148727669399553,"If you liked this framework-based approach to designing your personal model training pattern, follow me for more @marktenenholtz"
2728,@marktenenholtz,2022-01-20 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1484148726591471618,"3. Practice

Attacking projects is a learning process like anything else. Repetition helps you perfect your plan.

Your ideal structure will naturally emerge as you work and slowly mold it to your natural process. 

Start with something, work on it, improve it, and repeat."
2729,@marktenenholtz,2022-01-20 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1484148725396037639,"2. Standardize a project structure

Most data scientists work in scripts (traditional SWE structure), notebooks, or a combo. 

Personally, I use notebooks for EDA/testing and scripts for everything else.

You should pick the structure that makes you feel at home in your codebase."
2730,@marktenenholtz,2022-01-20 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1484148724330733575,"1. Pick a framework

Scikit-learn, PyTorch, TensorFlow, PyTorch Lightning, Keras...

Nobody cares (read: nobody SHOULD care) what you pick -- just pick one and get comfortable with it.

The only rules:
• It should make sense to you
• Don't be afraid to learn a new one later"
2731,@marktenenholtz,2022-01-20 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1484148723227627520,"The best way you can learn to produce machine learning models faster:

Find a tech stack and codebase structure that works for you.

Let's break it down 🧵"
2732,@marktenenholtz,2022-01-19 22:57:10+00:00,https://twitter.com/marktenenholtz/status/1483936599201222657,@TheZachMueller I’ve been saying that for two years 😬
2733,@marktenenholtz,2022-01-19 19:56:29+00:00,https://twitter.com/marktenenholtz/status/1483891130219569154,@catalinmpit Every programmer experiences true freedom when they realize that even the experts are still finding new things after years of experience
2734,@marktenenholtz,2022-01-19 16:14:12+00:00,https://twitter.com/marktenenholtz/status/1483835190203990016,@JFPuget Couldn’t agree more. It’s all about which one feels best to you.
2735,@marktenenholtz,2022-01-19 15:39:27+00:00,https://twitter.com/marktenenholtz/status/1483826446036611073,@LBacaj And because we all compare our raw thoughts to their heavily edited tweets
2736,@marktenenholtz,2022-01-19 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1483786337035583490,"The best machine learning framework is the one that you work most effectively with.

Frameworks have different appeal to different industries, application types, and thinking styles.

Find the one that works the best for you, and don’t be afraid to learn another in the future."
2737,@marktenenholtz,2022-01-19 03:49:56+00:00,https://twitter.com/marktenenholtz/status/1483647888974569472,"@LBacaj @kaggle Thanks Louie!

At the very least, I stick to what I’m good at 😅"
2738,@marktenenholtz,2022-01-18 19:02:04+00:00,https://twitter.com/marktenenholtz/status/1483515049578795014,"@svpino I’ve always believe this theorem tells us that there are much better ways of training than gradient descent.

After all, we end up needing many more than 1 hidden layer for pretty much every problem."
2739,@marktenenholtz,2022-01-18 18:14:08+00:00,https://twitter.com/marktenenholtz/status/1483502986332622859,"@IttaiSvidler This is a classic trade off in decentralization.

We’re seeing an analogous issue show up with Substack as well. 

Instead of paying a journal one fee for all of its creators, you now have to pay all of those creators individually, racking up a bigger bill for the consumer."
2740,@marktenenholtz,2022-01-18 17:00:02+00:00,https://twitter.com/marktenenholtz/status/1483484338964639749,@kaggle And now it's officially official! Top 500/0.3% in the world! https://t.co/1LzS5qmFlL
2741,@marktenenholtz,2022-01-18 14:05:44+00:00,https://twitter.com/marktenenholtz/status/1483440472500617217,"@GeeStacey317 Absolutely. If you try it, let me know how it works for you!"
2742,@marktenenholtz,2022-01-18 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1483423953540448258,"One of the best parts about working in machine learning:

The massive variety in backgrounds.

I got my degree in mechanical engineering. I've worked with psych Ph.D.'s, sociologists, economists, supply chain experts, and more.

The diversity of background is incredibly powerful."
2743,@marktenenholtz,2022-01-17 23:53:52+00:00,https://twitter.com/marktenenholtz/status/1483226095096930307,@bhutanisanyam1 @kaggle Thanks Sanyam!
2744,@marktenenholtz,2022-01-17 22:00:28+00:00,https://twitter.com/marktenenholtz/status/1483197554758230019,@aBHInonymous I took a stab at answering that here:
2745,@marktenenholtz,2022-01-17 20:09:35+00:00,https://twitter.com/marktenenholtz/status/1483169651622400005,@hopegray_it No problem! Let me know how it works for you!
2746,@marktenenholtz,2022-01-17 19:41:24+00:00,https://twitter.com/marktenenholtz/status/1483162557368131586,"@anonymonemous You can usually find a version of the story by reading the introduction/related works section of research papers.

They're usually not written that well and they usually focus on promoting their peers' work, but if you keep glancing at them you'll see a bigger picture."
2747,@marktenenholtz,2022-01-17 19:01:15+00:00,https://twitter.com/marktenenholtz/status/1483152453965070341,"@svpino The better use of your time is to improve your data preprocessing.

Almost always drives better performance gains that tweaking your hyper parameters"
2748,@marktenenholtz,2022-01-17 18:28:02+00:00,https://twitter.com/marktenenholtz/status/1483144094838886406,@SaiNikhileshRdy Let me know how it goes!
2749,@marktenenholtz,2022-01-17 18:18:41+00:00,https://twitter.com/marktenenholtz/status/1483141740940963851,@sukrutbilaskar Let me know how it goes!
2750,@marktenenholtz,2022-01-17 17:47:44+00:00,https://twitter.com/marktenenholtz/status/1483133955318685699,@AnjumSayed @kaggle A big thanks to the mighty datasaurus!
2751,@marktenenholtz,2022-01-17 17:34:40+00:00,https://twitter.com/marktenenholtz/status/1483130665088520195,@gusthema Here’s some work that has been done in the space: https://t.co/av50R3cu91
2752,@marktenenholtz,2022-01-17 17:32:01+00:00,https://twitter.com/marktenenholtz/status/1483129996931739650,"@tunguz @kaggle The more I try to do it myself, the more I can relate 😬"
2753,@marktenenholtz,2022-01-17 17:24:09+00:00,https://twitter.com/marktenenholtz/status/1483128019040579584,@tunguz @kaggle You bring a ton of positivity into a space that can get a bit cutthroat and nasty at times. Can't thank you enough.
2754,@marktenenholtz,2022-01-17 17:12:04+00:00,https://twitter.com/marktenenholtz/status/1483124979625598979,@gusthema Yep! It would probably work most of the time but it’s really hard to know when it would fail and how badly.
2755,@marktenenholtz,2022-01-17 17:01:33+00:00,https://twitter.com/marktenenholtz/status/1483122331044229133,"@tunguz @kaggle Thanks, Bojan! You've been a big inspiration for me."
2756,@marktenenholtz,2022-01-17 17:00:07+00:00,https://twitter.com/marktenenholtz/status/1483121969562193921,"Incredibly excited to finish the latest Petfinder @Kaggle competition with a silver medal. I only was able to spend about ~2-3 weeks working on it.

With this result, I'll officially be a Kaggle Master.

Next stop: GM https://t.co/teGzM78mjy"
2757,@marktenenholtz,2022-01-17 16:59:00+00:00,https://twitter.com/marktenenholtz/status/1483121689504460801,"@gusthema Here's how GPT-3 uses prompt engineering: (https://t.co/zR21pyFVb7).

Basic idea is that you give it a template to understand what you're looking for. You may say:

""The defendant in this case is: "" and let the model complete that idea."
2758,@marktenenholtz,2022-01-17 16:55:44+00:00,https://twitter.com/marktenenholtz/status/1483120867202711553,"@gusthema This is something that can be done through prompt engineering to get the specific answers that you want.

That being said, I'm not sure there's anything out there that I would trust to do it at scale (yet)."
2759,@marktenenholtz,2022-01-17 15:08:24+00:00,https://twitter.com/marktenenholtz/status/1483093857130713089,@prthgo Wonderful! Let me know how it works for you.
2760,@marktenenholtz,2022-01-17 14:50:27+00:00,https://twitter.com/marktenenholtz/status/1483089339634622465,@LaurentSellin @ylecun Yes! That’s exactly it.
2761,@marktenenholtz,2022-01-17 13:36:26+00:00,https://twitter.com/marktenenholtz/status/1483070710683742213,@skilp4d It works so well for me too!
2762,@marktenenholtz,2022-01-17 13:10:13+00:00,https://twitter.com/marktenenholtz/status/1483064112456290306,@svpino Let me know how it goes!
2763,@marktenenholtz,2022-01-17 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1483061569491591173,"Now that you have the mental framework, go learn something!

Follow me @marktenenholtz for more helpful frameworks, ways to enter the field, and discussions about machine learning."
2764,@marktenenholtz,2022-01-17 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1483061568380174341,"By learning the field as a story, you'll learn all of the valuable context that makes it easier to remember the individual pieces.

Now you don't have to memorize a method in isolation."
2765,@marktenenholtz,2022-01-17 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1483061567293779968,"Why is it unique?

In other words, what did this method introduce to solve the problem? Try to keep this as unique to the method as possible.

E.g., ResNet introduced skip connections to allow inputs to short circuit convolution layers."
2766,@marktenenholtz,2022-01-17 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1483061566253584389,"What was the problem?

All methods are created to solve a problem.

Now that you have the background, what is the specific problem the field was having?

E.g., prior to ResNet's invention, computer vision researchers were struggling to train deeper models."
2767,@marktenenholtz,2022-01-17 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1483061565171531776,"Where was the field?

Paint a general picture of what the field was like at the time of method's invention.

E.g., prior to ResNet's invention, the computer vision community discovered that bigger models were more accurate."
2768,@marktenenholtz,2022-01-17 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1483061564064165888,"The history of machine learning is one big story. Concepts will stick when you think of them like a chapter in that story.

The steps of that story should be:
1. Where was the field?
2. What was the problem?
3. Why is it unique?"
2769,@marktenenholtz,2022-01-17 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1483061563007266816,3 steps to learn any complex machine learning concept:
2770,@marktenenholtz,2022-01-17 02:49:58+00:00,https://twitter.com/marktenenholtz/status/1482908024415076358,@paneer_sharma Check this out
2771,@marktenenholtz,2022-01-16 23:49:03+00:00,https://twitter.com/marktenenholtz/status/1482862494121086984,"@ScottBrincat Sorry you didn’t like the tweet, Scott.

Sometimes, I tweet ideas that are on my mind. Other times, I tweet more specific, concrete advice.

Maybe you’ll like this tweet more where I provide a specific method for improving model validation."
2772,@marktenenholtz,2022-01-16 23:21:39+00:00,https://twitter.com/marktenenholtz/status/1482855599817822213,@JoshuaPickard_ Here’s my favorite paper on it! I also plan on writing some threads on how it’s best done on different types of data and situations. https://t.co/8EE4mLmaGH
2773,@marktenenholtz,2022-01-16 22:40:32+00:00,https://twitter.com/marktenenholtz/status/1482845252465610753,"@olivia_p_walker Agreed!

A perfectly set up analysis is a beautiful thing."
2774,@marktenenholtz,2022-01-16 22:09:44+00:00,https://twitter.com/marktenenholtz/status/1482837500313972739,@LBacaj Agreed. My tweets are much better when I schedule them multiple days in advance and then revisit them a couple of times before they go live.
2775,@marktenenholtz,2022-01-16 21:10:04+00:00,https://twitter.com/marktenenholtz/status/1482822482813927428,@edublancas I didn’t even learn roll-forward cross-validation in my Master’s program. Pretty awful.
2776,@marktenenholtz,2022-01-16 20:50:20+00:00,https://twitter.com/marktenenholtz/status/1482817518708445186,@aakashg0 My pinned tweet and this one are good examples
2777,@marktenenholtz,2022-01-16 20:49:44+00:00,https://twitter.com/marktenenholtz/status/1482817367482802182,@aakashg0 There were maybe 5 or 6 tweets/threads that blew up and accounted for most of the growth
2778,@marktenenholtz,2022-01-16 20:48:43+00:00,https://twitter.com/marktenenholtz/status/1482817112645283842,@aakashg0 Same to you!
2779,@marktenenholtz,2022-01-16 20:44:50+00:00,https://twitter.com/marktenenholtz/status/1482816133489209345,@aakashg0 Through the roof. I had 101 followers 5 weeks ago.
2780,@marktenenholtz,2022-01-16 20:43:59+00:00,https://twitter.com/marktenenholtz/status/1482815919101550595,@_MarkConway_ Indeed — model validation quickly gets very complicated in the real world
2781,@marktenenholtz,2022-01-16 20:43:13+00:00,https://twitter.com/marktenenholtz/status/1482815727857983501,"@aakashg0 Interesting! I’ve found that my engagement skyrockets when I tweet less and focus on quality over quantity.

Good to see others having success with other strategies."
2782,@marktenenholtz,2022-01-16 20:27:11+00:00,https://twitter.com/marktenenholtz/status/1482811694229471233,"@_kenny_joseph @rasbt For me, I would focus on as much hands on practice with as much variety of data as possible.

Kaggle competition datasets are great for this."
2783,@marktenenholtz,2022-01-16 18:19:27+00:00,https://twitter.com/marktenenholtz/status/1482779546239787010,"@JFPuget It’s the way I learned.

On competitions like Petfinder, I didn’t even have to be that worried that I had a bad public LB position because I was confident in my local CV."
2784,@marktenenholtz,2022-01-16 17:52:36+00:00,https://twitter.com/marktenenholtz/status/1482772792059830284,@Suryao01 It depends entirely on the data that you’re using
2785,@marktenenholtz,2022-01-16 17:25:37+00:00,https://twitter.com/marktenenholtz/status/1482766000894394380,"@_smanna Great point!

The best way to know how to create validation sets is to understand all aspects of your data and know what your model will see in production."
2786,@marktenenholtz,2022-01-16 15:56:49+00:00,https://twitter.com/marktenenholtz/status/1482743652501884929,"@tdinh_me With the right kind of cult appeal, some extremely successful brands don’t even do typical advertising.

Tesla markets their products extremely similarly to the way you do it when you build in public."
2787,@marktenenholtz,2022-01-16 15:52:04+00:00,https://twitter.com/marktenenholtz/status/1482742457855098880,"@__mharrison__ If done correctly on a hard enough problem, it’s indistinguishable from black magic!"
2788,@marktenenholtz,2022-01-16 15:33:26+00:00,https://twitter.com/marktenenholtz/status/1482737766282711057,@tunguz I know I wouldn’t have achieved Kaggle Master status by moving 440 places up the Petfinder private LB if I didn’t!
2789,@marktenenholtz,2022-01-16 14:41:54+00:00,https://twitter.com/marktenenholtz/status/1482724798249816065,"@marcrgruener I plan on writing threads on all different types of data and methods for creating validation sets on them.

Stay tuned!"
2790,@marktenenholtz,2022-01-16 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1482699169819029504,"The worst taught skill in machine learning: model validation.

For whatever reason, most universities find it sufficient to discuss train/test splitting and simple cross validation... and that's it.

Model validation is an art.

The best data scientists do it like Picasso."
2791,@marktenenholtz,2022-01-15 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1482336784830795776,"The four questions you must ask yourself before you build a model:

1. What are the inputs/outputs in training?

2. What are the inputs/outputs in production?

3. What are you expecting it to learn?

4. What happens when it fails?"
2792,@marktenenholtz,2022-01-14 21:33:22+00:00,https://twitter.com/marktenenholtz/status/1482103572179398664,"@abhi1thakur Don’t let the negativity keep your down. 

For every public detractor you have 1000 silent supporters."
2793,@marktenenholtz,2022-01-14 19:02:58+00:00,https://twitter.com/marktenenholtz/status/1482065723409190913,"@svpino Let’s do it.

One of the best ways I know to help you is to do some giving of my own.

My goal here is to reveal the gated knowledge in this field that will 10x everyone’s abilities."
2794,@marktenenholtz,2022-01-14 16:20:16+00:00,https://twitter.com/marktenenholtz/status/1482024778244440064,"@_jaydeepkarale @DThompsonDev It was just Halloween, I promise!"
2795,@marktenenholtz,2022-01-14 15:11:13+00:00,https://twitter.com/marktenenholtz/status/1482007401511456771,"@DThompsonDev Hey, I’m Mark!

I’m a data scientist trying to teach everyone, beginner to research scientist-level, the things they don’t know about applying machine learning! https://t.co/Y6wqQ3puGk"
2796,@marktenenholtz,2022-01-14 14:21:14+00:00,https://twitter.com/marktenenholtz/status/1481994823754203141,@blacksmithcode It’s really cool to see a field evolve!
2797,@marktenenholtz,2022-01-14 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1481974423364124675,"Thanks for reading and I'm glad we all know the truth about batch norm.

If you liked this, I've been posting threads on machine learning nearly every day. 

Follow me and learn more about the interesting parts of the field and how to be an elite practitioner!"
2798,@marktenenholtz,2022-01-14 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1481974422332272640,"So, after all of that, here is what we actually know:

1. Batch norm provides regularization
2. This regularization lets you use higher learning rates
3. Until recently, deep models needed it.

That's pretty much it."
2799,@marktenenholtz,2022-01-14 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1481974421245992961,"Some theories have been proposed about the true reason for its success.

For instance Santurkar et. al. claimed it smooths the loss landscape.

Unfortunately, theories are just about all we have."
2800,@marktenenholtz,2022-01-14 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1481974420247699460,"By showing this, they (seemingly) refuted the central claims of the original paper."
2801,@marktenenholtz,2022-01-14 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1481974419178151936,"Those plots show two things about the difference between regular batch norm and the noised version with covariate shift:

1. There's almost no difference in performance

2. The distributions of activations are quite similar"
2802,@marktenenholtz,2022-01-14 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1481974417676648449,"They set up an experiment where noise was added to the activations coming out of each layer, immediately after batch norm was applied.

The noise had non-zero mean and non-unit variance, and it changed at every time step. 

This purposefully causes covariate shift.

The results: https://t.co/wcPwfJpYby"
2803,@marktenenholtz,2022-01-14 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1481974412597338113,"This paper showed two main things:

1. It doesn't reduce covariate shift.
2. Even if it did, that wouldn't help.

Here's how they justified it:"
2804,@marktenenholtz,2022-01-14 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1481974410445656068,"Regardless, it was obvious this method helped a lot and the authors had discovered something monumental.

Over the next few years, several researchers worked to discover the truth that seemed missing."
2805,@marktenenholtz,2022-01-14 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1481974409376063492,"In addition, their central claim had a big implication: that the reduction of covariate shift helps the model.

Reading the paper leaves the reader feeling that the claim, while interestingly intuitive, wasn't definitively proven."
2806,@marktenenholtz,2022-01-14 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1481974407924908032,"However, there was a bit of a problem.

A close reading of the paper showed that if batch norm had an effect, it wasn't a huge one.

Take a look at this graphic from the paper that tries to justify reduced covariate shift. 

It's not really that conclusive! https://t.co/yzhlCYJpBc"
2807,@marktenenholtz,2022-01-14 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1481974403051130883,"The results were staggering.

Their best model had a top-5 error rate nearly 20% lower than the next highest score.

The authors, Ioffe and Szegedy, had struck gold."
2808,@marktenenholtz,2022-01-14 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1481974401817972738,"In other words, a layer in a model may start by only seeing activations between, say, -1 and 1.

However, as training progressed, the activations flowing into that layer may increase their size and scale.

By training's end, the incoming activations may be between, say, 0 and 10."
2809,@marktenenholtz,2022-01-14 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1481974400731602944,"In 2015, a paper came out with a bold claim.

Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

The claim was that the major issue hampering deep networks was that the scales of the activations flowing through the model fluctuated."
2810,@marktenenholtz,2022-01-14 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1481974399586553857,"However, this brought about crippling issues.

Most notably, the deeper the model was, the more noisy the loss curves.

Because the model's loss was more volatile, lower learning rates had to be used. 

This made deep models expensive for the era and time-consuming to train."
2811,@marktenenholtz,2022-01-14 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1481974398026682369,"(sidenote: 

I wrote a thread on how ResNet came about during that time. 

Read it if you're interested in the history behind the developement of modern computer vision architectures)
https://t.co/wwXaHgwJCe"
2812,@marktenenholtz,2022-01-14 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1481974396726087681,"Around this time, computer vision was entering a renaissance.

Imagenet records were being broken left and right, and the theme was consistent:

Deeper models were the way to go."
2813,@marktenenholtz,2022-01-14 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1481974395232927745,"You may think deep learning models are finicky and difficult to train now, but just prior to 2015, it was far worse.

Take a look at how bumpy the loss is of a VGG model with and without batch norm (h/t Santurkar et. al.) : https://t.co/9891qYFKQi"
2814,@marktenenholtz,2022-01-14 13:00:02+00:00,https://twitter.com/marktenenholtz/status/1481974389021110273,"You probably don't know why batch normalization works.

It's one of the most influential deep learning components, and yet the vast majority of practitioners think it works for the wrong reasons.

The reality is, we're not certain why it works.

Here's what we know 🧵"
2815,@marktenenholtz,2022-01-13 16:05:31+00:00,https://twitter.com/marktenenholtz/status/1481658677186768897,"@tunguz Sex is great, but have you ever committed art theft by downloading a jpeg?"
2816,@marktenenholtz,2022-01-13 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1481612034177331200,"To sum it up:

Learn the concept and build up an intuition for why it works. 

You should be able to explain without using any math for most methods.

The math is important, but only as far as it achieves the conceptual goal."
2817,@marktenenholtz,2022-01-13 13:00:10+00:00,https://twitter.com/marktenenholtz/status/1481612031916589056,"What makes a method ""stick"" in my head is hearing the concept that makes it work.

Momentum in SGD helps to push through bumpy loss regions.

Batch norm smooths the loss landscape by helping scale and shift activations."
2818,@marktenenholtz,2022-01-13 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1481612030695985155,"So why are we taught with such an emphasis on the math?

More often than not, mathematical justification for a method is created after the fact b/c researchers want to be rigorous.

When math is involved, it’s the means that achieves the intuitive end, not the other way around."
2819,@marktenenholtz,2022-01-13 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1481612029605531652,"One of the biggest misconceptions about deep learning research is that new research arises from mathematical derivation.

Breakthroughs in DL come from intuitive realizations about the issues a method has, not because a researcher sat down and furiously scribbled proofs."
2820,@marktenenholtz,2022-01-13 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1481612028435640321,"You were probably taught deep learning in an overly complex, difficult to comprehend manner.

After spending years learning in this field, I believe I've discovered the best ways to understand it.

Here's how I think everyone should be taught 🧵"
2821,@marktenenholtz,2022-01-13 01:00:11+00:00,https://twitter.com/marktenenholtz/status/1481430843058970624,@MeganRisdal What competition (that had any sort of prize) had the most participants?
2822,@marktenenholtz,2022-01-12 17:12:32+00:00,https://twitter.com/marktenenholtz/status/1481313157130100736,@AndrewMendez19 Great answer. Proper model validation is a tricky beast.
2823,@marktenenholtz,2022-01-12 17:00:03+00:00,https://twitter.com/marktenenholtz/status/1481310015008874498,"What's a machine learning/deep learning concept or method that you can never quite get to ""stick"" in your head?

The one you always seem to forget the details behind?"
2824,@marktenenholtz,2022-01-12 16:14:07+00:00,https://twitter.com/marktenenholtz/status/1481298456610328580,@aaditsh Definitely texting and answering calls when my hands are dirty
2825,@marktenenholtz,2022-01-12 15:12:12+00:00,https://twitter.com/marktenenholtz/status/1481282872933769224,@A_K_Nain It’s the simple things in life… like framework interoperability 🥲
2826,@marktenenholtz,2022-01-12 13:24:03+00:00,https://twitter.com/marktenenholtz/status/1481255657848381444,"@svpino The good test is the one that tests the right things, like finding out if the candidate is a good problem solver and a learner.

There’s no correct way to do it, but I’d argue there are incorrect ways of doing it :)"
2827,@marktenenholtz,2022-01-12 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1481249630310391809,"Just as PyTorch usurped TensorFlow with a clear vision to do one thing well, I predict JAX will do the same to the both of them.

And it will happen sooner rather than later."
2828,@marktenenholtz,2022-01-12 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1481249629190582272,"So why do I think that excerpt from the JAX docs is important?

To me, it shows their dedication to doing one thing well.

Sure, integration with a JAX-built dataloader could be really nice.

But this is what open-source ecosystems are for."
2829,@marktenenholtz,2022-01-12 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1481249627982626819,"I still believe that PyTorch is great at this.

However, as the devs feel the need to appeal to more audiences, they are quickly losing their touch."
2830,@marktenenholtz,2022-01-12 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1481249626879496192,"It was a library that gave you much more control of the individual steps. 

As opposed to how TensorFlow had you floating on a rigid cloud and handled everything mysteriously behind the curtains.

This was so popular that even TensorFlow transitioned to that style."
2831,@marktenenholtz,2022-01-12 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1481249625751232512,"Software design principles tell us that libraries should do one thing really well.

Andrej's point is that PyTorch used to do that; PyTorch made its name as a much more hackable version of TensorFlow."
2832,@marktenenholtz,2022-01-12 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1481249624492969984,"Why is this relevant?

As we watch PyTorch age, we're starting to see it forget what made it popular in the first place.

@karpathy brings up a great point in this tweet: https://t.co/QAZJPO0Adn"
2833,@marktenenholtz,2022-01-12 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1481249618738376704,"I've been deep-diving into JAX lately because I think it is the future of deep learning.

However, while reading the docs, the thing that stood out to me the most was not something it does

Rather, it was something it *doesn't* do. 🧵"
2834,@marktenenholtz,2022-01-11 21:47:40+00:00,https://twitter.com/marktenenholtz/status/1481020006351712257,"@LBacaj This is super interesting Louie!

“A whole year of one on one’s” from a highly successful, honest manager compressed into 90 minutes sounds like something that everyone needs to hear."
2835,@marktenenholtz,2022-01-11 19:03:12+00:00,https://twitter.com/marktenenholtz/status/1480978616494272520,"@svpino You can also quantify your error (mean squared error) as a combination of bias and variance!

Specifically:

MSE = Bias squared + variance"
2836,@marktenenholtz,2022-01-11 17:14:24+00:00,https://twitter.com/marktenenholtz/status/1480951239244455942,@gusthema Google is the AI that powers the training of AI
2837,@marktenenholtz,2022-01-11 16:09:01+00:00,https://twitter.com/marktenenholtz/status/1480934782766882818,@JustSomeDev I rarely find them to be useful
2838,@marktenenholtz,2022-01-11 16:08:34+00:00,https://twitter.com/marktenenholtz/status/1480934668279201793,@marco_gaertner I would say my favorite resource is @paperswithcode (shoutout to @omarsar0 )
2839,@marktenenholtz,2022-01-11 16:07:53+00:00,https://twitter.com/marktenenholtz/status/1480934497440968710,@rararaluna Exactly! It's hard to know what to do unless you are one with the data.
2840,@marktenenholtz,2022-01-11 16:07:19+00:00,https://twitter.com/marktenenholtz/status/1480934357267365891,"@EvanBudianto It's hard to give specific functions, but I'll write a thread that dives into a lot more detail of what you should be writing"
2841,@marktenenholtz,2022-01-11 16:06:32+00:00,https://twitter.com/marktenenholtz/status/1480934157320699907,@anuraggarg04 I'll write a thread about it in the near future with good examples
2842,@marktenenholtz,2022-01-11 16:06:00+00:00,https://twitter.com/marktenenholtz/status/1480934024545771525,"@Hakymulla I've gotten a decent amount of questions about this, so I'll write a thread about it."
2843,@marktenenholtz,2022-01-11 14:20:51+00:00,https://twitter.com/marktenenholtz/status/1480907563063558149,"@blacksmithcode I haven't checked that one out.

Thanks for the warning, haha"
2844,@marktenenholtz,2022-01-11 13:58:00+00:00,https://twitter.com/marktenenholtz/status/1480901813343764485,"@blacksmithcode No problem!

Python gives more options than a lot of languages to write clean, concise code, so it’s worth it to use them 😀"
2845,@marktenenholtz,2022-01-11 13:56:09+00:00,https://twitter.com/marktenenholtz/status/1480901347947991042,"@TheGeekyB0y @PrasoonPratham I did that a lot in this thread and most of my other tweets, but I wanted to test it without for once"
2846,@marktenenholtz,2022-01-11 13:52:56+00:00,https://twitter.com/marktenenholtz/status/1480900538615140354,@IttaiSvidler Just gimme a few more minutes 😜
2847,@marktenenholtz,2022-01-11 13:39:58+00:00,https://twitter.com/marktenenholtz/status/1480897272661651457,"@PrasoonPratham Completely agree!

And if it’s useful, I just wrote a thread with a bunch of useful Python patterns. 👇"
2848,@marktenenholtz,2022-01-11 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1480887239349452802,"I tried to be pretty comprehensive for these common situations. 

But, there are many more patterns that are worth covering if you want to write readable Python code.

Follow me @marktenenholtz as I share more useful Pythonic patterns."
2849,@marktenenholtz,2022-01-11 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1480887238179254275,"Adding new items to a dict:

If you're iterating to create a dict, use a dict comprehension:

{k: i for i, k in enumerate(mylist)}

If you're merging two dicts:

For Python&gt;=3.9: z = x | y
For Python&gt;=3.5: z = {**x, **y}"
2850,@marktenenholtz,2022-01-11 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1480887237172600832,"Simultaneous dictionary looping:

Use zip the same as before!

for r1, r2 in zip(dict1.items(), dict2.items()):
    ..."
2851,@marktenenholtz,2022-01-11 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1480887236098871299,"Simple dictionary looping:

These are more situational. Here are your options:

Only need the keys?:

for k in mydict.keys():
    ...

Only need the values?:

for v in mydict.values():
    ...

Need both?

for k, v in mydict.items():
    ..."
2852,@marktenenholtz,2022-01-11 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1480887235071279106,"Adding items to a new list:

Bad:

newlist = []
for item in mylist:
    newlist.append(item)

Pythonic, use list comprehensions:

newlist = [item for item in mylist]

You can even do nested for loops in list comprehensions."
2853,@marktenenholtz,2022-01-11 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1480887234014236672,"Simultaneous list looping:

Bad:

for i in range(len(mylist1)):
    item1 = mylist1[i]
    item2 = mylist2[i]

Pythonic, use zip:

for item1, item2 in zip(mylist1, mylist2):
    ...

If you need the index still:

for i, (it1, it2) in enumerate(zip(list1, list2)):
    ..."
2854,@marktenenholtz,2022-01-11 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1480887232919531520,"Simple list looping

Bad:

for i in range(len(mylist)):
    item = mylist[i]
    ...

Pythonic:

for item in mylist:
    ...

If you need the index, use the built-in enumerate function:

for i, item in enumerate(mylist):
    ..."
2855,@marktenenholtz,2022-01-11 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1480887231770337283,"If you're manually indexing a Python list or dict in a loop, you're probably doing it wrong.

Here are the Pythonic patterns you should be using 🧵"
2856,@marktenenholtz,2022-01-10 21:02:34+00:00,https://twitter.com/marktenenholtz/status/1480646268082921473,@frenchfries02 DM me
2857,@marktenenholtz,2022-01-10 15:52:51+00:00,https://twitter.com/marktenenholtz/status/1480568327298854912,"@evanlapointe Yes, especially if they're a CS person.

90% of even the most theoretical work is getting the right data in the right place at the right time.

If you're good at programming, you can add a ton of value there even if the science is way over your head."
2858,@marktenenholtz,2022-01-10 15:39:51+00:00,https://twitter.com/marktenenholtz/status/1480565055418732546,"@evanlapointe Great point, Evan.

In data science we're stuck in an era where candidates with the requisite knowledge were few and far between.

This knowledge (while still heavily gated) is becoming more of a commodity, so we need to move on to the  real differentiators, the above qualities."
2859,@marktenenholtz,2022-01-10 15:03:24+00:00,https://twitter.com/marktenenholtz/status/1480555880840105986,"@gusthema Not only a great analogy, but an original one that I haven’t heard before!

Replace these with data scientist, data engineer, and software engineer and you have a tech company"
2860,@marktenenholtz,2022-01-10 13:09:07+00:00,https://twitter.com/marktenenholtz/status/1480527124020740103,@abhi1thakur I’ve tried it and it has never helped
2861,@marktenenholtz,2022-01-10 13:01:23+00:00,https://twitter.com/marktenenholtz/status/1480525174587990021,"@svpino Personally, I prefer LightGBM to XGBoost.

Similarly accurate, but much more performant."
2862,@marktenenholtz,2022-01-10 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1480524883515772931,"There’s no particular order for these 4 tasks. They all go hand in hand.

But, you’re getting ahead of yourself if you’re seriously building models before you do these things.

Follow me for breakdowns on these 4 tasks and what to do after you complete them."
2863,@marktenenholtz,2022-01-10 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1480524882433613825,"Solution engineering

For a 3 month long project, I spend most of the first week or two after doing EDA catching up with the latest relevant research, planning/brainstorming my approach.

A good plan leads to good direction.

Good direction leads to effective problem solving."
2864,@marktenenholtz,2022-01-10 13:00:13+00:00,https://twitter.com/marktenenholtz/status/1480524881263398915,"Exploratory data analysis

It sounds obvious, but EDA truly separates the best data scientists from everyone else.

The best model is the one that's perfect for your data.

You should hesitate to move on to anything else until you know your data to the core."
2865,@marktenenholtz,2022-01-10 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1480524880118370306,"Write error analysis code

Similar to the previous item, but deserves its own mention.

If you can identify bad error patterns after training a model, it gives you immediate feedback on how to improve.

Do this early and waste less time later on bad ideas."
2866,@marktenenholtz,2022-01-10 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1480524879040368643,"Write convenience functions

These are functions that do common data transforms, visualization, error analysis, etc.

Deep into projects, it's common to skip steps because of code fatigue.

Don't give yourself this excuse.

Get the hard work out of the way and move faster."
2867,@marktenenholtz,2022-01-10 13:00:12+00:00,https://twitter.com/marktenenholtz/status/1480524877878624256,"I spent 500+ hours on Kaggle competitions last year and medaled 3 times, including my first solo gold medal.

All those hours taught me what an ideal model development pipeline should look like.

4 things you should do before training models 🧵"
2868,@marktenenholtz,2022-01-10 12:33:36+00:00,https://twitter.com/marktenenholtz/status/1480518183954096131,"@RyszardZyga Absolutely. 

That quality along with conscientiousness are probably the best predictors of success out there."
2869,@marktenenholtz,2022-01-10 12:32:02+00:00,https://twitter.com/marktenenholtz/status/1480517788229939205,"@kkkroger I would start by learning Python.

If you already know Python, start reading online tutorials/taking courses on data visualization, simple model building, and data engineering."
2870,@marktenenholtz,2022-01-10 00:48:29+00:00,https://twitter.com/marktenenholtz/status/1480340735429955584,@LBacaj Absolutely. Pretty much any tech job.
2871,@marktenenholtz,2022-01-09 22:09:53+00:00,https://twitter.com/marktenenholtz/status/1480300822068146184,@talkdatatomee Focus on testing how candidates work through a problem with layers
2872,@marktenenholtz,2022-01-09 21:02:44+00:00,https://twitter.com/marktenenholtz/status/1480283925163483148,@dvassallo A growth mindset is the tide that lifts all boats
2873,@marktenenholtz,2022-01-09 19:50:15+00:00,https://twitter.com/marktenenholtz/status/1480265682830909449,"@ustx987 I’ve interviewed many data scientists and know many others who also do interviews.

These are the competencies that we all evaluate candidates on."
2874,@marktenenholtz,2022-01-09 13:03:38+00:00,https://twitter.com/marktenenholtz/status/1480163354354327562,"The best data scientists don't necessarily have:

• The most calculus skill
• The most advanced degrees
• The most resources

They are:

• The most creative
• The most willing to learn
• The most persistent

All you need is the willingness to keep trying and keep learning."
2875,@marktenenholtz,2022-01-09 13:02:29+00:00,https://twitter.com/marktenenholtz/status/1480163065501036552,"@svpino The best part: 

So many accelerated learning paths exist that allow new folks that are motivated to leap frog less-motivated folks that are already in the field.

The field is progressing so quickly that learning from scratch now teaches you more modern best practices."
2876,@marktenenholtz,2022-01-08 22:14:13+00:00,https://twitter.com/marktenenholtz/status/1479939526185390088,"@jgat2011 @svpino No guarantees I’ll remember, so follow me if you don’t want to miss it!"
2877,@marktenenholtz,2022-01-08 20:47:11+00:00,https://twitter.com/marktenenholtz/status/1479917624372965378,"@haltakov Thank you!

I have a lot more of these planned 😀"
2878,@marktenenholtz,2022-01-08 20:23:34+00:00,https://twitter.com/marktenenholtz/status/1479911679232196614,"@svpino So far, the most interesting response distribution is the 3rd question, to me.

Maybe I should do some threads about the sub-field of reasoning and knowledge graphs!"
2879,@marktenenholtz,2022-01-08 17:25:50+00:00,https://twitter.com/marktenenholtz/status/1479866951581241344,"@fchollet In general, focus your resources where they are most efficiently invested.

Have a 10x engineer on your team? 

Give them the highest leverage work."
2880,@marktenenholtz,2022-01-08 16:38:26+00:00,https://twitter.com/marktenenholtz/status/1479855021005185030,"@LBacaj Another great story, Louie.

Sounds like this country has been wonderful for your family!"
2881,@marktenenholtz,2022-01-08 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1479800069054337026,"The 80/20 rule applies here, and 80% of the benefit always comes from understanding the problem and the data.

Kaggle competitions teach us what works and what doesn't so that we can spend less time building better models."
2882,@marktenenholtz,2022-01-08 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1479800067972206593,"How most data scientists/MLEs spend time solving problems:

20% scoping solution/data analysis
80% tweaking the model

How they should be spending time:

80% scoping solution/data analysis
20% tweaking the model"
2883,@marktenenholtz,2022-01-07 21:44:18+00:00,https://twitter.com/marktenenholtz/status/1479569607409254401,"@haltakov If you liked that, these cool visuals are for you:"
2884,@marktenenholtz,2022-01-07 17:50:20+00:00,https://twitter.com/marktenenholtz/status/1479510727216050180,"@Austen This is common in companies involved in high-volume supply chains.

Data sharing is often used to prevent the bullwhip effect."
2885,@marktenenholtz,2022-01-07 17:00:09+00:00,https://twitter.com/marktenenholtz/status/1479498099638738944,"Just got access to AWS SageMaker Studio Lab.

Pretty interested to try out the newest Colab competitor. https://t.co/V2mBOPhFRe"
2886,@marktenenholtz,2022-01-07 16:32:18+00:00,https://twitter.com/marktenenholtz/status/1479491090919137292,"@LBacaj I moreso mean an outline where you just get your big thoughts on paper and elaborate on them a bit.

I find this keeps my ideas fresh for *significantly* longer since I can re-insert myself into the mental state I was in when I initially wrote it."
2887,@marktenenholtz,2022-01-07 16:03:38+00:00,https://twitter.com/marktenenholtz/status/1479483876015751168,"@chrisalbon I would add that you should try multiple methods and use cross-validation to determine which one is most effective.

(I'm also assuming this is past the step of ""understanding why the value is missing"")"
2888,@marktenenholtz,2022-01-07 16:00:25+00:00,https://twitter.com/marktenenholtz/status/1479483067379204103,"This is absolutely incredible.

I vote to make this a standard for all NeurIPS papers."
2889,@marktenenholtz,2022-01-07 15:45:18+00:00,https://twitter.com/marktenenholtz/status/1479479263006973955,"@Ar_Douillard @julien_c It was mostly a joke. 

Don't think I'd expect those results to show up in the same way in such a large model with such a different type of dataset, but I guess who knows."
2890,@marktenenholtz,2022-01-07 15:31:22+00:00,https://twitter.com/marktenenholtz/status/1479475756715626500,@julien_c This is like the opposite of that Grokking paper
2891,@marktenenholtz,2022-01-07 15:28:13+00:00,https://twitter.com/marktenenholtz/status/1479474962507321344,"@LBacaj If you're worried about it losing its ""freshness"" in your mind, start writing a rough draft of a book.

You don't literally have to write the book, but it could serve as your own outline of the material that you could teach from."
2892,@marktenenholtz,2022-01-07 13:00:29+00:00,https://twitter.com/marktenenholtz/status/1479437785815601153,"Another mindblowing feature from PyCaret 🤯 

If this wasn't crazy enough for you, add ""create_docker('api_script')"" instead of running your api and it will create a Dockerfile to host your model for you!
https://t.co/aAA5qN5qiB"
2893,@marktenenholtz,2022-01-07 00:55:52+00:00,https://twitter.com/marktenenholtz/status/1479255431411388419,@LBacaj @wightmanr Thanks Louie! Can’t thank you enough for your support.
2894,@marktenenholtz,2022-01-06 23:17:34+00:00,https://twitter.com/marktenenholtz/status/1479230692550709254,@PyTorchLightnin https://t.co/HnEEKKjLVb
2895,@marktenenholtz,2022-01-06 22:08:09+00:00,https://twitter.com/marktenenholtz/status/1479213223672954883,"@themintsv @wightmanr I'm not sure I understand your first issue.

Do you mean you can't map timm models to torchvision models?"
2896,@marktenenholtz,2022-01-06 20:56:58+00:00,https://twitter.com/marktenenholtz/status/1479195309972594693,@dav_ell @wightmanr Or this?
2897,@marktenenholtz,2022-01-06 20:56:17+00:00,https://twitter.com/marktenenholtz/status/1479195138643709956,@dav_ell @wightmanr Does this suit your fancy?
2898,@marktenenholtz,2022-01-06 20:48:12+00:00,https://twitter.com/marktenenholtz/status/1479193103399636996,@dav_ell @wightmanr Thanks David! I plan on continuing to do a lot more threads in a similar vein to this one
2899,@marktenenholtz,2022-01-06 18:28:37+00:00,https://twitter.com/marktenenholtz/status/1479157977621934080,@PaaSDev @wightmanr I always feel like I’m seeing an old friend when I type “import timm”
2900,@marktenenholtz,2022-01-06 17:29:50+00:00,https://twitter.com/marktenenholtz/status/1479143183544115210,"@unsorsodicorda @wightmanr Personally, I just use this feature and create my own model"
2901,@marktenenholtz,2022-01-06 17:17:24+00:00,https://twitter.com/marktenenholtz/status/1479140053385453582,@Pritish88951762 @wightmanr I use a VSCode extension called CodeSnap
2902,@marktenenholtz,2022-01-06 17:01:32+00:00,https://twitter.com/marktenenholtz/status/1479136062584672257,This is built into @PyTorchLightnin and I use it in all of my projects
2903,@marktenenholtz,2022-01-06 17:01:32+00:00,https://twitter.com/marktenenholtz/status/1479136061469040640,"Want more reliable deep learning pipelines?

Run a couple validation steps to test your validation function before you start training.

Reliable pipelines fail fast so you don't waste GPU time.

You can waste 20+ minutes on big datasets if you get an error after your first epoch."
2904,@marktenenholtz,2022-01-06 16:59:21+00:00,https://twitter.com/marktenenholtz/status/1479135512476766213,@TheZachMueller Take it from someone who has some gray hair at 26 from overworking myself… you’re doing the right thing!
2905,@marktenenholtz,2022-01-06 16:32:12+00:00,https://twitter.com/marktenenholtz/status/1479128679880048646,@wightmanr Thanks for creating my favorite PyTorch library!
2906,@marktenenholtz,2022-01-06 13:00:29+00:00,https://twitter.com/marktenenholtz/status/1479075397031456772,I regularly post helpful machine learning threads like this. Follow me so you don't miss them!
2907,@marktenenholtz,2022-01-06 13:00:28+00:00,https://twitter.com/marktenenholtz/status/1479075395961974784,Check out the GitHub repository here: https://t.co/TzDxjR9Jkr
2908,@marktenenholtz,2022-01-06 13:00:28+00:00,https://twitter.com/marktenenholtz/status/1479075394871386116,"18/ New models are implemented at a rapid pace

The newest research always finds its way in rapidly.

For instance, FAIR's recent implementation of XCiT was released on June 17.

It was live in the timm package on July 12 🤯"
2909,@marktenenholtz,2022-01-06 13:00:28+00:00,https://twitter.com/marktenenholtz/status/1479075393743167490,"17/ Exclusive, more efficient/performant implementations

Ross has included some models that are only in the timm package.

Usually, these are models that come from community-lead or his own research.

I use the ECA version of NFNets and the ResNetd models all the time!"
2910,@marktenenholtz,2022-01-06 13:00:28+00:00,https://twitter.com/marktenenholtz/status/1479075392673574912,"16/ Training, validation, and inference scripts

Not only can you reproduce the published results, but you can use these scripts with your own created models or your own datasets to produce comparable results.

Reproducible research is invaluable!"
2911,@marktenenholtz,2022-01-06 13:00:27+00:00,https://twitter.com/marktenenholtz/status/1479075391574712322,"15/ Readable code

There are so many architectures in the wild nowadays that I often forget their exact implementations.

Fortunately, the code in timm.models is so readable that it's the first place I go.

For me, code is more comprehensible than an explanation in a paper."
2912,@marktenenholtz,2022-01-06 13:00:27+00:00,https://twitter.com/marktenenholtz/status/1479075390232563712,"14/ One-argument stochastic depth

To train deeper models, stochastic depth is regularization that skips a random amount of the deepest layers.

As incredibly useful as this is, it's another feature that usually requires changes to the underlying model code.

But not in timm! https://t.co/fwurGO6kf8"
2913,@marktenenholtz,2022-01-06 13:00:26+00:00,https://twitter.com/marktenenholtz/status/1479075386994528256,"13/ Useful schedulers

I'm a broken record at this point, but...

Want to use a scheduler that's not in torch.optim?

timm has you covered in the timm.scheduler module.

You can even use the base Scheduler class to implement your own."
2914,@marktenenholtz,2022-01-06 13:00:26+00:00,https://twitter.com/marktenenholtz/status/1479075385912348673,"12/ Useful optimizers

Want to use an optimizer that's not in torch.optim?

timm has you covered in the timm.optim module.

Some of my favorites are LAMB, LARS, and Meta's new MADGRAD optimizer."
2915,@marktenenholtz,2022-01-06 13:00:26+00:00,https://twitter.com/marktenenholtz/status/1479075384754774017,"11/ Useful augmentations

Complicated augmentation strategies are ubiquitous nowadays, and timm has implementations for the best ones.

https://t.co/w7h3bc8jHo has implementations for MixUp, CutMix, AutoAugment, AugMix, and more!"
2916,@marktenenholtz,2022-01-06 13:00:26+00:00,https://twitter.com/marktenenholtz/status/1479075383701999619,"10/ Useful layers

The timm.layers module gives you access to all of the useful layers as nn.Module's that were introduced by the different architectures available to you.

Some of my favorites are depthwise separable convs and squeeze-and-excitation layers."
2917,@marktenenholtz,2022-01-06 13:00:25+00:00,https://twitter.com/marktenenholtz/status/1479075382582136832,"9/ Useful loss functions

The timm.loss module exposes some very useful loss functions.

My personal favorite is the cross-entropy loss that includes label smoothing as a parameter."
2918,@marktenenholtz,2022-01-06 13:00:25+00:00,https://twitter.com/marktenenholtz/status/1479075380048760832,"7/ Easy feature map extraction

All models implement the .forward_features() method, which outputs the unpooled feature maps from the last model layer.

This is useful if you want to replace the global pooling layer with any sort of attention layer or custom pooling operation. https://t.co/o1ROj0e7Bx"
2919,@marktenenholtz,2022-01-06 13:00:24+00:00,https://twitter.com/marktenenholtz/status/1479075375795761152,"6/ Model leaderboards for many datasets

Curious how any of the pretrained models are performing?

Check out the model leaderboards under the ""results"" directory: https://t.co/KiRg28Cakz"
2920,@marktenenholtz,2022-01-06 13:00:23+00:00,https://twitter.com/marktenenholtz/status/1479075371894984705,"5/ Modular model interface

Swapping in and out any type of CNN or Transformer requires absolutely ZERO code changes.

This is facilitated by the ""num_features"" attribute that every model has.

Here's all the code you need to use any timm model on a binary classification dataset: https://t.co/QEel8iuIu9"
2921,@marktenenholtz,2022-01-06 13:00:21+00:00,https://twitter.com/marktenenholtz/status/1479075366085926916,"3/ Effortless integration with PyTorch frameworks

Since the timm library focuses on implementing nn.Modules, you can easily use these with frameworks like @PyTorchLightnin or @fastdotai"
2922,@marktenenholtz,2022-01-06 13:00:21+00:00,https://twitter.com/marktenenholtz/status/1479075364752158723,"2/ One line to load any pretrained model

For any of those 570 pretrained models (or the rest, with pretrained=False), it's as simple as one line

We'll see some of the other things you can do with timm.create_model() below 👇 https://t.co/FskE8uQjos"
2923,@marktenenholtz,2022-01-06 13:00:20+00:00,https://twitter.com/marktenenholtz/status/1479075361233141760,"1/ 713 architectures implemented

These include many different types of architectures (both of the CNN and Transformer families).

The best part? 

570 of those architectures come with pretrained weights. https://t.co/BxnfQJh658"
2924,@marktenenholtz,2022-01-06 13:00:19+00:00,https://twitter.com/marktenenholtz/status/1479075356162228225,"Using state-of-the-art computer vision models is easier than ever ⚡ 

The PyTorch Image Models (timm) library from @wightmanr is a one-stop-shop for it all.

Here are 18 reasons why 🧵"
2925,@marktenenholtz,2022-01-06 03:51:54+00:00,https://twitter.com/marktenenholtz/status/1478937341217628161,"@IttaiSvidler @MeghanCotant me, rn https://t.co/MSVfsaOVUv"
2926,@marktenenholtz,2022-01-06 03:16:48+00:00,https://twitter.com/marktenenholtz/status/1478928511318564864,@LBacaj Seems to me that spending the time to write our thoughts is a great extension of what makes that Japanese cultural element so powerful.
2927,@marktenenholtz,2022-01-06 03:15:24+00:00,https://twitter.com/marktenenholtz/status/1478928158485266437,"@LBacaj I interned at Toyota and worked with a lot of Japanese engineers.

In Japanese culture, when asked a question, it’s common to pause for even whole minutes to ponder it.

In Western culture we expect everyone to just have a perfectly formulated answer immediately."
2928,@marktenenholtz,2022-01-06 01:33:58+00:00,https://twitter.com/marktenenholtz/status/1478902630017548288,"@LBacaj @kuznetsdev I struggle with this in my own tweets.

Screenshots are prettier, but it’s easier to write too much code.

I try to limit myself to 10-20 lines MAX unless I have an incredibly good reason.

And I’d be happy to teach you 😀"
2929,@marktenenholtz,2022-01-06 00:40:48+00:00,https://twitter.com/marktenenholtz/status/1478889251362291714,"@LBacaj Yup and that relates to the first point.

People put too much effort into using fancy project management tools and not enough effort into doing the important things.

Doesn’t take anything more than iOS notes!"
2930,@marktenenholtz,2022-01-05 18:55:15+00:00,https://twitter.com/marktenenholtz/status/1478802291297820675,"@Mrigendra_A Kaggle is great, but I’m taking places where you have to pay for GPU compute. 

Arguably, Colab shouldn’t be on the list since it’s still limited access."
2931,@marktenenholtz,2022-01-05 18:53:53+00:00,https://twitter.com/marktenenholtz/status/1478801946769346560,@svpino Just thought I’d leave this here https://t.co/5HeN5lh7xs
2932,@marktenenholtz,2022-01-05 18:00:02+00:00,https://twitter.com/marktenenholtz/status/1478788394293469185,"3 tips to keep your projects maintainable

• The best documentation strategy is the one you actually do
• Keep a tech debt log
• Done coding? Log what you want to do tomorrow

The logs can be as simple as a text file.

Worry less about fancy tooling, worry more about doing it."
2933,@marktenenholtz,2022-01-05 16:17:42+00:00,https://twitter.com/marktenenholtz/status/1478762643527806982,"@kuznetsdev @kaggle Kaggle is a good option.

Colab is probably the other best option."
2934,@marktenenholtz,2022-01-05 15:00:15+00:00,https://twitter.com/marktenenholtz/status/1478743151330267138,"GPUs are hard to buy and expensive.

But, for machine learning, you don't need to buy one.

Here are the cheapest places to rent GPUs:

1. VastAI
2. Google Colab
3. DataCrunch
4. JarvisCloud
5. Paperspace

Do your testing locally and then let their hardware work for you."
2935,@marktenenholtz,2022-01-04 17:00:18+00:00,https://twitter.com/marktenenholtz/status/1478410973950083076,"The world changes faster and faster every day.

The best way to keep up?

Get 1% better every day and change your life. https://t.co/5QRol4zBvc"
2936,@marktenenholtz,2022-01-04 13:00:09+00:00,https://twitter.com/marktenenholtz/status/1478350536906731521,"I love learning, but I love teaching about as much.

I hope you have as much fun consuming the content as I have creating it.

Follow me to learn as I share it!"
2937,@marktenenholtz,2022-01-04 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1478350535673532416,"3/ The most useful way to consume machine learning research is to remember its unique contributions.

""RetinaNet? Right, that's the paper that introduced Focal Loss.""

So, I post threads like this to make it all digestible.
https://t.co/vpLnFDdEDI"
2938,@marktenenholtz,2022-01-04 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1478350534369165314,"2/ Knowledge is gated and rapidly evolving

Keeping up with the latest in Python and machine learning is like drinking from a firehose.

Most of these advances happen in small circles, like research companies or Kaggle competitions.

But don't fret. This is where I come in."
2939,@marktenenholtz,2022-01-04 13:00:08+00:00,https://twitter.com/marktenenholtz/status/1478350533211557889,"Machine learning content should be accessible to all.

But, if you *really* want to be a top-tier practitioner, you need to also stick around for the advanced content."
2940,@marktenenholtz,2022-01-04 13:00:07+00:00,https://twitter.com/marktenenholtz/status/1478350531940610048,"1/ Beginner content, but intermediate+ too

Google any machine learning topic.

What do you find?

A plethora of introductory level content and very, very, very little beyond that.

This is a problem. Intro level content doesn't cut it in industry and research."
2941,@marktenenholtz,2022-01-03 21:22:48+00:00,https://twitter.com/marktenenholtz/status/1478114647119118338,"@svpino @capeandcode @VladPasca5 Same here, no intention of dunking on anyone.

It's better to spend time building a sustainable, efficient balance.

It's harder to do, but infinitely valuable."
2942,@marktenenholtz,2022-01-03 21:07:40+00:00,https://twitter.com/marktenenholtz/status/1478110837579603972,"@svpino @capeandcode @VladPasca5 Nobody makes it in life when they're constantly facing burnout.

I have some gray hair at 26 b/c I'm really good at overworking myself.

One of my recent goals in life has been to better manage my stress.

But hey, as you've taught, controversial opinions are good for engagement!"
2943,@marktenenholtz,2022-01-03 20:06:19+00:00,https://twitter.com/marktenenholtz/status/1478095397537173511,@tunguz Bump:
2944,@marktenenholtz,2022-01-03 19:05:42+00:00,https://twitter.com/marktenenholtz/status/1478080145038749696,"@svpino This is a wonderful format and it’s great to see you following up on the questions in this way.

I learned a lot from your Twitter growth guide and I’m sure I’ll learn more from these questions!"
2945,@marktenenholtz,2022-01-03 17:00:06+00:00,https://twitter.com/marktenenholtz/status/1478048536801394689,"Need to merge two Python dictionaries?

Here are the best ways to do it, along with some bad ways of doing it that I've seen.

Python 3.9 introduced my favorite way of doing it 👇 https://t.co/xOi1N1AE9O"
2946,@marktenenholtz,2022-01-03 15:40:07+00:00,https://twitter.com/marktenenholtz/status/1478028406625349632,"@LBacaj It’s okay, Louie, I’ll do it for you. That’s why you keep me around."
2947,@marktenenholtz,2022-01-03 15:37:19+00:00,https://twitter.com/marktenenholtz/status/1478027702011633678,"@LBacaj Man, 5 whole sick days. What a gig! 😂"
2948,@marktenenholtz,2022-01-03 13:00:06+00:00,https://twitter.com/marktenenholtz/status/1477988136827244546,Link to the paper: https://t.co/Fj1UafvCeA
2949,@marktenenholtz,2022-01-03 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1477988135476756482,"Bayesian neural networks remove diversity

This paper showed that Bayesian networks tend to settle on more similar ways of solving the problem, cutting out all diversity.

Randomly initialized ensembles, instead, find different solutions that give them better model diversity. https://t.co/mWigcCgi7u"
2950,@marktenenholtz,2022-01-03 13:00:05+00:00,https://twitter.com/marktenenholtz/status/1477988132456853505,"How do you make use of this?

If you want to squeeze more accuracy out of your deep learning model, just train the model a few times on different seeds and average the predictions from those models"
2951,@marktenenholtz,2022-01-03 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1477988131404062722,"However, this paper shows that randomly initialized models find different but nearly equally accurate solutions, or ways of approaching the problem.

This creates effectively diverse models.

(Just think about that for a second. Same models/data, different init = better results!)"
2952,@marktenenholtz,2022-01-03 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1477988130376458243,"Differently initialized neural networks ensemble well

Traditional ML experience tells us that models with the same architecture and trained on the same data shouldn't ensemble well because there's no model diversity."
2953,@marktenenholtz,2022-01-03 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1477988128971362308,"Fascinating paper you need to read: 

Deep Ensembles: A Loss Landscape Perspective

Everything you need to know 🧵 https://t.co/sPCd5bXqN2"
2954,@marktenenholtz,2022-01-02 13:00:04+00:00,https://twitter.com/marktenenholtz/status/1477625740354936833,S/O to Moez for sharing this here: https://t.co/Ywlx3Cm6Kv
2955,@marktenenholtz,2022-01-02 13:00:03+00:00,https://twitter.com/marktenenholtz/status/1477625738865954816,"Mindblowing speed to model deployment 🤯 

The newest version of PyCaret allows you to deploy your entire ML pipeline as a FastAPI web API from a notebook!

You can do it all in 4 lines of code 👇 https://t.co/2iUlWa9ffK"
2956,@marktenenholtz,2022-01-02 01:56:13+00:00,https://twitter.com/marktenenholtz/status/1477458677544148997,@tunguz @dvassallo Take it and run!
2957,@marktenenholtz,2022-01-02 01:36:07+00:00,https://twitter.com/marktenenholtz/status/1477453619842662403,"@dvassallo Sometimes, smarter people are just better at deluding themselves."
2958,@marktenenholtz,2022-01-01 21:17:29+00:00,https://twitter.com/marktenenholtz/status/1477388534285754375,@LBacaj It’s easier when you recognize how many things we needlessly hide from each other
2959,@marktenenholtz,2022-01-01 19:16:55+00:00,https://twitter.com/marktenenholtz/status/1477358189649833990,@svpino This was my first go at it:
2960,@marktenenholtz,2022-01-01 19:15:40+00:00,https://twitter.com/marktenenholtz/status/1477357878235254786,"@svpino This is a great idea.

It’s good to see what others in ML Twitter are planning for the new year.

For me, I’m planning on doing threads on the key new ideas that different papers/architectures introduced to make them accessible.

Keep crushing it, Santiago!"
2961,@marktenenholtz,2022-01-01 19:01:59+00:00,https://twitter.com/marktenenholtz/status/1477354434288041990,"@svpino Definitely 3. 

What is this question from?"
2962,@marktenenholtz,2022-01-01 17:02:22+00:00,https://twitter.com/marktenenholtz/status/1477324329763364865,"The best way to learn to train better models?

Learn what it takes to deploy them."
2963,@marktenenholtz,2022-01-01 05:00:05+00:00,https://twitter.com/marktenenholtz/status/1477142560741081090,"You will:
• Use the Serverless Framework to manage and deploy serverless apps
• Serve @huggingface transformers in AWS Lambda
• Set up an HTTP endpoint for inference

On AWS Free Tier, you'll be able to run thousands of predictions for free. 

Give it a shot!"
2964,@marktenenholtz,2022-01-01 05:00:04+00:00,https://twitter.com/marktenenholtz/status/1477142558840999936,"The best way to start deploying models in the cloud? 

Serverless architecture. The benefits:
• Cheap
• Massively scalable
• No server management

Want to get started? Check out this great tutorial from @_philschmid. 

Here's what you'll learn 👇

https://t.co/aPcXfypHvq https://t.co/Vk9MNFCKmC"
2965,@marktenenholtz,2021-12-31 22:26:21+00:00,https://twitter.com/marktenenholtz/status/1477043474411274243,@tunguz https://t.co/uP1rM6u3wn
2966,@marktenenholtz,2021-12-31 22:22:51+00:00,https://twitter.com/marktenenholtz/status/1477042594077196290,"@tunguz I would say AlphaFold. 

The technology it enables should be widespread and highly impactful."
2967,@marktenenholtz,2021-12-31 15:33:06+00:00,https://twitter.com/marktenenholtz/status/1476939480212942850,"I didn't make any resolutions for 2021, so I asked GPT-3 to generate some for me. Here they are:

- Write a blog
- Solve some challenges on Math and Science
- Take up a new hobby
- Do something more outdoors
- Do something in particular that takes up a lot of my time"
2968,@marktenenholtz,2021-12-31 15:27:27+00:00,https://twitter.com/marktenenholtz/status/1476938056276054036,"@DThompsonDev And now my whole Twitter account is dedicated to distributing that knowledge to the world, so maybe I’m addicted to teaching too!"
2969,@marktenenholtz,2021-12-31 15:25:57+00:00,https://twitter.com/marktenenholtz/status/1476937678465781761,"@DThompsonDev I seek competition wherever I can find it.

It’s the fastest way to learn anything, and made me better at machine learning and Python faster than any course I’ve ever taken."
2970,@marktenenholtz,2021-12-31 13:14:23+00:00,https://twitter.com/marktenenholtz/status/1476904568093085697,@tunguz Jax/Flax
2971,@marktenenholtz,2021-12-31 03:47:19+00:00,https://twitter.com/marktenenholtz/status/1476761863023120422,@tdinh_me Started less than a month ago. Trying to unleash the depths of gated machine learning knowledge with tweets like this
2972,@marktenenholtz,2021-12-31 01:27:49+00:00,https://twitter.com/marktenenholtz/status/1476726756002484225,"@LBacaj Congrats, Louie! You’re letting your success speak for itself — as it should be."
2973,@marktenenholtz,2021-12-30 17:24:47+00:00,https://twitter.com/marktenenholtz/status/1476605195316563975,"@LBacaj It truly is crazy. 

The amount of time I’ve spent learning how to use Lambda, DynamoDB, and deployment tools for serverless workflows is some of the most valuable time I’ve spent learning new technical skills."
2974,@marktenenholtz,2021-12-30 16:37:13+00:00,https://twitter.com/marktenenholtz/status/1476593226219991050,"@Nicolascole77 It says a lot about their mindset.

The first type embrace a growth mindset.

The second cling to a zero-sum mindset."
2975,@marktenenholtz,2021-12-30 16:26:47+00:00,https://twitter.com/marktenenholtz/status/1476590601965645830,"For MLOps, one of the most important cost-savings questions you can ask yourself:

""Do I really need a dedicated server for this?""

Switching to a serverless stack can slash your costs, sometimes to nearly nothing."
2976,@marktenenholtz,2021-12-30 14:59:25+00:00,https://twitter.com/marktenenholtz/status/1476568612496461831,Speed up your PyTorch dataloading on the GPU with one extra argument https://t.co/7dEX7ZZtYd
2977,@marktenenholtz,2021-12-30 14:20:28+00:00,https://twitter.com/marktenenholtz/status/1476558812215750664,"@dickiebush It’s not just for the skill.

For me, I did this with programming for a long time. 

While building the habit, you may struggle to get just about anything done.

But by doing it every day, you start to grease your wheels, and productivity comes to you much more easily."
2978,@marktenenholtz,2021-12-30 13:27:35+00:00,https://twitter.com/marktenenholtz/status/1476545503219310592,@sumit0k @Nicolascole77 Guess it works 😀
2979,@marktenenholtz,2021-12-29 18:29:58+00:00,https://twitter.com/marktenenholtz/status/1476259213009723401,@tunguz It just becomes very shallow
2980,@marktenenholtz,2021-12-29 17:11:52+00:00,https://twitter.com/marktenenholtz/status/1476239558513672194,"@Nicolascole77 Lifetime value is the proper KPI, not immediate sales.

Many sellers/creators get fooled by creating products for a broad, unspecific audience.

In reality, each one of those loyal customers has a lifetime value of 100x+ the rest."
2981,@marktenenholtz,2021-12-29 13:58:48+00:00,https://twitter.com/marktenenholtz/status/1476190971331235856,"@LBacaj It’s a good strategy, but I try to use it sparingly.

If I trust the other person, I try to make the promise as accurate as I can."
2982,@marktenenholtz,2021-12-29 13:55:10+00:00,https://twitter.com/marktenenholtz/status/1476190056847220738,@tunguz Those never make it more than two days in my household
2983,@marktenenholtz,2021-12-29 13:26:11+00:00,https://twitter.com/marktenenholtz/status/1476182762994470917,The simplest algorithms have the most incredible results https://t.co/yyLPb6Imdo
2984,@marktenenholtz,2021-12-29 13:02:41+00:00,https://twitter.com/marktenenholtz/status/1476176847436136450,"@kevpacker Yeah this is one of those universal fallacies, I believe."
2985,@marktenenholtz,2021-12-28 22:31:02+00:00,https://twitter.com/marktenenholtz/status/1475957490768617474,"@dvassallo I felt the same way right before I switched fields into data science. 

The best thing you can do is focus on putting yourself on the right path.

The next thing to do is trust the process.

Wash, rinse, repeat."
2986,@marktenenholtz,2021-12-28 22:29:10+00:00,https://twitter.com/marktenenholtz/status/1475957022126342146,"@Jeande_d Becoming a specialist with expert-level knowledge in one of these fields is one of the best ways to make yourself indispensable. 

I wrote a thread about how I did just that:"
2987,@marktenenholtz,2021-12-28 17:05:14+00:00,https://twitter.com/marktenenholtz/status/1475875498844725251,@LBacaj Thanks Louie! You've been a huge inspiration and a wonderful teacher yourself.
2988,@marktenenholtz,2021-12-28 16:45:55+00:00,https://twitter.com/marktenenholtz/status/1475870640007786497,"I’m living all of these tips and continuing to find more. Follow me @marktenenholtz to keep learning from me and my journey.

While I’m just starting and my account is small now, follow along and we will grow together."
2989,@marktenenholtz,2021-12-28 16:45:55+00:00,https://twitter.com/marktenenholtz/status/1475870638694965251,"Deeper meaning: Provide differentiation

Businesses that provide value while differentiating their products succeed. 

Those that do not enter a zero-sum game with cutthroat competition.

Make sure you provide *unique* value."
2990,@marktenenholtz,2021-12-28 16:45:54+00:00,https://twitter.com/marktenenholtz/status/1475870637386350593,"Tip #3: Provide value

The (partially correct) reason usually given:

If you provide value, people will eagerly consume your content and always want more."
2991,@marktenenholtz,2021-12-28 16:45:54+00:00,https://twitter.com/marktenenholtz/status/1475870636098883591,"Deeper meaning #2.2: Proof

If you provide value one time, there's no reason to follow you. The consumer got all that there is to have.

Consistency creates proof, proof creates FOMO. They must either follow you or risk missing out on your future value creation."
2992,@marktenenholtz,2021-12-28 16:45:54+00:00,https://twitter.com/marktenenholtz/status/1475870634425171975,"Deeper meaning #2.1: Practice

I can guarantee you need to get better at writing. 

The level of skill it takes to write well on twitter is enormously underappreciated. Doing it every day refines those skills."
2993,@marktenenholtz,2021-12-28 16:45:53+00:00,https://twitter.com/marktenenholtz/status/1475870633183559688,"Tip #2: Be consistent

The (10% correct) reason usually given:

Tweet every day. The algorithm favors those who produce content, and if you stay consistent, you're more likely to have a tweet that blows up."
2994,@marktenenholtz,2021-12-28 16:45:53+00:00,https://twitter.com/marktenenholtz/status/1475870631858221056,"Deeper meaning #1.2: Construct a social network

I've met people here that I would have never met otherwise.

The engagement you get from your social network not only helps you get off the ground, but also gives you and your network invaluable feedback for improvement."
2995,@marktenenholtz,2021-12-28 16:45:53+00:00,https://twitter.com/marktenenholtz/status/1475870630654484485,"But, what does it mean to provide value? Well, that's point #3."
2996,@marktenenholtz,2021-12-28 16:45:53+00:00,https://twitter.com/marktenenholtz/status/1475870629404549120,"When you're small, you can see what works for you by adding value in replies to bigger accounts.

If your content gets engagement there, you've found a fit."
2997,@marktenenholtz,2021-12-28 16:45:52+00:00,https://twitter.com/marktenenholtz/status/1475870628133675011,"Deeper meaning #1.1: Engaging will help you find content-market fit

The only way anyone will care what you're saying is by creating content that's in demand.

It goes for consumer goods the same way it does for content."
2998,@marktenenholtz,2021-12-28 16:45:52+00:00,https://twitter.com/marktenenholtz/status/1475870626665619456,"Tip #1: Engage with others

The (Partially correct) reason usually given:

Nobody is looking at your timeline, so increase your reach by replying to bigger accounts."
2999,@marktenenholtz,2021-12-28 16:45:52+00:00,https://twitter.com/marktenenholtz/status/1475870625239609350,"I tripled my Twitter following in 20 days.

Here are 3 of the most important tips I picked up along the way, and the deeper insights hidden underneath the surface🧵"
3000,@marktenenholtz,2021-12-28 13:31:00+00:00,https://twitter.com/marktenenholtz/status/1475821586338889731,"Don't like how columns are named after doing a pandas agg? Here's some syntax that isn't commonly known for a far better experience.

This improved my pandas pipelines tremendously. https://t.co/ib6l6VmUQD"
3001,@marktenenholtz,2021-12-27 20:29:42+00:00,https://twitter.com/marktenenholtz/status/1475564567602343938,"@dvassallo I would argue preparation is more important than execution.

Solid execution and solid timing are essentially guaranteed with continual preparation."
3002,@marktenenholtz,2021-12-27 20:01:54+00:00,https://twitter.com/marktenenholtz/status/1475557573441081351,@themodev @omarsar0 modev is a recommended follow of mine for sure!
3003,@marktenenholtz,2021-12-27 20:00:55+00:00,https://twitter.com/marktenenholtz/status/1475557325968756743,@omarsar0 Hello! I’ve been posting tips like this pretty regularly and I plan on doing a lot more writing about machine learning/deep learning knowledge that’s hard to find! https://t.co/Asp25w1OWv
3004,@marktenenholtz,2021-12-27 18:26:32+00:00,https://twitter.com/marktenenholtz/status/1475533572396273665,"@fchollet And this is why I always use LightGBM over XGBoost or resnet18 over resnet50 until I’ve done a lot of iterating.

Perfecting your pipeline is the most essential thing, and faster models give you the best opportunity to improve it."
3005,@marktenenholtz,2021-12-27 18:23:20+00:00,https://twitter.com/marktenenholtz/status/1475532766125166593,"@fchollet It’s much easier to see how to improve specific parts when you’ve at least made a rough, but complete product.

The ways the components interact are far more clear, allowing you to do the next most important thing: refactor your code to something more suitable."
3006,@marktenenholtz,2021-12-27 14:58:10+00:00,https://twitter.com/marktenenholtz/status/1475481136629440513,"My favorite way of ensembling models:

Stacking can be computationally intensive and produce inconsistent results. Instead, use a linear optimizer to assign a weight to each model in your ensemble.

Here, I optimize MSE, but you can use any sklearn metric! https://t.co/87ThOkqvlQ"
3007,@marktenenholtz,2021-12-27 13:01:38+00:00,https://twitter.com/marktenenholtz/status/1475451807979786247,@svpino I constantly find myself smiling when I use it. The completions it comes up with are truly incredible.
3008,@marktenenholtz,2021-12-27 13:00:26+00:00,https://twitter.com/marktenenholtz/status/1475451509114642433,@PrasoonPratham In node we code?
3009,@marktenenholtz,2021-12-26 19:18:39+00:00,https://twitter.com/marktenenholtz/status/1475184302396739585,@themodev Thanks for the support!
3010,@marktenenholtz,2021-12-26 18:14:49+00:00,https://twitter.com/marktenenholtz/status/1475168236908027904,"I hope you enjoyed this fun bit of ML history. My goal for 2022 is to keep bringing you well-formulated ML knowledge and stories that are hard to find. 

Join me in learning by following me @marktenenholtz"
3011,@marktenenholtz,2021-12-26 18:14:49+00:00,https://twitter.com/marktenenholtz/status/1475168234752159749,"As it turned out, allowing the model to essentially learn nothing at a particular layer was the difference between worse performance and state-of-the-art.

This is what defined the architecture that we know today as ResNet. Six years in the future, it's still a top performer."
3012,@marktenenholtz,2021-12-26 18:14:47+00:00,https://twitter.com/marktenenholtz/status/1475168227982458880,"Contrary to expectation, they didn't overfit to the training data... they underfit!

Since the model wasn't able to use the deepest layers, they decided to give it the ability to short-circuit itself with skip connections.

If the layer doesn't help, just act like it's not there. https://t.co/Yhrn8rsBSq"
3013,@marktenenholtz,2021-12-26 18:14:43+00:00,https://twitter.com/marktenenholtz/status/1475168212111269888,"So, they tried it. 

An easy assumption may be that they saw training errors dipping to zero as the increase in parameters allowed a tighter and tighter fit. Regularization could come later.

Instead, shockingly, the more layers they added, the WORSE the training error got! https://t.co/OE58ECupnM"
3014,@marktenenholtz,2021-12-26 18:14:41+00:00,https://twitter.com/marktenenholtz/status/1475168201982058503,"AlexNet used some new tricks, like ReLU, local normalization, and overlapping pooling.

Arguably, though, the most notable change was that it was flat-out bigger. Was a deeper network all we needed?

Three years later, enter He et al. with a simple question: https://t.co/JBRLqF2Vnw"
3015,@marktenenholtz,2021-12-26 18:14:37+00:00,https://twitter.com/marktenenholtz/status/1475168186941284358,"Before there were CNNs, researchers relied on hand-crafted features combined with models such as support vector machines.

The CNN revolution began with architectures like LeNet and AlexNet. Below you can see the architectures that began to change the landscape. https://t.co/NwwqxscivD"
3016,@marktenenholtz,2021-12-26 18:14:33+00:00,https://twitter.com/marktenenholtz/status/1475168169211871241,"Convolutional neural networks reshaped the world of computer vision.

However, they would be a shadow of their current performance if Kaiming He et al. hadn't noticed some bizarre behavior.

How a head-scratching moment changed the way computers see 🧵"
3017,@marktenenholtz,2021-12-26 15:02:00+00:00,https://twitter.com/marktenenholtz/status/1475119711599796224,"me: new billion param language model just dropped

my gpu: https://t.co/k8d95c3ND0"
3018,@marktenenholtz,2021-12-26 13:11:51+00:00,https://twitter.com/marktenenholtz/status/1475091992350765057,"@svpino The problem we have in tech is that the field moves so quickly that later entrants to the field may have more advanced knowledge by virtue of when they learned the material.

The best way to be valued as a senior employee is to always be hungry for those new advances."
3019,@marktenenholtz,2021-12-26 03:47:31+00:00,https://twitter.com/marktenenholtz/status/1474949975104753664,@LBacaj I think that thread will be coming tomorrow :)
3020,@marktenenholtz,2021-12-26 03:43:28+00:00,https://twitter.com/marktenenholtz/status/1474948954601230341,"@LBacaj I’ve spent a lot of time on product-market fit. My content is based on what I see work on other platforms plus DMing new followers.

That, and adding useful replies to larger accounts."
3021,@marktenenholtz,2021-12-26 03:39:33+00:00,https://twitter.com/marktenenholtz/status/1474947970307088385,"@LBacaj Yeah it’s just a distribution with extremely high variance. The trick is that one time doesn’t do it — you need to have good outcomes many times which is a sign of a good process.

I just recently came across them. I need to dig in to what they have because it seems high quality."
3022,@marktenenholtz,2021-12-26 03:32:29+00:00,https://twitter.com/marktenenholtz/status/1474946187887271941,"@LBacaj Sure, I agree. My point is moreso that so many interactions occur that the “luck” you need is basically bound to happen at some point. For some folks it may be one day, others a year, etc."
3023,@marktenenholtz,2021-12-26 03:29:55+00:00,https://twitter.com/marktenenholtz/status/1474945543906500614,"@LBacaj I mean look at me, I grew 140+ followers overnight to more than double my follower count. But I believe it’s because I’ve started to put enough good content out."
3024,@marktenenholtz,2021-12-26 03:28:59+00:00,https://twitter.com/marktenenholtz/status/1474945307519684609,"@LBacaj I think the timing is luck, but the process is almost always not"
3025,@marktenenholtz,2021-12-26 03:28:17+00:00,https://twitter.com/marktenenholtz/status/1474945131040104450,@burkov Do you think most universities would be willing to make the massive cuts to research budgets that they would need to make if they got rid of undergraduates?
3026,@marktenenholtz,2021-12-26 03:24:27+00:00,https://twitter.com/marktenenholtz/status/1474944168380317699,"@LBacaj Very true. There’s also many accounts that get huge that seem generic. My hypothesis is that in those cases, it’s impossible for a mere observer to truly understand what makes that account special without long term observation.

This extends to e-commerce as a whole, honestly"
3027,@marktenenholtz,2021-12-26 03:19:44+00:00,https://twitter.com/marktenenholtz/status/1474942979718750221,@LBacaj For growing an audience online: what most think is luck is actually large scale social complexity
3028,@marktenenholtz,2021-12-25 23:11:55+00:00,https://twitter.com/marktenenholtz/status/1474880616416092168,"@mervenoyann You’re 100% right. Everything you can do in torch you can do in TF. Keras is even a lot more flexible than folks think, but we have a major deficiency of intermediate+ level content in the ML community. That’s the need I’m trying to address with my Twitter account."
3029,@marktenenholtz,2021-12-25 23:04:43+00:00,https://twitter.com/marktenenholtz/status/1474878803671457792,"@mervenoyann Like I said, TF is great. But I would rather have libraries like timm and all the newest research than things like TF lite. Those who just say “TF sucks” are ignorant.

I love that we have both frameworks because they both drive innovation in different ways."
3030,@marktenenholtz,2021-12-25 21:34:02+00:00,https://twitter.com/marktenenholtz/status/1474855982941548544,"@mervenoyann Fair enough for point 1. For point 3, a lot more research and pretrained models are implemented in Torch"
3031,@marktenenholtz,2021-12-25 19:21:07+00:00,https://twitter.com/marktenenholtz/status/1474822533459365891,"@mervenoyann Here’s perspective from a torch “power user”. My 3 critiques of TF:

1. There are too many ways to do the same thing.
2. It’s obvious TF wasn’t originally built for eager execution under the hood.
3. Ecosystem is worse

Other than that, it’s a great library"
3032,@marktenenholtz,2021-12-25 19:08:05+00:00,https://twitter.com/marktenenholtz/status/1474819253492723713,@svpino A good extension into deep learning after one learns gradient boosting is focal loss
3033,@marktenenholtz,2021-12-25 13:04:21+00:00,https://twitter.com/marktenenholtz/status/1474727718105325569,"Here's ""a house decorated for christmas"" according to the DALL-E mini model. I did this on @huggingface spaces.

Merry Christmas! https://t.co/vnBD4TAwtT"
3034,@marktenenholtz,2021-12-24 21:59:24+00:00,https://twitter.com/marktenenholtz/status/1474499977493762048,@abhi74k @svpino Thanks! Let me know if there’s anything you’d like to see!
3035,@marktenenholtz,2021-12-24 19:18:01+00:00,https://twitter.com/marktenenholtz/status/1474459367344910339,"@tunguz It’s a double edged sword, though. Sample too uneven and some models will be predicting entirely out of distribution and perform quite poorly. 

This is something I have to contend a lot with since I work with time series data all day."
3036,@marktenenholtz,2021-12-24 19:14:01+00:00,https://twitter.com/marktenenholtz/status/1474458358505783304,"@tunguz For inference, you’re definitely right. However, it can be hard to get a great estimation of model performance if you have a big variation in error between your folds. 

Despite that as you are saying, a diverse ensemble is the best ensemble."
3037,@marktenenholtz,2021-12-24 19:03:50+00:00,https://twitter.com/marktenenholtz/status/1474455797711179792,@svpino Hello! I’ve been posting tips like this pretty regularly and I plan on doing a lot more writing about machine learning knowledge that’s hard to find!
3038,@marktenenholtz,2021-12-24 14:23:01+00:00,https://twitter.com/marktenenholtz/status/1474385126075379715,@TheZachMueller @huggingface Now THIS is a model zoo
3039,@marktenenholtz,2021-12-24 14:19:35+00:00,https://twitter.com/marktenenholtz/status/1474384261314748432,@Kaszanas I’d probably get a call from my therapist if we did that
3040,@marktenenholtz,2021-12-24 14:09:18+00:00,https://twitter.com/marktenenholtz/status/1474381672820916267,"Another PyTorch efficiency tip:

Use torch.jit.script() to fuse pointwise operations in your deep learning models. This means, instead of spinning up a kernel for each operation, torch can create one kernel for the whole thing. https://t.co/a9rRvidMwA"
3041,@marktenenholtz,2021-12-23 23:52:43+00:00,https://twitter.com/marktenenholtz/status/1474166107737149445,@sama We value integration. Stores of value separate value. Web3 integrates it.
3042,@marktenenholtz,2021-12-23 20:04:32+00:00,https://twitter.com/marktenenholtz/status/1474108685001572360,"I haven't improved as a programmer so much as I've improved as a StackOverflow retrieval algorithm.

@huggingface should put me in their model hub."
3043,@marktenenholtz,2021-12-23 19:01:24+00:00,https://twitter.com/marktenenholtz/status/1474092794004316167,"@svpino @freeCodeCamp Even if you never use the SQL language, the language’s patterns are useful for any data processing package!"
3044,@marktenenholtz,2021-12-23 17:41:29+00:00,https://twitter.com/marktenenholtz/status/1474072684753567751,@tunguz Surprising to me to see how many folks think Agile is what created Kanban
3045,@marktenenholtz,2021-12-23 15:40:26+00:00,https://twitter.com/marktenenholtz/status/1474042221443928070,"@tunguz Ah yes, I call this DeepScream"
3046,@marktenenholtz,2021-12-23 15:32:00+00:00,https://twitter.com/marktenenholtz/status/1474040097766981634,"When writing code, I try to document it by leaving helpful comments. For instance, the amount of agony I was in at the time."
3047,@marktenenholtz,2021-12-23 14:23:51+00:00,https://twitter.com/marktenenholtz/status/1474022947983867904,@jackbutcher It’s also the only way to reciprocate it
3048,@marktenenholtz,2021-12-23 13:52:11+00:00,https://twitter.com/marktenenholtz/status/1474014980123115527,"Want fast Python code? Don't use NumPy's append method.

The built-in list append is way faster (~100x in this test). Even np.concatenate is significantly faster (~2x in this test). https://t.co/6QiGGq3zwQ"
3049,@marktenenholtz,2021-12-22 22:09:40+00:00,https://twitter.com/marktenenholtz/status/1473777787454504962,@themodev Thanks for the support! You’ve been an inspiration to me as well.
3050,@marktenenholtz,2021-12-22 21:36:08+00:00,https://twitter.com/marktenenholtz/status/1473769347759935492,@LBacaj Thanks Louie! Still figuring out the best ways of getting value out there and you’ve been a great inspiration.
3051,@marktenenholtz,2021-12-22 21:09:34+00:00,https://twitter.com/marktenenholtz/status/1473762660948791301,"Thanks to @LBacaj for writing the below thread that inspired me to write my own. You should check it out too, b/c it's a perfect complement. 

Follow me @marktenenholtz and let's climb aboard the rocket ship together in 2022.

https://t.co/MD1Kqx49LL"
3052,@marktenenholtz,2021-12-22 21:09:33+00:00,https://twitter.com/marktenenholtz/status/1473762659023691784,"So now, I'm hoping to give some of this back. My goal is to make it easier for those who were in my shoes to do what I am doing by giving them the information and skills I wish I had."
3053,@marktenenholtz,2021-12-22 21:09:33+00:00,https://twitter.com/marktenenholtz/status/1473762656049848320,"I went from no machine learning experience to building the forecasting models used by one of the USA's largest retailers.

I get to work alongside some of the smartest researchers I've ever met and absorb their wisdom because I brought something new to the table."
3054,@marktenenholtz,2021-12-22 21:09:27+00:00,https://twitter.com/marktenenholtz/status/1473762630888300557,"So, I immersed myself. What did I find?

""I took the one less traveled by,
And that has made all the difference."""
3055,@marktenenholtz,2021-12-22 21:09:25+00:00,https://twitter.com/marktenenholtz/status/1473762622491213824,"Along the way, I found the @fastai course, the (almost too) vibrant research community, and @Kaggle competitions.

In these resources, I found a hidden gold mine of material that you couldn't find anywhere else on the internet. I found the place where progress was being made."
3056,@marktenenholtz,2021-12-22 21:09:23+00:00,https://twitter.com/marktenenholtz/status/1473762614018809863,"So, I decided to pursue quality #2. I wanted to find the new stuff that was going to change the field. Rather than focusing too much time on the canonical texts, I looked for the material that was on the cutting edge."
3057,@marktenenholtz,2021-12-22 21:09:22+00:00,https://twitter.com/marktenenholtz/status/1473762612093628422,"I already had a good claim on quality #1: I had strong experience with C++ and Python, and had a solid background in math and technical problem solving."
3058,@marktenenholtz,2021-12-22 21:09:20+00:00,https://twitter.com/marktenenholtz/status/1473762603503697928,"The problem with canonical texts, though, is that when the field advances quickly, everyone is learning outdated material. @tunguz wrote about this here: https://t.co/Usy8KUR8vx"
3059,@marktenenholtz,2021-12-22 21:09:18+00:00,https://twitter.com/marktenenholtz/status/1473762595161223182,"Most fields have a few canonical resources that almost every professional has learned from, usually some sort of textbook.

For machine learning, I found it was resources like Elements of Statistical Learning or Hands-On Machine Learning with Scikit-Learn and TensorFlow."
3060,@marktenenholtz,2021-12-22 21:09:16+00:00,https://twitter.com/marktenenholtz/status/1473762586411819011,"Sometimes, differentiation comes from having an incredibly high skill level at something important. Other times, it may just be having a unique or uncommon capability in a niche part of your field."
3061,@marktenenholtz,2021-12-22 21:09:15+00:00,https://twitter.com/marktenenholtz/status/1473762583534571524,"The most important qualities to have that give you value as a founder or employee are:

1) Having the core skills/aptitude to succeed in your field
2) Having a differentiated skillset that sets you apart"
3062,@marktenenholtz,2021-12-22 21:09:15+00:00,https://twitter.com/marktenenholtz/status/1473762581030576146,"How I entered a new field with almost no experience and quickly made myself valuable

I studied mechanical engineering at a top college, but had lost passion. I stumbled upon machine learning and in 6 months went from zero knowledge to a valuable asset.

🧵Here are my secrets👇"
3063,@marktenenholtz,2021-12-22 19:04:48+00:00,https://twitter.com/marktenenholtz/status/1473731263362129935,"@svpino Is ""import *"" what being a full stack developer is like?"
3064,@marktenenholtz,2021-12-22 19:00:49+00:00,https://twitter.com/marktenenholtz/status/1473730262538280968,@svpino Why have some when you can have it all?
3065,@marktenenholtz,2021-12-22 14:31:32+00:00,https://twitter.com/marktenenholtz/status/1473662495801982978,@ykilcher @GaryMarcus https://t.co/52nnkkJ5mQ
3066,@marktenenholtz,2021-12-22 13:30:41+00:00,https://twitter.com/marktenenholtz/status/1473647182347706374,"@svpino At no point in my life will I ever be able to remember the specific definitions of sensitivity, specificity, precision, and recall"
3067,@marktenenholtz,2021-12-22 13:21:33+00:00,https://twitter.com/marktenenholtz/status/1473644883554582528,"@themodev Great question. I actually learned TensorFlow first but I just prefer how torch feels. There’s also a far better ecosystem of pretrained models and new research.

Maybe I should do some TensorFlow tips too, though :)"
3068,@marktenenholtz,2021-12-22 13:03:13+00:00,https://twitter.com/marktenenholtz/status/1473640267534249991,"Improve the extracted features from your deep learning vision models

GeM (generalized mean) is a trainable pooling function that lets your network optimize a mix between both mean and max pooling. It's been used successfully in computer vision tasks. https://t.co/NTQTiktBkS"
3069,@marktenenholtz,2021-12-22 04:12:15+00:00,https://twitter.com/marktenenholtz/status/1473506648064933888,"@LBacaj @dvassallo I've started to post a lot of ML/Python tips and I struggle to post things that I feel are too basic.

What I forget is all the beginner content I've been helped by in niches outside of my specialty.

Most of the time I spend creating these tips is getting over this mentality."
3070,@marktenenholtz,2021-12-22 03:39:08+00:00,https://twitter.com/marktenenholtz/status/1473498312321941509,@burkov One day we’ll have AI that does our SEO for us. That way we’ll have machines tailoring the web to machines.
3071,@marktenenholtz,2021-12-21 21:15:37+00:00,https://twitter.com/marktenenholtz/status/1473401796936802306,@PrasoonPratham I have a windows/Ubuntu dual boot desktop sitting next to 2 Macs and an iPad 😬
3072,@marktenenholtz,2021-12-21 21:13:50+00:00,https://twitter.com/marktenenholtz/status/1473401349660454913,"@catalinmpit Enjoyment is a matter of perspective.

When you take on a challenge, you can choose to be stressed when you hit a roadblock, or excited that you’ll learn a new way to defeat that problem."
3073,@marktenenholtz,2021-12-21 21:03:30+00:00,https://twitter.com/marktenenholtz/status/1473398745635201034,@tunguz The joke here is that the wife is still in the picture
3074,@marktenenholtz,2021-12-21 20:34:54+00:00,https://twitter.com/marktenenholtz/status/1473391551992799248,@lexfridman This is why I train conversational NLP models to tell me that they love me
3075,@marktenenholtz,2021-12-21 13:11:09+00:00,https://twitter.com/marktenenholtz/status/1473279878082576389,"Need to split your data for a multilabel classification problem?

The iterstrat package has it covered for you. This works just like scikit-learn's StratifiedKFold. https://t.co/TOisYtInrq"
3076,@marktenenholtz,2021-12-20 22:30:41+00:00,https://twitter.com/marktenenholtz/status/1473058301122392070,@tunguz Make a big pot of chili!
3077,@marktenenholtz,2021-12-20 22:29:43+00:00,https://twitter.com/marktenenholtz/status/1473058055709466639,"@TheZachMueller Excel spreadsheets on my left
Apartments on my right 

🎵stuck in the middle with you 🎵"
3078,@marktenenholtz,2021-12-20 21:43:51+00:00,https://twitter.com/marktenenholtz/status/1473046515539222541,"@TobiOlabode3 Yep! Between each training step, you have to call .zero_grad(). 

This is because you may want to accumulate gradients over a few batches before your backward pass if you have huge input tensors, in which case you’d call .zero_grad() every few steps (gradient accumulation)."
3079,@marktenenholtz,2021-12-20 21:32:34+00:00,https://twitter.com/marktenenholtz/status/1473043672711286795,@tunguz A classic price elasticity experiment
3080,@marktenenholtz,2021-12-20 21:04:11+00:00,https://twitter.com/marktenenholtz/status/1473036531225042945,"@fchollet One of the most incredible things in deep learning research is when a highly-engineered solution is replaced with a simple, elegant, and more representative end-to-end model.

Simplicity is beautiful."
3081,@marktenenholtz,2021-12-20 17:17:45+00:00,https://twitter.com/marktenenholtz/status/1472979547712860172,"Typo: I meant to write that is more memory efficient, not faster"
3082,@marktenenholtz,2021-12-20 17:17:10+00:00,https://twitter.com/marktenenholtz/status/1472979399813312515,"@anbarbazan @ThomasViehmann I didn't realize the typo until now, but I should have said ""more memory efficient"" rather than ""faster"". Check here:  https://t.co/zzip2OyRyu"
3083,@marktenenholtz,2021-12-20 15:58:45+00:00,https://twitter.com/marktenenholtz/status/1472959666305085454,@DThompsonDev Me when I got up late and haven't had my coffee yet
3084,@marktenenholtz,2021-12-20 14:41:47+00:00,https://twitter.com/marktenenholtz/status/1472940299152408576,"@themodev You’re right. Comfort with the wrong tools sets you on a bad trajectory, not just a bad position."
3085,@marktenenholtz,2021-12-20 14:17:00+00:00,https://twitter.com/marktenenholtz/status/1472934059504328704,There's no more permanent solution than a functional temporary solution
3086,@marktenenholtz,2021-12-20 13:40:44+00:00,https://twitter.com/marktenenholtz/status/1472924934896824325,"@svpino Also, here’s a tip you can use for preventing a bad split on regression data:"
3087,@marktenenholtz,2021-12-20 13:34:50+00:00,https://twitter.com/marktenenholtz/status/1472923449115004938,"@svpino I’m certainly nitpicking, though. Thanks for the chat"
3088,@marktenenholtz,2021-12-20 13:28:45+00:00,https://twitter.com/marktenenholtz/status/1472921918709846016,@svpino Overfitting is a sort of special case of a lack of generalization. A lack of generalization can also mean that the model is actually didn’t see a representative sample of real world data (second case). But it hasn’t necessarily begun to overfit. They require diff treatment
3089,@marktenenholtz,2021-12-20 13:25:10+00:00,https://twitter.com/marktenenholtz/status/1472921015781101579,"@svpino I probably shouldn’t have used that word, you’re right.

In overfitting, you’re fitting to patterns that don’t exist in the test set (or probably the real world).

In the other case, it could just be that your data is poorly split and the test case is harder.

(1/2)"
3090,@marktenenholtz,2021-12-20 13:11:55+00:00,https://twitter.com/marktenenholtz/status/1472917683742158848,"@svpino That really just means your model isn’t generalizing very well, which is slightly different than overfitting. Overfitting is fitting to patterns that don’t exist, bad generalization is not finding the best patterns."
3091,@marktenenholtz,2021-12-20 13:07:57+00:00,https://twitter.com/marktenenholtz/status/1472916681525809154,@svpino You’re actually only overfitting if your test loss begins to get worse
3092,@marktenenholtz,2021-12-20 13:03:18+00:00,https://twitter.com/marktenenholtz/status/1472915515358302209,Credit to @abhi1thakur for the inspiration!
3093,@marktenenholtz,2021-12-20 13:03:18+00:00,https://twitter.com/marktenenholtz/status/1472915511994425346,"Improve your cross-validation for regression tasks!

A regular random split doesn't guarantee all of your folds see a realistic sample of your data.

If you bin your target like a histogram and stratify on those bins, you can guarantee an even split! https://t.co/U2PKhUNIZm"
3094,@marktenenholtz,2021-12-19 22:13:45+00:00,https://twitter.com/marktenenholtz/status/1472691650824753159,@merl_syrex @omarsar0 @rasbt Your best bet is probably Papers With Code. I also plan on posting more threads on papers like I did here:
3095,@marktenenholtz,2021-12-19 17:13:11+00:00,https://twitter.com/marktenenholtz/status/1472616012109565962,@WalterReade This is like the alternate universe version of Hogwarts. It's the school of Nefariousness and Necromancy.
3096,@marktenenholtz,2021-12-19 16:52:18+00:00,https://twitter.com/marktenenholtz/status/1472610757204340738,"In Star Wars: “set phasers to stun”

In PyTorch: “set grads to None”"
3097,@marktenenholtz,2021-12-19 16:12:43+00:00,https://twitter.com/marktenenholtz/status/1472600791819112449,"@LBacaj When you’ve already learned something, it seems trivial to you. It’s certainly not trivial to someone who hasn’t had the same experiences as you, though. 

Those things are definitely worth producing content around!"
3098,@marktenenholtz,2021-12-19 15:19:52+00:00,https://twitter.com/marktenenholtz/status/1472587495229505543,"@chrishlad This is why I always try to get as much done in the morning as possible. 

There’s no better feeling than accomplishing a lot and then still having most of the day still ahead of you."
3099,@marktenenholtz,2021-12-19 13:05:11+00:00,https://twitter.com/marktenenholtz/status/1472553599439970304,"Speed up your PyTorch code!

Almost every piece of documentation and tutorial uses .zero_grad() when it's faster to set the gradients to None. https://t.co/FojOx3Ytpi"
3100,@marktenenholtz,2021-12-19 12:40:48+00:00,https://twitter.com/marktenenholtz/status/1472547462254673922,@omarsar0 A good amount of kaggle experience is big for a candidate imo. It shows they know how to implement and correctly validate high-performing models better than 98% of personal projects do.
3101,@marktenenholtz,2021-12-18 23:37:27+00:00,https://twitter.com/marktenenholtz/status/1472350326896443393,"@BethCarey12 @GaryMarcus Sure but we don’t have anything that tells us that the question has been answered. Instead, we have models that confidently return BS. This is an interesting first step:"
3102,@marktenenholtz,2021-12-18 22:26:57+00:00,https://twitter.com/marktenenholtz/status/1472332583677612041,"@GaryMarcus From my experience, there are two areas we need to improve to better scale logical reasoning tasks:

1. engineering surrounding the model, i.e. pairing a model with a knowledge bank in a better way
2. models that can tell the user when they don’t know the answer"
3103,@marktenenholtz,2021-12-18 19:33:04+00:00,https://twitter.com/marktenenholtz/status/1472288826341138437,"@karpathy Considering transformers are the most generalized architecture in common use, it makes sense. 

If you want AGI, you need a general model!"
3104,@marktenenholtz,2021-12-18 19:02:52+00:00,https://twitter.com/marktenenholtz/status/1472281226564284418,"@svpino I do something like this as well, and it's great to check off tasks from your list of things to accomplish that day.

Not only is it satisfying, but you can get a really good idea of how much you can do in a day. Helps a lot when estimating how long a new feature will take."
3105,@marktenenholtz,2021-12-18 15:49:25+00:00,https://twitter.com/marktenenholtz/status/1472232543529443340,@TheZachMueller Friend of mine did this and it was baaaad news... but best of luck!
3106,@marktenenholtz,2021-12-18 14:35:32+00:00,https://twitter.com/marktenenholtz/status/1472213948187484166,"Quick machine learning tip:

Want to fit models in parallel? Here's some quick Python code to do it.

I use Prophet, but you can use any model (or thread-safe function!) that isn't using enough of your CPU. https://t.co/7CpVNo3fMV"
3107,@marktenenholtz,2021-12-17 20:40:13+00:00,https://twitter.com/marktenenholtz/status/1471943338135801858,"@ishandutta0098 @kaggle That is great. 

I know that at least most data science interviewers I’ve talked to are impressed by kaggle experience these days, even if not mentioned in the posting."
3108,@marktenenholtz,2021-12-17 20:03:28+00:00,https://twitter.com/marktenenholtz/status/1471934088068186114,"@omarsar0 You’ll also never have any idea if your models are good or bad if you don’t understand the data and the best way of setting up a validation scheme.

It’s pointless to train a model if you can’t know how good it is!"
3109,@marktenenholtz,2021-12-17 17:05:12+00:00,https://twitter.com/marktenenholtz/status/1471889225201205254,"@TGurgick @svpino Yeah, it’s a classic case of putting the cart before the horse. Problems drive solutions, not the other way around."
3110,@marktenenholtz,2021-12-17 15:38:02+00:00,https://twitter.com/marktenenholtz/status/1471867290773770247,"@svpino Same thing goes in data science with deep learning. 

There will always be space for other model types and methodologies."
3111,@marktenenholtz,2021-12-17 15:08:20+00:00,https://twitter.com/marktenenholtz/status/1471859816310132745,"@radekosmulski That’s an awesome setup! Where do you take it with you? 

At that point I’d consider setting up SSH permissions for myself rather than hauling it."
3112,@marktenenholtz,2021-12-17 15:05:05+00:00,https://twitter.com/marktenenholtz/status/1471858997477232643,@IttaiSvidler Does this count? It’s a set of tools for web3 devs https://t.co/WsET6Cro1U
3113,@marktenenholtz,2021-12-16 21:16:12+00:00,https://twitter.com/marktenenholtz/status/1471590005827158017,"@DThompsonDev I was a mechanical engineer in college that couldn’t have named a single machine learning model until 2 months before my job interview.

Now I’m building forecasting models for one of the largest grocers in the world.

Never stop learning, never stop trying!"
3114,@marktenenholtz,2021-12-16 20:22:14+00:00,https://twitter.com/marktenenholtz/status/1471576424263954440,@omarsar0 Here’s one you can check out: https://t.co/JsqwGUFjPE
3115,@marktenenholtz,2021-12-16 19:50:41+00:00,https://twitter.com/marktenenholtz/status/1471568485234925575,"Finally, the images in this thread came from this wonderful paper: https://t.co/7mBihjF6S0.

If you thought this was interesting, check it out and you'll learn even more."
3116,@marktenenholtz,2021-12-16 19:50:41+00:00,https://twitter.com/marktenenholtz/status/1471568483825635332,"8/ I hope you enjoyed this quick trip inside a CNN. 

I plan on threads of this format for other topics, so follow for more and let me know what else you want to see."
3117,@marktenenholtz,2021-12-16 19:50:41+00:00,https://twitter.com/marktenenholtz/status/1471568482076663816,"7/ The final layer — the most refined of them all. We have everything from faces, lattice shapes, text, logos, and eyeballs. At this point, not only can the model identify complex shapes, but it can even pick up on the minute details that could distinguish dog breeds. https://t.co/3Qq5VFcjJ3"
3118,@marktenenholtz,2021-12-16 19:50:40+00:00,https://twitter.com/marktenenholtz/status/1471568478318563335,"6/ First, we were looking for lines. Then, combos of lines. Then, shapes made up of those combos. Those simpler features are so representative that we can identify even the most complex patterns. We can highlight the fine details of a dog's face and the legs of a bird. https://t.co/7eq7ovc3Cl"
3119,@marktenenholtz,2021-12-16 19:50:39+00:00,https://twitter.com/marktenenholtz/status/1471568474115817475,"5/ Now, the rubber meets the road. We're now really good at picking up patterns of lines. After all, the patterns you see here are just combos of what we were looking for in layer 2! We also are starting to see some super fine detail (bot right). https://t.co/7rp1iLV71T"
3120,@marktenenholtz,2021-12-16 19:50:38+00:00,https://twitter.com/marktenenholtz/status/1471568469053386761,"4/ To make this a bit more concrete, look at the images the model was looking at when it picked up on those patterns. Windows and door frames... that makes sense! Look at the corners! You can even line the features in the previous tweets up with this image. https://t.co/qS7LVQ9vu7"
3121,@marktenenholtz,2021-12-16 19:50:37+00:00,https://twitter.com/marktenenholtz/status/1471568465710526470,"3/ Layer 2 is a little fancier! Remember when we were just looking for lines? Now, we're looking for a combo of lines — a corner (bot right)!

We're also looking for curves and circles (upper middle).

This is what people mean when they say CNN's learn ""hierarchical"" features. https://t.co/1P0jDKBvuv"
3122,@marktenenholtz,2021-12-16 19:50:36+00:00,https://twitter.com/marktenenholtz/status/1471568461629468693,"2/ Let's start where the image enters — the first layer.

The image you see shows some of the patterns the model looks for. Some of these are looking for lines at various angles (top left, top center, mid). Others (bot right) show the are for simple color gradients. https://t.co/8maCvMJ0Gt"
3123,@marktenenholtz,2021-12-16 19:50:35+00:00,https://twitter.com/marktenenholtz/status/1471568458378842112,"1/ CNN's operate using two principles:

1. Look for patterns in small groups of pixels
2. Use simple patterns as building blocks for more complicated patterns

While you read, try to relate it to how you see objects. What shapes and patterns do you look for in objects?"
3124,@marktenenholtz,2021-12-16 19:50:35+00:00,https://twitter.com/marktenenholtz/status/1471568456763981827,"THREAD: What the heck is going on in a convolutional neural network (CNN)? Maybe you learned what it is in a machine learning class, maybe you've just heard of them.

But you probably haven't *visually* seen what the model learns. 

Let's take a visual journey inside a CNN 👇"
3125,@marktenenholtz,2021-12-16 19:01:25+00:00,https://twitter.com/marktenenholtz/status/1471556084427075590,@svpino The singularity occurs when these hype trains collide
3126,@marktenenholtz,2021-12-16 17:40:36+00:00,https://twitter.com/marktenenholtz/status/1471535746540617734,"@sama This is a wonderful marriage between abstractive and extractive question answering. 

Plus, it makes the answers a lot more usable if citations are included. Almost like the blurb you see on Google search."
3127,@marktenenholtz,2021-12-16 16:22:17+00:00,https://twitter.com/marktenenholtz/status/1471516039108255744,"@radekosmulski @bhutanisanyam1 @kaggle It’s often the most beautiful part of the solution. The modeling tricks I learn are amazing, but the validation tricks I learn are invaluable."
3128,@marktenenholtz,2021-12-16 15:51:04+00:00,https://twitter.com/marktenenholtz/status/1471508182941851660,@bhutanisanyam1 @radekosmulski Learn how to validate your models and start building your own model building pipeline that you feel comfortable hacking
3129,@marktenenholtz,2021-12-15 23:26:35+00:00,https://twitter.com/marktenenholtz/status/1471260428307156992,"@fchollet Makes even more sense if you imagine showing it to someone from 500 years ago. To them, it’s indistinguishable from magic."
3130,@marktenenholtz,2021-12-15 16:59:06+00:00,https://twitter.com/marktenenholtz/status/1471162914618937346,@HamelHusain @tunguz @sh_reya My recent mission has been to spread those locked secrets far and wide. It’s a shame it’s such gated knowledge.
3131,@marktenenholtz,2021-12-15 16:16:25+00:00,https://twitter.com/marktenenholtz/status/1471152175162302467,"Me, in school: “gonna die if I have to read the mathematical derivation of another model”

Me, a graduate: *furiously poring over LightGBM’s C code for fun*"
3132,@marktenenholtz,2021-12-15 15:13:39+00:00,https://twitter.com/marktenenholtz/status/1471136379375652877,@WalterReade Dante says the 7th ring is violence and I’m pretty certain that can be considered violence
3133,@marktenenholtz,2021-12-15 15:09:48+00:00,https://twitter.com/marktenenholtz/status/1471135408645980160,@WalterReade Was this hell? Were you in hell?
3134,@marktenenholtz,2021-12-15 14:58:38+00:00,https://twitter.com/marktenenholtz/status/1471132597069750277,"One little lifesaver for me over the years has been learning to dig through GitHub issues.

Any esoteric error you’ve encountered with a big package is probably in there."
3135,@marktenenholtz,2021-12-14 19:45:06+00:00,https://twitter.com/marktenenholtz/status/1470842301073215492,@tunguz It's great... if you like slow!
3136,@marktenenholtz,2021-12-14 15:34:05+00:00,https://twitter.com/marktenenholtz/status/1470779132455927818,"@TheZachMueller Trying something is the best way to learn, too, and it may give you the knowledge to discover something new in the future!"
3137,@marktenenholtz,2021-12-14 13:30:17+00:00,https://twitter.com/marktenenholtz/status/1470747975823544320,I've found myself using these videos as reference materials during my day job constantly. Everything is implemented entirely from scratch and is explained crystal clear.
3138,@marktenenholtz,2021-12-14 13:30:16+00:00,https://twitter.com/marktenenholtz/status/1470747974561062921,"(side note, I actually reference the 2019 version since it's the one I originally followed. You can find it here: https://t.co/Gu3ZyQNvfC)"
3139,@marktenenholtz,2021-12-14 13:30:16+00:00,https://twitter.com/marktenenholtz/status/1470747972820512769,"Want to learn deep learning from the foundations with only limited programming experience?

The course I've found myself coming back to time and time again throughout my career is the @fastdotai course.

You can check it out here: (https://t.co/vVl5xmw1cC)

#100DaysOfCode"
3140,@marktenenholtz,2021-12-13 20:55:10+00:00,https://twitter.com/marktenenholtz/status/1470497547294760972,"@johnsbarton @kolorhari @svpino Market data isn't the type of data I'm talking about. 

Example: Heard an interview that discussed how a company sells ore shipment monitoring that can tell hedge funds, for example, exactly how much iron ore is being mined by a specific company before it's publicly released."
3141,@marktenenholtz,2021-12-13 20:46:09+00:00,https://twitter.com/marktenenholtz/status/1470495279493390340,"@HamelHusain Really what they're doing here is tuning their learning rate scheduler.

A model scheduled with cosine annealing for 100 epochs but early stopped after 10 will be different than the same model only scheduled for 10 epochs."
3142,@marktenenholtz,2021-12-13 18:22:44+00:00,https://twitter.com/marktenenholtz/status/1470459185058004992,"How can you tell if a supervised machine learning method performs well?

See if it wins @kaggle competitions. The rest usually don't."
3143,@marktenenholtz,2021-12-13 16:20:20+00:00,https://twitter.com/marktenenholtz/status/1470428384081719300,"@kolorhari @svpino Yeah it is a bit far-fetched. Hedge funds pay for data sources that give them such an incredible advantage, you'll never be able to touch them unless you have the same resources. They talk about it in the course."
3144,@marktenenholtz,2021-12-13 15:50:32+00:00,https://twitter.com/marktenenholtz/status/1470420883080634372,"@svpino I got my master’s through the same program. Honestly, the main benefit of the course is to learn ML from some wonderful professors in a great format. 

You won’t exactly be able to beat the market with what you learn here, though."
3145,@marktenenholtz,2021-12-11 14:59:58+00:00,https://twitter.com/marktenenholtz/status/1469683382183514118,"Struggling to learn something complex?

Don’t bang your head against the wall. Revisit it after a night’s sleep.

It’s amazing how often it just clicks in your head afterwards."
3146,@marktenenholtz,2021-12-11 14:30:32+00:00,https://twitter.com/marktenenholtz/status/1469675976330301450,@tunguz Congrats Bojan! Looking forward to the highlight reel.
3147,@marktenenholtz,2021-12-10 22:04:31+00:00,https://twitter.com/marktenenholtz/status/1469427838206070790,@aureliengeron I think you mean 0-offsetting :D
3148,@marktenenholtz,2021-12-10 19:37:14+00:00,https://twitter.com/marktenenholtz/status/1469390773003112456,"@omarsar0 And when you’re learning something, make it interesting. Pick a project that both interests you and forces you to try something new. 

Reading/listening without doing won’t get you anywhere."
3149,@marktenenholtz,2021-12-10 18:17:52+00:00,https://twitter.com/marktenenholtz/status/1469370796078424069,"@fchollet Your validation set (being smaller) inevitably doesn’t contain all the complexities that the training set does, so the model will bottom out on that set faster. Giving it an extra epoch or two means it can generalize a bit more before actually overfitting."
3150,@marktenenholtz,2021-12-10 17:30:42+00:00,https://twitter.com/marktenenholtz/status/1469358928010043393,"@LBacaj Plus, if you’re an intermediate in a wide enough field, you and someone else at your “level” probably know a lot of stuff that the other doesn’t.

That means you both have a lot to offer each other!"
3151,@marktenenholtz,2021-12-10 11:10:59+00:00,https://twitter.com/marktenenholtz/status/1469263368062181378,"@bhutanisanyam1 Interesting that removing dropout helped out here. I’ve seen a lot of fairly recent discussion about dropout hurting transformers when they are fine tuned on a regression task, so I wonder what else it will help."
3152,@marktenenholtz,2021-12-10 11:04:44+00:00,https://twitter.com/marktenenholtz/status/1469261798272643074,@julien_c But can pip handle dependency resolution well enough to compete with conda yet? Reducing dependencies isn’t always a focus for internal packages when teams are trying to move quickly and that gives conda an advantage.
3153,@marktenenholtz,2021-12-09 16:05:23+00:00,https://twitter.com/marktenenholtz/status/1468975070022352903,"@paulg There's often nothing people want to talk about more than their pain points. 

Every company has to scratch some itch, and peers are generally happy to tell you what they'd buy to scratch it."
3154,@marktenenholtz,2021-12-09 14:34:44+00:00,https://twitter.com/marktenenholtz/status/1468952255382659073,"@aureliengeron I'm late to this, but the idea here is ""index"" (Julia, Matlab, etc) vs. ""offset"" (Python, C++, etc). In Julia, you're asking for element #1. In Python, you're asking for the element 0 places away from the start of the list."
3155,@marktenenholtz,2021-12-09 14:31:53+00:00,https://twitter.com/marktenenholtz/status/1468951538525687812,"@Austen Most technical interviewers that I interact with just glance at your degree quickly to get a little more background info on you. It's not something that should take up a lot of real estate on your resume, for sure."
3156,@marktenenholtz,2021-12-08 22:15:54+00:00,https://twitter.com/marktenenholtz/status/1468705926978977794,"TL;DR, little feature engineering, roll-forward cross validation, ensemble of GRU and Transformer using @PyTorchLightnin and @PyTorch"
3157,@marktenenholtz,2021-12-08 22:15:54+00:00,https://twitter.com/marktenenholtz/status/1468705925276045312,Here's how I got a solo gold medal on an @MLB engagement forecasting @Kaggle competition. https://t.co/SgbA8WYLyq
3158,@marktenenholtz,2021-12-08 21:56:01+00:00,https://twitter.com/marktenenholtz/status/1468700920523497473,@IttaiSvidler those websites look like a tour into the days of web past
3159,@marktenenholtz,2021-12-08 20:06:40+00:00,https://twitter.com/marktenenholtz/status/1468673403410391046,"@Austen When you say their median is $50k, do you mean among all grads or among grads with similar majors to the programs you offer?"
3160,@marktenenholtz,2021-12-08 18:03:26+00:00,https://twitter.com/marktenenholtz/status/1468642392098197512,"@KentWeyrauch @PFF_Moo @EthanCDouglas @benbbaldwin @PFF_George I'm saying a betting line is a measure of market opinion, which is what makes it an objective measure. It does not, however, objectively measure how good a team is."
3161,@marktenenholtz,2021-12-08 17:55:01+00:00,https://twitter.com/marktenenholtz/status/1468640272204681219,"@PFF_Moo @EthanCDouglas @benbbaldwin @PFF_George @KentWeyrauch It seems to me like this debate would be less confusing if ""quantitative"" was used instead of ""objective"" in a few places. Direct measurements are objective (that's what betting lines are, by definition) but, say, a forecasting model is probably better classified as subjective."
3162,@marktenenholtz,2021-12-08 17:31:12+00:00,https://twitter.com/marktenenholtz/status/1468634278552559620,"@julien_c I would basically view it at the same price as Microsoft paid. The valuation (imo) came from Microsoft's vision and plan for it, and the goodwill they paid in is now starting to materialize. A company with less of a vision wouldn't have a good reason to pay that huge sum."
3163,@marktenenholtz,2021-12-08 15:06:33+00:00,https://twitter.com/marktenenholtz/status/1468597876221788160,@tunguz “Doing more with fewer parameters” I tell myself as I download a pretrained Megatron-11B
3164,@marktenenholtz,2021-12-07 16:16:41+00:00,https://twitter.com/marktenenholtz/status/1468253138494832653,@itsAntWright Basically they grade every play for every player and give them a grade from -2 to +2. My understanding is that it’s considered highly credible even at the pro level. You can read about it here https://t.co/ydbV0EaIYy.
3165,@marktenenholtz,2021-11-09 18:36:21+00:00,https://twitter.com/marktenenholtz/status/1458141426785472513,@tunguz https://t.co/77k6NOfNTB
3166,@marktenenholtz,2021-10-02 13:22:22+00:00,https://twitter.com/marktenenholtz/status/1444291670321897484,@tunguz One of the smartest data scientists I’ve worked with has a PhD in computational psychology and has a remarkable ability to empathize with customer behavior patterns. It’s incredibly valuable thought diversity.
3167,@marktenenholtz,2021-09-30 14:03:41+00:00,https://twitter.com/marktenenholtz/status/1443577292265984012,@abhi1thakur https://t.co/oEB3LA3RvF! Love the guitar hero version of him
3168,@marktenenholtz,2021-05-20 00:36:08+00:00,https://twitter.com/marktenenholtz/status/1395176480225775618,@tunguz They looked like they had some pretty cool face masks
3169,@marktenenholtz,2021-01-14 16:05:22+00:00,https://twitter.com/marktenenholtz/status/1349749458121224194,@EthanCDouglas I think the word you’re looking for is “modularize”
