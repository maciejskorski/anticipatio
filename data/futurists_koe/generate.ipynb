{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             @elonmusk\n",
       "1            @BillGates\n",
       "2            @JeffBezos\n",
       "3             @tim_cook\n",
       "4         @sundarpichai\n",
       "             ...       \n",
       "251             @future\n",
       "252               @NASA\n",
       "253    @superforecaster\n",
       "254      @DigiworldInst\n",
       "255         @BigDataBRA\n",
       "Name: 2, Length: 256, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:scrapping:@elonmusk\n",
      "INFO:scrapping:@BillGates\n"
     ]
    }
   ],
   "source": [
    "# install social media scrapper: !pip3 install snscrape\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# configure logging\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger('scrapping')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "# twits ranges\n",
    "start_date = datetime.datetime(2021,1,1,tzinfo=datetime.timezone.utc)\n",
    "attributes = ('date','url','rawContent')\n",
    "\n",
    "\n",
    "def get_tweets(username,n_tweets=None,attributes=attributes):\n",
    "    tweets = sntwitter.TwitterSearchScraper(f'from:{username}').get_items() # invoke the scrapper\n",
    "    tweets = itertools.islice(tweets,n_tweets) # stopped when the count reached\n",
    "    tweets = itertools.takewhile(lambda t:t.date>=start_date, tweets) # stop when date passed\n",
    "    tweets = map(lambda t: (username,)+tuple(getattr(t,a) for a in attributes),tweets) # keep only attributes needed\n",
    "    pd.DataFrame(tweets).to_csv(f'./data/{username}.csv')\n",
    "    logger.info(username)\n",
    "    return tweets\n",
    "\n",
    "\n",
    "# a list of accounts to scrape\n",
    "user_names = pd.read_csv('./2023_02_16_A_list_of_tech_and_future_KOE.txt',header=None,sep='\\t')[2][:2]\n",
    "\n",
    "# parallelise queries for speed ! \n",
    "with mp.Pool(4) as p:\n",
    "    results = p.imap(get_tweets, user_names)\n",
    "    p.close()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e97d664ee103394b0533f1ee531435373195263bf8c5b5c10c43c63457b8b388"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
