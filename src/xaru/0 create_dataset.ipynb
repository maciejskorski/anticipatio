{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "repo_path = Path('/home/krajda/anticipatio/')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unique users:400\n",
                        "Unique texts:1200003\n",
                        "Unique posts (timestamp+text): 1458018\n"
                    ]
                }
            ],
            "source": [
                "def open_fn(f):\n",
                "    try:\n",
                "        return pd.read_csv(f, engine='python')\n",
                "    except:\n",
                "        return pd.DataFrame()\n",
                "\n",
                "tweets = pd.concat([\n",
                "    pd.concat(map(open_fn, Path(repo_path / 'data/futurists_kol/data').rglob('*csv'))), \n",
                "    pd.concat(map(open_fn, Path(repo_path / 'data/futurists_rossdawson/data').rglob('*csv')))\n",
                "])\n",
                "\n",
                "tweets.columns = ['index','user','timestamp','url','txt']\n",
                "tweets.reset_index(drop=True,inplace=True)\n",
                "\n",
                "tweets['txt'] = tweets['txt'].astype(str)\n",
                "tweets['user']=tweets['user'].str.replace('@','').str.strip().str.lower()\n",
                "tweets['timestamp'] = pd.to_datetime(tweets['timestamp'])\n",
                "\n",
                "tweets = tweets.drop(columns=['index'])\n",
                "\n",
                "tweets.drop_duplicates(inplace=True,subset=['timestamp','txt'])\n",
                "tweets.reset_index(inplace=True,drop=True)\n",
                "\n",
                "print('Unique users:{}\\nUnique texts:{}'.format(tweets['user'].nunique(),tweets['txt'].nunique()))\n",
                "docs = tweets['txt'].tolist()\n",
                "\n",
                "print('Unique posts (timestamp+text): {}'.format(len(docs)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TEXT CLEANING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cleaning tweets... html unescape\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1458018/1458018 [00:00<00:00, 1643832.90it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cleaning tweets... removing regex # 0\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1458018/1458018 [00:04<00:00, 324226.89it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cleaning tweets... removing regex # 1\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1458018/1458018 [00:13<00:00, 107991.76it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cleaning tweets... removing regex # 2\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1458018/1458018 [00:11<00:00, 123369.33it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Cleaning tweets... removing RTs\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1458018/1458018 [00:00<00:00, 1837098.65it/s]\n"
                    ]
                }
            ],
            "source": [
                "import html\n",
                "import re\n",
                "from tqdm import tqdm\n",
                "\n",
                "regexes = [\n",
                "    re.compile(\n",
                "        r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,\"\n",
                "        r\"}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(\"\n",
                "        r\"?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\"\n",
                "    ), # URLS\n",
                "    re.compile(r\"\\S*@\\S*\\..\\S*\"), # EMAILS\n",
                "    re.compile(r\"(?<=\\s)(@[\\w\\-\\.]+)(?=[\\:\\,\\.\\!\\?\\s]?)|^(@[\\w\\-\\.]+)(?=[\\:\\,\\.\\!\\?\\s]?)\"), # HANDLES\n",
                "]\n",
                "\n",
                "docs = tweets['txt'].tolist()\n",
                "\n",
                "print('Cleaning tweets... html unescape')\n",
                "docs = [html.unescape(t) for t in tqdm(docs)]\n",
                "\n",
                "for regex in regexes:\n",
                "    print('Cleaning tweets... removing regex #', regexes.index(regex))\n",
                "    docs = [regex.sub('', t) for t in tqdm(docs)]\n",
                "\n",
                "print('Cleaning tweets... removing RTs')\n",
                "docs = [t[4:] if t.startswith('RT :') else t for t in tqdm(docs)]\n",
                "\n",
                "tweets['original_text'] = tweets['txt']\n",
                "tweets['txt'] = docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "pd.to_pickle(tweets,repo_path / 'data/final.pkl')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "3.9.16",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
